{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LMRTFY stands for Let Me Run That For You . Create functions that run in the cloud, on your servers or even on your laptop. Call them from code, anywhere, just like a regular function. Quickstart Guide Tutorial Examples API Reference How to report issues and how to contribute Introduction \u00b6 LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possile to use LMRTFY Call deployed function like any other function provided by a local library Warning LMRTFY is currently in an early phase. Things will likely change in future releases. Quickstart - TL;DR \u00b6 install with pip install lmrtfy login/sign up with lmrtfy login annotate your code's inputs with variable and its outputs with result deploy the script: lmrtfy deploy examples/deployment/calc_compound_interest.py --local Use the deployed function (from another terminal, or another computer!): open examples/calling_cloud_functions/call_function.py run python examples/call_deployed_function.py to call the deployed function and get the results. As you can see in step 5, it's as simple as calling a regular function from any other library you have installed locally. Examples \u00b6 The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one!","title":"Introduction"},{"location":"#introduction","text":"LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possile to use LMRTFY Call deployed function like any other function provided by a local library Warning LMRTFY is currently in an early phase. Things will likely change in future releases.","title":"Introduction"},{"location":"#quickstart-tldr","text":"install with pip install lmrtfy login/sign up with lmrtfy login annotate your code's inputs with variable and its outputs with result deploy the script: lmrtfy deploy examples/deployment/calc_compound_interest.py --local Use the deployed function (from another terminal, or another computer!): open examples/calling_cloud_functions/call_function.py run python examples/call_deployed_function.py to call the deployed function and get the results. As you can see in step 5, it's as simple as calling a regular function from any other library you have installed locally.","title":"Quickstart - TL;DR"},{"location":"#examples","text":"The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one!","title":"Examples"},{"location":"CHANGELOG/","text":"v0.0.7 - 16/sep/2022 \u00b6 introduced catalog feature to call 'cloud' functions directly from code. enhanced documentation (API reference, links, quickstart, structure, ...) general bug fixes for better stability v0.0.6 - 08/sep/2022 \u00b6 changed input formats for json as follows: { \"profile_id\" : \"<profile_id>\" , \"job_parameters\" : { \"time\" : 200.0 }, \"parameter_units\" : { \"time\" : \"s\" } } to { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } modified readme to match docs added check if job_id is valid UUID v0.0.5 - 06/sep/2022 \u00b6 included version check changed listener to remove profile collisions v0.0.4 - 05/sep/2022 \u00b6 First release This creates the following commands: * lmrtfy login => get token * lmrtfy deploy <script.py> --local => get profile_id for deployed script profile * lmrtfy submit <profile_id> <input.json> => get job_id for submitted job * lmrtfy fetch <job_id> <path_to_save> => get results for finished job","title":"Changelog"},{"location":"CHANGELOG/#v007-16sep2022","text":"introduced catalog feature to call 'cloud' functions directly from code. enhanced documentation (API reference, links, quickstart, structure, ...) general bug fixes for better stability","title":"v0.0.7 - 16/sep/2022"},{"location":"CHANGELOG/#v006-08sep2022","text":"changed input formats for json as follows: { \"profile_id\" : \"<profile_id>\" , \"job_parameters\" : { \"time\" : 200.0 }, \"parameter_units\" : { \"time\" : \"s\" } } to { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } modified readme to match docs added check if job_id is valid UUID","title":"v0.0.6 - 08/sep/2022"},{"location":"CHANGELOG/#v005-06sep2022","text":"included version check changed listener to remove profile collisions","title":"v0.0.5 - 06/sep/2022"},{"location":"CHANGELOG/#v004-05sep2022","text":"First release This creates the following commands: * lmrtfy login => get token * lmrtfy deploy <script.py> --local => get profile_id for deployed script profile * lmrtfy submit <profile_id> <input.json> => get job_id for submitted job * lmrtfy fetch <job_id> <path_to_save> => get results for finished job","title":"v0.0.4 - 05/sep/2022"},{"location":"about/","text":"lmrtfy is a project that is currently work in progress. Things will change. If you have feature requests please let us know!","title":"About"},{"location":"contributing/","text":"We welcome all contributors and their contributions. If you want to contribute code, you will need to fork the lmrtfy repository and create your changes there. You should create an issue with the original repository to let everyone know that you are working on something that you intend to merge into the original repository. This way, we will know that you are working on something and we can get started on discussing the details and if it fits into our strategy. After you are done with your changes, create a Pull Request and explain the changes. Pull Requests should have a small scope and not change everything at once.","title":"Contributing"},{"location":"quickstart/","text":"Installation: \u00b6 We provide a PyPI package Note We recommend installing LMRTFY into a virtual environment to keep your system clean. Linux and MacOS \u00b6 We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy Windows \u00b6 On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy First login \u00b6 Login/sign up to receive access token: $ lmrtfy login . The token is saved in ~/.lmrtfy/auth/token but you should not need to manually open the token. Create code annotations \u00b6 Annotate the inputs and outputs of your script with variable and results and save it as script.py : # import the required things from lmrtfy from lmrtfy import variable , result # annotate an input time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) speed = result ( 9.81 * time , name = \"speed\" , min = 0 , max = 9810 , unit = \"m/s\" ) This calculates the velocity of an object in free fall after time seconds. Run python script.py to create the profile. This always works, even without access to lmrtfy. Deploy the script to accept jobs from the generated web API \u00b6 Run lmrtfy deploy script.py --local to generate the API and start the runner to listen to jobs for your script. You can find the profile id to submit a job in the logs: INFO Starting deployment of examples/velocity_from_gravity/calc_velocity.py WARNING Deploying locally. INFO Profile_id to be used for requests: 7ff68a0cfd8c61122cfdaf0a835c7cd1f94e7db9 Submit jobs with CLI \u00b6 Open a new terminal and submit a new job ( <profile_id> is the profile id you received during the deployment ( 7ff... in the example above)): $ lmrtfy submit <profile_id> <input.json> For the example above save the following in your input.json : { \"argument_values\" : { \"time\" : 200.0 }, \"argument_units\" : { \"time\" : \"s\" } } You will receive a job_id which you will need to fetch the results later on: INFO Job-id: 14584640 -778c-4f91-a288-03cffc2b9c7a Fetch results \u00b6 Get the results by calling $ lmrtfy fetch <job_id> <path> . Currently, the results will be saved in <path>/<job_id> as JSON files.","title":"Quickstart"},{"location":"quickstart/#installation","text":"We provide a PyPI package Note We recommend installing LMRTFY into a virtual environment to keep your system clean.","title":"Installation:"},{"location":"quickstart/#linux-and-macos","text":"We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy","title":"Linux and MacOS"},{"location":"quickstart/#windows","text":"On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy","title":"Windows"},{"location":"quickstart/#first-login","text":"Login/sign up to receive access token: $ lmrtfy login . The token is saved in ~/.lmrtfy/auth/token but you should not need to manually open the token.","title":"First login"},{"location":"quickstart/#create-code-annotations","text":"Annotate the inputs and outputs of your script with variable and results and save it as script.py : # import the required things from lmrtfy from lmrtfy import variable , result # annotate an input time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) speed = result ( 9.81 * time , name = \"speed\" , min = 0 , max = 9810 , unit = \"m/s\" ) This calculates the velocity of an object in free fall after time seconds. Run python script.py to create the profile. This always works, even without access to lmrtfy.","title":"Create code annotations"},{"location":"quickstart/#deploy-the-script-to-accept-jobs-from-the-generated-web-api","text":"Run lmrtfy deploy script.py --local to generate the API and start the runner to listen to jobs for your script. You can find the profile id to submit a job in the logs: INFO Starting deployment of examples/velocity_from_gravity/calc_velocity.py WARNING Deploying locally. INFO Profile_id to be used for requests: 7ff68a0cfd8c61122cfdaf0a835c7cd1f94e7db9","title":"Deploy the script to accept jobs from the generated web API"},{"location":"quickstart/#submit-jobs-with-cli","text":"Open a new terminal and submit a new job ( <profile_id> is the profile id you received during the deployment ( 7ff... in the example above)): $ lmrtfy submit <profile_id> <input.json> For the example above save the following in your input.json : { \"argument_values\" : { \"time\" : 200.0 }, \"argument_units\" : { \"time\" : \"s\" } } You will receive a job_id which you will need to fetch the results later on: INFO Job-id: 14584640 -778c-4f91-a288-03cffc2b9c7a","title":"Submit jobs with CLI"},{"location":"quickstart/#fetch-results","text":"Get the results by calling $ lmrtfy fetch <job_id> <path> . Currently, the results will be saved in <path>/<job_id> as JSON files.","title":"Fetch results"},{"location":"report_bugs/","text":"When you find an issue, don't hesitate to create an issue to let us know about the bug. This is an important step to make us aware of the bug. We are very grateful for all kinds of issues in order to enhance the overall user experience!","title":"How to report an issue"},{"location":"roadmap/","text":"","title":"Roadmap"},{"location":"api_reference/annotation/","text":"result ( value , name , min = None , max = None , unit = None ) \u00b6 result defines an input parameter of the script. It is used to create the profile for the code. r1 = result(5*v1, name='res1') creates a result named 'res1' in the profile. If run with the python interpreter r1 is equal to 5*v1 Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric result takes. Currently not checked. None max [optional] Maximum value that a numeric result takes. Currently not checked. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `result` defines an input parameter of the script. It is used to create the profile for the code. `r1 = result(5*v1, name='res1')` creates a result named 'res1' in the profile. If run with the python interpreter `r1` is equal to `5*v1` :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric result takes. Currently not checked. :param max: [optional] Maximum value that a numeric result takes. Currently not checked. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'result' , dtype = _get_type ( value ), unit = None , min = min , max = max ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with ( open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_result_ { name } .json' )), 'w' )) as f : json . dump ({ name : value }, f , cls = NumpyEncoder ) logging . info ( f \"Running: Saved result ' { name } ' with value ' { value } '.\" ) return value variable ( value , name , min = None , max = None , unit = None ) \u00b6 variable defines an input parameter of the script. It is used to create the profile for the code. v1 = variable(5, name='var1') create a variable named 'var1' in the profile. If run with the python interpreter v1 takes the value 5 and can be used in the rest of the code just like any other variable. Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric variable takes. Checked by API. None max [optional] Maximum value that a numeric variable takes. Checked by API. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `variable` defines an input parameter of the script. It is used to create the profile for the code. `v1 = variable(5, name='var1')` create a variable named 'var1' in the profile. If run with the python interpreter `v1` takes the value 5 and can be used in the rest of the code just like any other variable. :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric variable takes. Checked by API. :param max: [optional] Maximum value that a numeric variable takes. Checked by API. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'variable' , dtype = _get_type ( value ), min = min , max = max , unit = unit ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_variable_ { name } .json' )), 'r' ) as f : tmp = json . load ( f ) # TODO: Make it work for numpy dtypes. dtype = _inverse_type_map [ type ( value )] value = dtype ( tmp [ name ]) logging . info ( f \"Running: Loaded variable ' { name } ' of type ' { dtype } ' and value ' { value } '.\" ) return value","title":"Annotation"},{"location":"api_reference/annotation/#src.lmrtfy.annotation.result","text":"result defines an input parameter of the script. It is used to create the profile for the code. r1 = result(5*v1, name='res1') creates a result named 'res1' in the profile. If run with the python interpreter r1 is equal to 5*v1 Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric result takes. Currently not checked. None max [optional] Maximum value that a numeric result takes. Currently not checked. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `result` defines an input parameter of the script. It is used to create the profile for the code. `r1 = result(5*v1, name='res1')` creates a result named 'res1' in the profile. If run with the python interpreter `r1` is equal to `5*v1` :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric result takes. Currently not checked. :param max: [optional] Maximum value that a numeric result takes. Currently not checked. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'result' , dtype = _get_type ( value ), unit = None , min = min , max = max ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with ( open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_result_ { name } .json' )), 'w' )) as f : json . dump ({ name : value }, f , cls = NumpyEncoder ) logging . info ( f \"Running: Saved result ' { name } ' with value ' { value } '.\" ) return value","title":"result()"},{"location":"api_reference/annotation/#src.lmrtfy.annotation.variable","text":"variable defines an input parameter of the script. It is used to create the profile for the code. v1 = variable(5, name='var1') create a variable named 'var1' in the profile. If run with the python interpreter v1 takes the value 5 and can be used in the rest of the code just like any other variable. Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric variable takes. Checked by API. None max [optional] Maximum value that a numeric variable takes. Checked by API. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `variable` defines an input parameter of the script. It is used to create the profile for the code. `v1 = variable(5, name='var1')` create a variable named 'var1' in the profile. If run with the python interpreter `v1` takes the value 5 and can be used in the rest of the code just like any other variable. :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric variable takes. Checked by API. :param max: [optional] Maximum value that a numeric variable takes. Checked by API. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'variable' , dtype = _get_type ( value ), min = min , max = max , unit = unit ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_variable_ { name } .json' )), 'r' ) as f : tmp = json . load ( f ) # TODO: Make it work for numpy dtypes. dtype = _inverse_type_map [ type ( value )] value = dtype ( tmp [ name ]) logging . info ( f \"Running: Loaded variable ' { name } ' of type ' { dtype } ' and value ' { value } '.\" ) return value","title":"variable()"},{"location":"api_reference/catalog/","text":"Catalog \u00b6 Bases: object The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during from lmrtfy import catalog . If you want to retrieve newly deployed function, call catalog.update() . To run a deployed function from the catalog call catalog.<deployed_function>(*args, **kwargs) . Each function that has been pulled into the catalog is available via the help() command. Source code in src/lmrtfy/functions.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class Catalog ( object ): \"\"\" The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during `from lmrtfy import catalog`. If you want to retrieve newly deployed function, call `catalog.update()`. To run a deployed function from the catalog call `catalog.<deployed_function>(*args, **kwargs)`. Each function that has been pulled into the catalog is available via the `help()` command. \"\"\" def __init__ ( self ): h = LoginHandler () if h . login (): h . get_token () self . config = get_cliconfig () self . token = load_token_data ()[ 'access_token' ] self . headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { self . token } \" } self . profiles = None self . update () def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" try : r = requests . get ( self . config [ 'api_catalog_url' ], headers = self . headers ) if r . status_code == 200 : self . profiles = r . json () logging . info ( \"Updated function catalog.\" ) for profile in self . profiles [ 'profiles' ]: pid = profile . split ( '/' )[ - 1 ] t = fetch_profile ( pid ) func_name = unique_name ( self , t [ 'filename' ] . replace ( ' \\\\ ' , '/' ) . split ( '/' )[ - 1 ] . split ( '.' )[ 0 ] . strip ()) logging . info ( f \"Added function { func_name } .\" ) self . __add_function ( func_name , * signature_from_profile ( t ), pid = pid ) except : # TODO: Except clause too broad! logging . error ( \"Could not update function catalog.\" ) def __add_function ( self , name , sig , res_ann , pid ): template = fetch_template ( pid ) def f ( ** kwargs ) -> Job : for p in kwargs : template [ 'argument_values' ][ p ] = kwargs [ p ] r = requests . post ( self . config [ 'api_submit_url' ] + f '/ { pid } ' , data = json . dumps ( template , cls = NumpyEncoder ), headers = self . headers ) if r . status_code == 200 : return Job ( r . json ()[ 'job_id' ]) if r . status_code == 400 : logging . error ( f 'Input Error: { r . json () } ' ) setattr ( self , name , create_function ( sig , f , func_name = name )) update () \u00b6 Call update to update the catalog with newly deployed functions. Source code in src/lmrtfy/functions.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" try : r = requests . get ( self . config [ 'api_catalog_url' ], headers = self . headers ) if r . status_code == 200 : self . profiles = r . json () logging . info ( \"Updated function catalog.\" ) for profile in self . profiles [ 'profiles' ]: pid = profile . split ( '/' )[ - 1 ] t = fetch_profile ( pid ) func_name = unique_name ( self , t [ 'filename' ] . replace ( ' \\\\ ' , '/' ) . split ( '/' )[ - 1 ] . split ( '.' )[ 0 ] . strip ()) logging . info ( f \"Added function { func_name } .\" ) self . __add_function ( func_name , * signature_from_profile ( t ), pid = pid ) except : # TODO: Except clause too broad! logging . error ( \"Could not update function catalog.\" )","title":"Catalog"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog","text":"Bases: object The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during from lmrtfy import catalog . If you want to retrieve newly deployed function, call catalog.update() . To run a deployed function from the catalog call catalog.<deployed_function>(*args, **kwargs) . Each function that has been pulled into the catalog is available via the help() command. Source code in src/lmrtfy/functions.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class Catalog ( object ): \"\"\" The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during `from lmrtfy import catalog`. If you want to retrieve newly deployed function, call `catalog.update()`. To run a deployed function from the catalog call `catalog.<deployed_function>(*args, **kwargs)`. Each function that has been pulled into the catalog is available via the `help()` command. \"\"\" def __init__ ( self ): h = LoginHandler () if h . login (): h . get_token () self . config = get_cliconfig () self . token = load_token_data ()[ 'access_token' ] self . headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { self . token } \" } self . profiles = None self . update () def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" try : r = requests . get ( self . config [ 'api_catalog_url' ], headers = self . headers ) if r . status_code == 200 : self . profiles = r . json () logging . info ( \"Updated function catalog.\" ) for profile in self . profiles [ 'profiles' ]: pid = profile . split ( '/' )[ - 1 ] t = fetch_profile ( pid ) func_name = unique_name ( self , t [ 'filename' ] . replace ( ' \\\\ ' , '/' ) . split ( '/' )[ - 1 ] . split ( '.' )[ 0 ] . strip ()) logging . info ( f \"Added function { func_name } .\" ) self . __add_function ( func_name , * signature_from_profile ( t ), pid = pid ) except : # TODO: Except clause too broad! logging . error ( \"Could not update function catalog.\" ) def __add_function ( self , name , sig , res_ann , pid ): template = fetch_template ( pid ) def f ( ** kwargs ) -> Job : for p in kwargs : template [ 'argument_values' ][ p ] = kwargs [ p ] r = requests . post ( self . config [ 'api_submit_url' ] + f '/ { pid } ' , data = json . dumps ( template , cls = NumpyEncoder ), headers = self . headers ) if r . status_code == 200 : return Job ( r . json ()[ 'job_id' ]) if r . status_code == 400 : logging . error ( f 'Input Error: { r . json () } ' ) setattr ( self , name , create_function ( sig , f , func_name = name ))","title":"Catalog"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.update","text":"Call update to update the catalog with newly deployed functions. Source code in src/lmrtfy/functions.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" try : r = requests . get ( self . config [ 'api_catalog_url' ], headers = self . headers ) if r . status_code == 200 : self . profiles = r . json () logging . info ( \"Updated function catalog.\" ) for profile in self . profiles [ 'profiles' ]: pid = profile . split ( '/' )[ - 1 ] t = fetch_profile ( pid ) func_name = unique_name ( self , t [ 'filename' ] . replace ( ' \\\\ ' , '/' ) . split ( '/' )[ - 1 ] . split ( '.' )[ 0 ] . strip ()) logging . info ( f \"Added function { func_name } .\" ) self . __add_function ( func_name , * signature_from_profile ( t ), pid = pid ) except : # TODO: Except clause too broad! logging . error ( \"Could not update function catalog.\" )","title":"update()"},{"location":"examples/compound_interest/","text":"Example 3: Compound interest \u00b6 The third example calculates the compound interest \\(C\\) starting from a principal value \\(P\\) with annualt interest \\(I\\) after \\(N\\) years: \\[ C = P \\cdot (1 + I)^N - P \\] Very common formula in anything related to finance. Again, we start with the plain code, as you would implement it right away: # file: ci.py def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : principal = 10_000 interest = 6 years = 10 ci = compound_interest ( principal , interest , years ) print ( f \"Compound interest after { years } years: { ci } \" ) You can run this example with $ python ci.py and it should print 7908.47 . Which is the compound interest after 10 years if you started with 10000 units that grow by 6% each year. There are several problems with this solution: 1. You need to change the code to run it for other inputs 2. Units are unclear! principal is a currency, but that actually does not matter. The real problem is the interest . Is it decimal or in %? Annotate with lmrtfy \u00b6 Using lmrtfy, you would annotate the script as follows: # file: calc_compound_interest.py from lmrtfy import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): \"\"\" Compute the compound interest for `years` when starting from `principal` with `annual interest`. compound interest = principal * (1 + annual_interest)^years - principal \"\"\" return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : principal = variable ( 10000. , name = \"principal\" , min = 0 ) annual_interest = variable ( 6.0 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ) years = variable ( 10 , name = \"years\" , min = 0 ) ci = result ( compound_interest ( principal , annual_interest , years ), name = \"compound_interest\" Now, we run python ci_lmrtfy_1.py to generate the profile. Warning The type annotation are not enforced when run locally. LMRTFY checks the types and units only if jobs are submitted through its API. This guarantees that you can run your code without our service. Deployment \u00b6 After creating the profile we can easily deploy with $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local Call compound_interest from code \u00b6 Similar to the other examples we just need to import the catalog and call the correct function: # file: examples/compound_interest/call_compound_interest.py from time import sleep from lmrtfy import catalog job = catalog . calc_compound_interest ( 5. , 10. , 5 ) if job : print ( job . id , job . status ) while not job . ready : sleep ( 1. ) print ( job . results ) Call compound_interest from the CLI \u00b6 The output should be similar to this: INFO Profile_id to be used for requests: <profile_id> The <profile_id> is important to submit jobs. To submit a job you are currently required to save the input parameters as JSON (e.g. input.json ): { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } Now, we have everything that is needed to start a job: $ lmrtfy submit <profile_id> input.json The job id for this job is printed to the terminal: INFO Job-id: <job_id> We need the <job_id> later to fetch the results from the computation. Alternative Annotation \u00b6 A more compact but working alternative is to create the result as follows: from lmrtfy import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : ci = result ( compound_interest ( principal = variable ( 10000. , name = \"principal\" , min = 0 ), annual_interest = variable ( 6 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ), years = variable ( 10 , name = \"years\" , min = 0 ) ), name = \"compound_interest\" ) print ( ci ) It's not necessarily prettier to look at, but also works!","title":"Compound Interest"},{"location":"examples/compound_interest/#example-3-compound-interest","text":"The third example calculates the compound interest \\(C\\) starting from a principal value \\(P\\) with annualt interest \\(I\\) after \\(N\\) years: \\[ C = P \\cdot (1 + I)^N - P \\] Very common formula in anything related to finance. Again, we start with the plain code, as you would implement it right away: # file: ci.py def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : principal = 10_000 interest = 6 years = 10 ci = compound_interest ( principal , interest , years ) print ( f \"Compound interest after { years } years: { ci } \" ) You can run this example with $ python ci.py and it should print 7908.47 . Which is the compound interest after 10 years if you started with 10000 units that grow by 6% each year. There are several problems with this solution: 1. You need to change the code to run it for other inputs 2. Units are unclear! principal is a currency, but that actually does not matter. The real problem is the interest . Is it decimal or in %?","title":"Example 3: Compound interest"},{"location":"examples/compound_interest/#annotate-with-lmrtfy","text":"Using lmrtfy, you would annotate the script as follows: # file: calc_compound_interest.py from lmrtfy import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): \"\"\" Compute the compound interest for `years` when starting from `principal` with `annual interest`. compound interest = principal * (1 + annual_interest)^years - principal \"\"\" return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : principal = variable ( 10000. , name = \"principal\" , min = 0 ) annual_interest = variable ( 6.0 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ) years = variable ( 10 , name = \"years\" , min = 0 ) ci = result ( compound_interest ( principal , annual_interest , years ), name = \"compound_interest\" Now, we run python ci_lmrtfy_1.py to generate the profile. Warning The type annotation are not enforced when run locally. LMRTFY checks the types and units only if jobs are submitted through its API. This guarantees that you can run your code without our service.","title":"Annotate with lmrtfy"},{"location":"examples/compound_interest/#deployment","text":"After creating the profile we can easily deploy with $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local","title":"Deployment"},{"location":"examples/compound_interest/#call-compound_interest-from-code","text":"Similar to the other examples we just need to import the catalog and call the correct function: # file: examples/compound_interest/call_compound_interest.py from time import sleep from lmrtfy import catalog job = catalog . calc_compound_interest ( 5. , 10. , 5 ) if job : print ( job . id , job . status ) while not job . ready : sleep ( 1. ) print ( job . results )","title":"Call compound_interest from code"},{"location":"examples/compound_interest/#call-compound_interest-from-the-cli","text":"The output should be similar to this: INFO Profile_id to be used for requests: <profile_id> The <profile_id> is important to submit jobs. To submit a job you are currently required to save the input parameters as JSON (e.g. input.json ): { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } Now, we have everything that is needed to start a job: $ lmrtfy submit <profile_id> input.json The job id for this job is printed to the terminal: INFO Job-id: <job_id> We need the <job_id> later to fetch the results from the computation.","title":"Call compound_interest from the CLI"},{"location":"examples/compound_interest/#alternative-annotation","text":"A more compact but working alternative is to create the result as follows: from lmrtfy import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : ci = result ( compound_interest ( principal = variable ( 10000. , name = \"principal\" , min = 0 ), annual_interest = variable ( 6 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ), years = variable ( 10 , name = \"years\" , min = 0 ) ), name = \"compound_interest\" ) print ( ci ) It's not necessarily prettier to look at, but also works!","title":"Alternative Annotation"},{"location":"examples/free_fall/","text":"Example 2: Velocity due to gravtity in free fall \u00b6 The second example calculates the velocity of an object falling from the sky (without air resistance). The standard gravity on earth is 9.81 m*s^(-2). Multiplicated by the fall time, we will get the velocity of the object after that time. If you like equations more, you might recognize these from your physics class: $$ v = g \\cdot t $$ In regular python code that you run locally it would look like this: standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now we want to be able to share that functionality via the lmrtfy web API. All we have to do is decide which variables are considered to be an input and a result of the computation: # file: examples/free_fall/calc_velocity.py from lmrtfy import variable , result standard_gravity = 9.81 time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) velocity = result ( standard_gravity * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m/s\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now run $ python examples/velocity_from_gravity/calc_velocity.py to generate the required profile. This way you can also check if your code is actually working the way you expect it. Deploying the script \u00b6 To deploy, you simply run $ lmrtfy deploy examples/velocity_from_gravity/calc_velocity.py --local . Do not stop that process, because than you will not be able to submit a job. Calling from code \u00b6 Calling calc_velocity by code as easy as it was for the first example . import time from lmrtfy import catalog job = catalog . calc_velocity ( time = 100.0 ) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) Note You can also run help(calc_velocity) to see the corresponding help. Right now, only the function signature is shown but in the future you will also be able to see the docstrings. Calling from CLI \u00b6 Open a new terminal in the same directory and run $ lmrtfy submit <profile_id> . The profile_id has been printed in the lmrtfy deploy step. This does not work right out of the box, because you need to specify a JSON file that contains the input parameters for your job. A template for that JSON should have been printed in the CLI. Create such a JSON file and name it input.json and put values of the correct type into the values (no type conversion is happening in the API, so if float is required, you cannot input an int ). Alternatively, use the provided input.json in examples/free_fall/input.json : { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } Now run $ lmrtfy submit <profile_id> examples/free_fall/input.json . You will receive a job_id which we will shortly need to fetch the results after they are computed. After your job has run, you can get the results by running $ lmrtfy fetch <job_id> <path to store results> . The results are downloaded and stored inside the specified path within a directory that has the job_id as its name.","title":"Free Fall"},{"location":"examples/free_fall/#example-2-velocity-due-to-gravtity-in-free-fall","text":"The second example calculates the velocity of an object falling from the sky (without air resistance). The standard gravity on earth is 9.81 m*s^(-2). Multiplicated by the fall time, we will get the velocity of the object after that time. If you like equations more, you might recognize these from your physics class: $$ v = g \\cdot t $$ In regular python code that you run locally it would look like this: standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now we want to be able to share that functionality via the lmrtfy web API. All we have to do is decide which variables are considered to be an input and a result of the computation: # file: examples/free_fall/calc_velocity.py from lmrtfy import variable , result standard_gravity = 9.81 time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) velocity = result ( standard_gravity * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m/s\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now run $ python examples/velocity_from_gravity/calc_velocity.py to generate the required profile. This way you can also check if your code is actually working the way you expect it.","title":"Example 2: Velocity due to gravtity in free fall"},{"location":"examples/free_fall/#deploying-the-script","text":"To deploy, you simply run $ lmrtfy deploy examples/velocity_from_gravity/calc_velocity.py --local . Do not stop that process, because than you will not be able to submit a job.","title":"Deploying the script"},{"location":"examples/free_fall/#calling-from-code","text":"Calling calc_velocity by code as easy as it was for the first example . import time from lmrtfy import catalog job = catalog . calc_velocity ( time = 100.0 ) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) Note You can also run help(calc_velocity) to see the corresponding help. Right now, only the function signature is shown but in the future you will also be able to see the docstrings.","title":"Calling from code"},{"location":"examples/free_fall/#calling-from-cli","text":"Open a new terminal in the same directory and run $ lmrtfy submit <profile_id> . The profile_id has been printed in the lmrtfy deploy step. This does not work right out of the box, because you need to specify a JSON file that contains the input parameters for your job. A template for that JSON should have been printed in the CLI. Create such a JSON file and name it input.json and put values of the correct type into the values (no type conversion is happening in the API, so if float is required, you cannot input an int ). Alternatively, use the provided input.json in examples/free_fall/input.json : { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } Now run $ lmrtfy submit <profile_id> examples/free_fall/input.json . You will receive a job_id which we will shortly need to fetch the results after they are computed. After your job has run, you can get the results by running $ lmrtfy fetch <job_id> <path to store results> . The results are downloaded and stored inside the specified path within a directory that has the job_id as its name.","title":"Calling from CLI"},{"location":"examples/starting_example/","text":"Example 1: Simple annotation \u00b6 This is a simple example to showcase the general usage of lmrtfy. It can be found in examples/example1/example1.py . The two core concepts are the variable and result functions which annotate the inputs and outputs of the script. They are needed to create the profile which is used to create the API. # file: examples/starting/example1.py import numpy as np from lmrtfy import variable , result # 1 x = variable ( 5 , name = \"x\" , min = 1 , max = 10 ) # 2 y = variable ( np . linspace ( 0. , 1. , 101 , dtype = np . float64 ), name = \"y\" , min =- 1. , max = 11. , unit = \"m\" ) # 3 z = variable ( \"abc\" , name = \"z\" ) z1 = variable ([ \"abc\" , \"def\" ], name = \"z1\" ) # 4 z2 = variable ([ \"abc\" , 1 , 1.1 ], name = \"z2\" ) z3 = variable ({ 'a' : \"abc\" , 'b' : 1 }, name = \"z3\" ) a = result ( x * y , name = \"a\" ) # 5 b = result ( x * z , name = \"b\" ) The functions need to be imported from the lmrtfy library The variable x has the local value 5 and can be between 1 and 10. You can have numpy arrays as inputs Lists and dictionaries work, too! Results are similar to variables. They have a name and an expression that they will become. Run python examples/starting/example1.py to create the profile needed for the deployment. Deployment \u00b6 To deploy the script run lmrtfy deploy examples/starting/example1.py --local Call example1 from code \u00b6 Now you can simply call catalog.example1() with the correct arguments, and you are good to go: #!/usr/bin/env python # -*- coding: utf-8 -*- import time from lmrtfy import catalog job = catalog . example1 ( x = 1 , y = [ 1 , 2.0 , 3.0 ], z = \"foobar\" , z1 = [ \"bar\" , \"foo\" ], z2 = [ \"foo\" , 1 , 42 ], z3 = { \"foo\" : \"bar\" , \"bar\" : \"foo\" } ) #help(catalog.example1) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) if job: is currently required to ensure that you actually got a job object back from the function which would not be the case if the submission failed. Calling example1 from the CLI \u00b6 Note We encourage you to use code to submit jobs and get results. During the deployment you should have received a profile_id : Profile_id to be used for requests: 392b3bf6fb7cf32ba1a10052f138583e1a594354 We need the profile_id to submit a job from the CLI: lmrtfy submit 392b3bf6fb7cf32ba1a10052f138583e1a594354 examples/starting/example1.json If the JSON file has the correct inputs, in a valid range with correct units you will see that the job submission was successful. INFO Job submission successful. INFO Job-id: cfaf7e58-d6fc-4509-96b9-439fb2877f85 With this job_id you can now get the job results: lmrtfy fetch cfaf7e58-d6fc-4509-96b9-439fb2877f85 . That's all that is to it. Happy Hacking!","title":"Starting Example"},{"location":"examples/starting_example/#example-1-simple-annotation","text":"This is a simple example to showcase the general usage of lmrtfy. It can be found in examples/example1/example1.py . The two core concepts are the variable and result functions which annotate the inputs and outputs of the script. They are needed to create the profile which is used to create the API. # file: examples/starting/example1.py import numpy as np from lmrtfy import variable , result # 1 x = variable ( 5 , name = \"x\" , min = 1 , max = 10 ) # 2 y = variable ( np . linspace ( 0. , 1. , 101 , dtype = np . float64 ), name = \"y\" , min =- 1. , max = 11. , unit = \"m\" ) # 3 z = variable ( \"abc\" , name = \"z\" ) z1 = variable ([ \"abc\" , \"def\" ], name = \"z1\" ) # 4 z2 = variable ([ \"abc\" , 1 , 1.1 ], name = \"z2\" ) z3 = variable ({ 'a' : \"abc\" , 'b' : 1 }, name = \"z3\" ) a = result ( x * y , name = \"a\" ) # 5 b = result ( x * z , name = \"b\" ) The functions need to be imported from the lmrtfy library The variable x has the local value 5 and can be between 1 and 10. You can have numpy arrays as inputs Lists and dictionaries work, too! Results are similar to variables. They have a name and an expression that they will become. Run python examples/starting/example1.py to create the profile needed for the deployment.","title":"Example 1: Simple annotation"},{"location":"examples/starting_example/#deployment","text":"To deploy the script run lmrtfy deploy examples/starting/example1.py --local","title":"Deployment"},{"location":"examples/starting_example/#call-example1-from-code","text":"Now you can simply call catalog.example1() with the correct arguments, and you are good to go: #!/usr/bin/env python # -*- coding: utf-8 -*- import time from lmrtfy import catalog job = catalog . example1 ( x = 1 , y = [ 1 , 2.0 , 3.0 ], z = \"foobar\" , z1 = [ \"bar\" , \"foo\" ], z2 = [ \"foo\" , 1 , 42 ], z3 = { \"foo\" : \"bar\" , \"bar\" : \"foo\" } ) #help(catalog.example1) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) if job: is currently required to ensure that you actually got a job object back from the function which would not be the case if the submission failed.","title":"Call example1 from code"},{"location":"examples/starting_example/#calling-example1-from-the-cli","text":"Note We encourage you to use code to submit jobs and get results. During the deployment you should have received a profile_id : Profile_id to be used for requests: 392b3bf6fb7cf32ba1a10052f138583e1a594354 We need the profile_id to submit a job from the CLI: lmrtfy submit 392b3bf6fb7cf32ba1a10052f138583e1a594354 examples/starting/example1.json If the JSON file has the correct inputs, in a valid range with correct units you will see that the job submission was successful. INFO Job submission successful. INFO Job-id: cfaf7e58-d6fc-4509-96b9-439fb2877f85 With this job_id you can now get the job results: lmrtfy fetch cfaf7e58-d6fc-4509-96b9-439fb2877f85 . That's all that is to it. Happy Hacking!","title":"Calling example1 from the CLI"},{"location":"tutorial/annotation/","text":"Annotate your script \u00b6 The annotation of your script tells the lmrtfy tool which python variables are considered inputs and outputs, which is done via the variable and results functions. This step is important, because lmrtfy traces the calls to variable and result to create a profile for the code. This profile includes the inputs and outputs as well as the additional meta information ( min , max , unit , and possibly more in the future). Let's assume that you have create a script to calculate the velocity of an object after a certain time: # file: free_fall.py standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now, if you want to recalculate for a different time, you would edit the script and run it again. While this might work for a small script like this, this becomes tedious if you have different input variables and want others to use your script easily, too. Let's change the script in such a way that lmrtfy can create a profile which can be used to deploy the function and make it available to other users: # file: free_fall_lmrtfy.py # import the required things from lmrtfy from lmrtfy import variable , result standard_gravity = 9.81 # annotate an input time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # annotate an output velocity = result ( 9.81 * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) If you run python free_fall_lmrtfy.py you get the exact same result as before. During the run, lmrtfy created the profile for free_fall_lmrtfy.py which will be needed to deploy the function. Create the annotation profile \u00b6 It is required to run your script at least once with the regular python interpreter to create the annotation profile which will be used to generate the API. $ python <script.py> The profile is currently saved under ~/.lmrtfy/profiles which will change in the future to respect XDG directory specifications. Focus: variable and result \u00b6 The API reference for variable and result can be found here . These functions are transparent. That means the assignment a = variable(5, name=\"a\") assigns a the value 5 . This way you can run the script simply with your local python interpreter if lmrtfy is installed in the environment. variable and result do not have any external dependency (e.g. API calls) The functions signatures are as follows. Only the first two arguments are required. Ideally, the name argument should match the name of the variable you assign to, although that is not necessary. It's considered to be a best practice, because it reduces possible errors and fosters a more intuitive understanding of the code. # variable signature: variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type # result signature: result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type The value argument specifies the default argument for the variable and has to be one of the supported types: int, float, complex, bool, np.ndarray, list, dict . name declares the name of the variable that will be used for the API generation. A sensible choice is the same name as the variable's name in the code itself. min and max can be used to specify boundaries for the input and output values in case they are numeric. This might come in handy if the code only works for certain input parameter ranges. If the inputs are outside the specified range the job will be rejected by the generated API. unit is a str that declares the unit of the variable/result. This is especially useful in scientific calculations where units are often not standardized and unclear.","title":"Annotate your script"},{"location":"tutorial/annotation/#annotate-your-script","text":"The annotation of your script tells the lmrtfy tool which python variables are considered inputs and outputs, which is done via the variable and results functions. This step is important, because lmrtfy traces the calls to variable and result to create a profile for the code. This profile includes the inputs and outputs as well as the additional meta information ( min , max , unit , and possibly more in the future). Let's assume that you have create a script to calculate the velocity of an object after a certain time: # file: free_fall.py standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now, if you want to recalculate for a different time, you would edit the script and run it again. While this might work for a small script like this, this becomes tedious if you have different input variables and want others to use your script easily, too. Let's change the script in such a way that lmrtfy can create a profile which can be used to deploy the function and make it available to other users: # file: free_fall_lmrtfy.py # import the required things from lmrtfy from lmrtfy import variable , result standard_gravity = 9.81 # annotate an input time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # annotate an output velocity = result ( 9.81 * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) If you run python free_fall_lmrtfy.py you get the exact same result as before. During the run, lmrtfy created the profile for free_fall_lmrtfy.py which will be needed to deploy the function.","title":"Annotate your script"},{"location":"tutorial/annotation/#create-the-annotation-profile","text":"It is required to run your script at least once with the regular python interpreter to create the annotation profile which will be used to generate the API. $ python <script.py> The profile is currently saved under ~/.lmrtfy/profiles which will change in the future to respect XDG directory specifications.","title":"Create the annotation profile"},{"location":"tutorial/annotation/#focus-variable-and-result","text":"The API reference for variable and result can be found here . These functions are transparent. That means the assignment a = variable(5, name=\"a\") assigns a the value 5 . This way you can run the script simply with your local python interpreter if lmrtfy is installed in the environment. variable and result do not have any external dependency (e.g. API calls) The functions signatures are as follows. Only the first two arguments are required. Ideally, the name argument should match the name of the variable you assign to, although that is not necessary. It's considered to be a best practice, because it reduces possible errors and fosters a more intuitive understanding of the code. # variable signature: variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type # result signature: result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type The value argument specifies the default argument for the variable and has to be one of the supported types: int, float, complex, bool, np.ndarray, list, dict . name declares the name of the variable that will be used for the API generation. A sensible choice is the same name as the variable's name in the code itself. min and max can be used to specify boundaries for the input and output values in case they are numeric. This might come in handy if the code only works for certain input parameter ranges. If the inputs are outside the specified range the job will be rejected by the generated API. unit is a str that declares the unit of the variable/result. This is especially useful in scientific calculations where units are often not standardized and unclear.","title":"Focus: variable and result"},{"location":"tutorial/deployment/","text":"Deploy the function (local runner) \u00b6 Now you can deploy the function and make it available via the LMRTFY API. This is simply done by running $ lmrtfy deploy <path_to_script.py> --local The --local flag means that the script will run locally on your computer and waits for jobs from the outside. The LMRTFY API only allows job submissions that fit your deployed annotation profile. In the future you will be able to deploy directly to the cloud. Then, you do not have to host the runner yourself. Warning Don't change the script after you have deployed it. The current advice would be to copy and rename the script before deployment. In later versions, this will be taken care of by the lmrtfy tool. When a job is submitted the types of the job's input parameters are checked by the lmrtfy API. Futhermore they are also checked for their bounds and their units. This way, only jobs that can be run successfully with the script belonging to the deployed profile. Deploying to the cloud \u00b6 LMRTFY is currently not able to deploy your scripts directly to the cloud; however, you can do this manually by using lmrtfy inside of a docker container. The docker container can be run on your laptop, one of your own servers or in the cloud. The current workaround would be to use lmrtfy on a server or inside a docker container which can be hosted in the cloud. Inside the docker container you run the same command as if you were to run locally. Note In the future, we will provide a sample docker container to simplify that.","title":"Deploy your script/function"},{"location":"tutorial/deployment/#deploy-the-function-local-runner","text":"Now you can deploy the function and make it available via the LMRTFY API. This is simply done by running $ lmrtfy deploy <path_to_script.py> --local The --local flag means that the script will run locally on your computer and waits for jobs from the outside. The LMRTFY API only allows job submissions that fit your deployed annotation profile. In the future you will be able to deploy directly to the cloud. Then, you do not have to host the runner yourself. Warning Don't change the script after you have deployed it. The current advice would be to copy and rename the script before deployment. In later versions, this will be taken care of by the lmrtfy tool. When a job is submitted the types of the job's input parameters are checked by the lmrtfy API. Futhermore they are also checked for their bounds and their units. This way, only jobs that can be run successfully with the script belonging to the deployed profile.","title":"Deploy the function (local runner)"},{"location":"tutorial/deployment/#deploying-to-the-cloud","text":"LMRTFY is currently not able to deploy your scripts directly to the cloud; however, you can do this manually by using lmrtfy inside of a docker container. The docker container can be run on your laptop, one of your own servers or in the cloud. The current workaround would be to use lmrtfy on a server or inside a docker container which can be hosted in the cloud. Inside the docker container you run the same command as if you were to run locally. Note In the future, we will provide a sample docker container to simplify that.","title":"Deploying to the cloud"},{"location":"tutorial/fetch_results/","text":"Fetching results in your code \u00b6 When you call a function from your code, the results are part of the job object created when calling the function . You can only get the results when they are ready by using job.results : while not job . ready : sleep ( 1. ) print ( job . results ) Currently, the while loop is necessary to wait for the results. This isn't the most ergonomic way to do this. This could easily become a future (in the sense of concurrent programming) later on. We are also looking for feedback, what would work best for you. Get results with the CLI \u00b6 LMRTFY also provides a way to download the results of the computation. All you need is the <job_id> that you received when you submitted the job. Then, you simply run $ lmrtfy fetch <job_id> <save_path> The results will be saved in <save_path>/<job_id>/.. . Each result is currently saved as a JSON file with the following format: { \"<var_name>\" : < value > } Each variable has its own file. Warning This will very likely change in the future to be more ergonomic.","title":"Get results"},{"location":"tutorial/fetch_results/#fetching-results-in-your-code","text":"When you call a function from your code, the results are part of the job object created when calling the function . You can only get the results when they are ready by using job.results : while not job . ready : sleep ( 1. ) print ( job . results ) Currently, the while loop is necessary to wait for the results. This isn't the most ergonomic way to do this. This could easily become a future (in the sense of concurrent programming) later on. We are also looking for feedback, what would work best for you.","title":"Fetching results in your code"},{"location":"tutorial/fetch_results/#get-results-with-the-cli","text":"LMRTFY also provides a way to download the results of the computation. All you need is the <job_id> that you received when you submitted the job. Then, you simply run $ lmrtfy fetch <job_id> <save_path> The results will be saved in <save_path>/<job_id>/.. . Each result is currently saved as a JSON file with the following format: { \"<var_name>\" : < value > } Each variable has its own file. Warning This will very likely change in the future to be more ergonomic.","title":"Get results with the CLI"},{"location":"tutorial/installation/","text":"There are two ways to install the lmrtfy . We recommend the usage of virtual environments at the moment due to the frequent updates and changes of lmrtfy. Linux and MacOS \u00b6 We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy Windows \u00b6 On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy This way you will always have the most recent release of lmrtfy . Install from Source \u00b6 You can also install from git which is the best way to use the nightly features. Clone the git repository and install manually: $ git clone --branch main https://github.com/lmrtfy/lmrtfy.git $ cd lmrtfy $ pip install . The main branch is the release branch and should always work with the lmrtfy API. Alternatively, you can use the develop branch. This should be the most up-to-date branch in the repository, but things might break. So be careful while using the develop branch.","title":"Installation"},{"location":"tutorial/installation/#linux-and-macos","text":"We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy","title":"Linux and MacOS"},{"location":"tutorial/installation/#windows","text":"On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy This way you will always have the most recent release of lmrtfy .","title":"Windows"},{"location":"tutorial/installation/#install-from-source","text":"You can also install from git which is the best way to use the nightly features. Clone the git repository and install manually: $ git clone --branch main https://github.com/lmrtfy/lmrtfy.git $ cd lmrtfy $ pip install . The main branch is the release branch and should always work with the lmrtfy API. Alternatively, you can use the develop branch. This should be the most up-to-date branch in the repository, but things might break. So be careful while using the develop branch.","title":"Install from Source"},{"location":"tutorial/login/","text":"To use LMRTFY you need to login to get an API token. The API token is necassary to deploy scripts as a callable function submit jobs as call cloud functions When you run an LMRTFY actions that require a token you will automatically be asked to login. Each token is valid for 24h before you need to relogin. If you allow cookies, you will probably not have to login again because the authentication provider Auth0 recognizes you. Info Tokens are currently valid for 24 hours. After that you will be requested to login again. That also means that you cannot have scripts deployed more than 24 hours right now. This will change soon so that you can deploy scripts longer than that. Just run lmrtfy deploy <script> --local again after 24h and you are fine if you need longer running deployments right now.) Sign-Up \u00b6 Before you can login the first time you need to sign up. Currently, you have two options to do that: email adress + password google account If you would like to sign up with another social login (e.g. GitHub) please let us know so that we can prioritize this issue.","title":"Login"},{"location":"tutorial/login/#sign-up","text":"Before you can login the first time you need to sign up. Currently, you have two options to do that: email adress + password google account If you would like to sign up with another social login (e.g. GitHub) please let us know so that we can prioritize this issue.","title":"Sign-Up"},{"location":"tutorial/submission/","text":"Calling a Deployed Function \u00b6 Let's assume that you have just deployed the script calc_compound_interest.py from the examples provided in examples/compound_interest/calc_compound_interest.py by running $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local Do not close that terminal as there won't be a runner to receive jobs in that case. Calling a Function from Code \u00b6 Our goal is to provide an interface to deployed functions that works just like any other function in any other library that you have locally installed. We provide an example call_compound_interest.py' in the same directory as the calc_compound_interest.py` script. As you can see, it's very close to a native function already. # file: examples/compound_interest/call_compound_interest.py from time import sleep from lmrtfy import catalog #1 job = catalog . calc_compound_interest ( 5. , 10. , 5 ) #2 if job : print ( job . id , job . status ) while not job . ready : #3 sleep ( 1. ) print ( job . results ) #4 Run the script with your local python interpreter python examples/compound_interest/call_compound_interest.py to make use of the deployed script. The output of the script looks like this: INFO Validating auth token. INFO Auth token accepted. INFO Valid access token found. Login not necessary. INFO Updated function catalog. INFO Added function calc_compound_interest. INFO Job 78b69e45-3d51-4734-92db-a4901ee6d02b created. Status is RUNNING. 78b69e45-3d51-4734-92db-a4901ee6d02b JobStatus.RUNNING { 'compound_interest' : 2 .7628156250000035 } The ID of the job is going to be different from the one shown in the example output. Let's discuss some aspects of the code a little closer: Importing the catalog triggers the catalog update to get newly deployed functions every time you run the code The function calc_compount_interest is now part of the catalog and can be called just like a normal function in your code, however this function is not executed locally and run wherever the corresponding runner is deployed. Loop until the job is ready. In this context ready means that the results are ready to be fetched. which can simply be done by calling job.results . The return value is a dictionary with the keys corresponding to the names of the results and the values are the actual values of the result. Using the CLI \u00b6 LMRTFY also provides a way to submit jobs with the lmrtfy CLI tool. All you need for this is a profile_id which is provided by you during the deployment and a JSON file that contains the input parameters. Attention This is a good way to call deployed scripts from another language as you can always build the JSON file and call the lmrtfy CLI. If you are using it this we, please contact us. We want to provide more native-feeling interfaces to languages other than python as well but would love to hear what you use to priortize. For the example calculating the compound interest, the JSON file would look like this: { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } argument_values and argument_units contain a key-value pair each for each of the inputs in the annotation profile. The types need to match exactly. No implicit type casting in performed during the submission. The unit also has to match exactly. Save the JSON file es input.json and run: $ lmrtfy deploy <profile_id> input.json --local Info Later on, we might perform automatic conversion in case of a unit mismatch, e.g. if the profile requires s (as in seconds) but the input is given as h (as in hours). There will be an option to enable/disable the function. If you have any opinions about that, please let us know When you submit your job you will receive a job_id which is needed to fetch the results as you will see in the next part of this guide.","title":"Submit a job"},{"location":"tutorial/submission/#calling-a-deployed-function","text":"Let's assume that you have just deployed the script calc_compound_interest.py from the examples provided in examples/compound_interest/calc_compound_interest.py by running $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local Do not close that terminal as there won't be a runner to receive jobs in that case.","title":"Calling a Deployed Function"},{"location":"tutorial/submission/#calling-a-function-from-code","text":"Our goal is to provide an interface to deployed functions that works just like any other function in any other library that you have locally installed. We provide an example call_compound_interest.py' in the same directory as the calc_compound_interest.py` script. As you can see, it's very close to a native function already. # file: examples/compound_interest/call_compound_interest.py from time import sleep from lmrtfy import catalog #1 job = catalog . calc_compound_interest ( 5. , 10. , 5 ) #2 if job : print ( job . id , job . status ) while not job . ready : #3 sleep ( 1. ) print ( job . results ) #4 Run the script with your local python interpreter python examples/compound_interest/call_compound_interest.py to make use of the deployed script. The output of the script looks like this: INFO Validating auth token. INFO Auth token accepted. INFO Valid access token found. Login not necessary. INFO Updated function catalog. INFO Added function calc_compound_interest. INFO Job 78b69e45-3d51-4734-92db-a4901ee6d02b created. Status is RUNNING. 78b69e45-3d51-4734-92db-a4901ee6d02b JobStatus.RUNNING { 'compound_interest' : 2 .7628156250000035 } The ID of the job is going to be different from the one shown in the example output. Let's discuss some aspects of the code a little closer: Importing the catalog triggers the catalog update to get newly deployed functions every time you run the code The function calc_compount_interest is now part of the catalog and can be called just like a normal function in your code, however this function is not executed locally and run wherever the corresponding runner is deployed. Loop until the job is ready. In this context ready means that the results are ready to be fetched. which can simply be done by calling job.results . The return value is a dictionary with the keys corresponding to the names of the results and the values are the actual values of the result.","title":"Calling a Function from Code"},{"location":"tutorial/submission/#using-the-cli","text":"LMRTFY also provides a way to submit jobs with the lmrtfy CLI tool. All you need for this is a profile_id which is provided by you during the deployment and a JSON file that contains the input parameters. Attention This is a good way to call deployed scripts from another language as you can always build the JSON file and call the lmrtfy CLI. If you are using it this we, please contact us. We want to provide more native-feeling interfaces to languages other than python as well but would love to hear what you use to priortize. For the example calculating the compound interest, the JSON file would look like this: { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } argument_values and argument_units contain a key-value pair each for each of the inputs in the annotation profile. The types need to match exactly. No implicit type casting in performed during the submission. The unit also has to match exactly. Save the JSON file es input.json and run: $ lmrtfy deploy <profile_id> input.json --local Info Later on, we might perform automatic conversion in case of a unit mismatch, e.g. if the profile requires s (as in seconds) but the input is given as h (as in hours). There will be an option to enable/disable the function. If you have any opinions about that, please let us know When you submit your job you will receive a job_id which is needed to fetch the results as you will see in the next part of this guide.","title":"Using the CLI"}]}