{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","tags":["Introduction"],"text":"Let Me Run That For You. \u2705 Create functions that run in the cloud , on your servers or even on your laptop . \u2705 Call them from code that runs somewhere else, just like a regular function . \u2705 Share functions with friends and colleagues, track their usage and monetize their usage. \u2705 Works with Python, but more languages will be added in the future. Quickstart Guide Tutorial Examples API Reference How to report issues and how to contribute Introduction \u00b6 LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possible to use LMRTFY Call deployed function like any other function provided by a local library Info LMRTFY is currently in an early phase. Things will likely change in future releases. Quickstart - TL;DR \u00b6 install with pip install lmrtfy login/sign up with lmrtfy login run $ ipython and from lmrtfy.functions import catalog call the provided example with job = catalog . examples . free_fall_lmrtfy ( 100. ) get the results with job . results As you can see in step 4, it's as simple as calling a regular function from any other library you have installed locally. Examples \u00b6 The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one!","title":"Introduction"},{"location":"#introduction","text":"LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possible to use LMRTFY Call deployed function like any other function provided by a local library Info LMRTFY is currently in an early phase. Things will likely change in future releases.","title":"Introduction"},{"location":"#quickstart-tldr","text":"install with pip install lmrtfy login/sign up with lmrtfy login run $ ipython and from lmrtfy.functions import catalog call the provided example with job = catalog . examples . free_fall_lmrtfy ( 100. ) get the results with job . results As you can see in step 4, it's as simple as calling a regular function from any other library you have installed locally.","title":"Quickstart - TL;DR"},{"location":"#examples","text":"The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one!","title":"Examples"},{"location":"CHANGELOG/","text":"0.0.12 - 18/oct/2022 \u00b6 output of lmrtfy is more precise and cleaner added PMF example which can be used here . 0.0.11 - 12/oct/2022 \u00b6 added LMRTFY_ACCESS_TOKEN to deploy and submit jobs catalog.issue_deploy_token(<function>) and catalog.issue_submit_token(<function>) 0.0.10 - 10/oct/2022 \u00b6 minor bug fixes v0.0.9 - 10/oct/2022 \u00b6 short UUIDs for jobs and profiles examples to use bug fixes revised documentation (structure, content, ...) sharing v0.0.8 - 16/sep/2022 \u00b6 fixed missing dependencies for installation v0.0.7 - 16/sep/2022 \u00b6 introduced catalog feature to call 'cloud' functions directly from code. enhanced documentation (API reference, links, quickstart, structure, ...) general bug fixes for better stability v0.0.6 - 08/sep/2022 \u00b6 changed input formats for json as follows: { \"profile_id\" : \"<profile_id>\" , \"job_parameters\" : { \"time\" : 200.0 }, \"parameter_units\" : { \"time\" : \"s\" } } to { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } modified readme to match docs added check if job_id is valid UUID v0.0.5 - 06/sep/2022 \u00b6 included version check changed listener to remove profile collisions v0.0.4 - 05/sep/2022 \u00b6 First release This creates the following commands: * lmrtfy login => get token * lmrtfy deploy <script.py> --local => get profile_id for deployed script profile * lmrtfy submit <profile_id> <input.json> => get job_id for submitted job * lmrtfy fetch <job_id> <path_to_save> => get results for finished job","title":"Changelog"},{"location":"CHANGELOG/#0012-18oct2022","text":"output of lmrtfy is more precise and cleaner added PMF example which can be used here .","title":"0.0.12 - 18/oct/2022"},{"location":"CHANGELOG/#0011-12oct2022","text":"added LMRTFY_ACCESS_TOKEN to deploy and submit jobs catalog.issue_deploy_token(<function>) and catalog.issue_submit_token(<function>)","title":"0.0.11 - 12/oct/2022"},{"location":"CHANGELOG/#0010-10oct2022","text":"minor bug fixes","title":"0.0.10 - 10/oct/2022"},{"location":"CHANGELOG/#v009-10oct2022","text":"short UUIDs for jobs and profiles examples to use bug fixes revised documentation (structure, content, ...) sharing","title":"v0.0.9 - 10/oct/2022"},{"location":"CHANGELOG/#v008-16sep2022","text":"fixed missing dependencies for installation","title":"v0.0.8 - 16/sep/2022"},{"location":"CHANGELOG/#v007-16sep2022","text":"introduced catalog feature to call 'cloud' functions directly from code. enhanced documentation (API reference, links, quickstart, structure, ...) general bug fixes for better stability","title":"v0.0.7 - 16/sep/2022"},{"location":"CHANGELOG/#v006-08sep2022","text":"changed input formats for json as follows: { \"profile_id\" : \"<profile_id>\" , \"job_parameters\" : { \"time\" : 200.0 }, \"parameter_units\" : { \"time\" : \"s\" } } to { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } modified readme to match docs added check if job_id is valid UUID","title":"v0.0.6 - 08/sep/2022"},{"location":"CHANGELOG/#v005-06sep2022","text":"included version check changed listener to remove profile collisions","title":"v0.0.5 - 06/sep/2022"},{"location":"CHANGELOG/#v004-05sep2022","text":"First release This creates the following commands: * lmrtfy login => get token * lmrtfy deploy <script.py> --local => get profile_id for deployed script profile * lmrtfy submit <profile_id> <input.json> => get job_id for submitted job * lmrtfy fetch <job_id> <path_to_save> => get results for finished job","title":"v0.0.4 - 05/sep/2022"},{"location":"cli_reference/","text":"The lmrtfy CLI tools uses the following schema: lmrtfy [ COMMAND ] [ COMMAND_OPTIONS ] where [COMMAND] is one of the following: login , logouot , deploy , submit , and fetch . Commands \u00b6 login \u00b6 The usage of the LMRTFY services is connected to a user accouunt. With lmrtfy login you can directly trigger the login process. This is usually not necessary, because each command that needs a valid authorization token will trigger the login if needed. logout \u00b6 Logout of the currently active account. This should only be necessary in special cases, e.g. if you have more than one account for some reason (private and work account for example). deploy <scipt> [OPTIONS] \u00b6 This command deploys the <script> and makes it available via the LMRTFY platform. It's now ready to take jobs. Note Currently, only local deployment via the --local flag works. If you want to deploy to the cloud you need to manually do this and run lmrtfy deploy <script> --local on the remote resource. Option Description --local Deploy locally on the current system. --run_as_daemon Run the deployment as daemon in the background. submit <profile_id> <arguments.json> \u00b6 The CLI allows you to submit jobs for a specific profile_id which is returned by the deploy step. <argument.json> needs to contain valid input data, otherwise the job is rejected. The structure of the JSON file can be found here . fetch <job_id> <path> \u00b6 It's also possible to fetch the results from a job with job_id . The job ID is displayed during the submission step. With <path> you specify where the results should be saved.","title":"CLI reference"},{"location":"cli_reference/#commands","text":"","title":"Commands"},{"location":"cli_reference/#login","text":"The usage of the LMRTFY services is connected to a user accouunt. With lmrtfy login you can directly trigger the login process. This is usually not necessary, because each command that needs a valid authorization token will trigger the login if needed.","title":"login"},{"location":"cli_reference/#logout","text":"Logout of the currently active account. This should only be necessary in special cases, e.g. if you have more than one account for some reason (private and work account for example).","title":"logout"},{"location":"cli_reference/#deploy-scipt-options","text":"This command deploys the <script> and makes it available via the LMRTFY platform. It's now ready to take jobs. Note Currently, only local deployment via the --local flag works. If you want to deploy to the cloud you need to manually do this and run lmrtfy deploy <script> --local on the remote resource. Option Description --local Deploy locally on the current system. --run_as_daemon Run the deployment as daemon in the background.","title":"deploy &lt;scipt&gt; [OPTIONS]"},{"location":"cli_reference/#submit-profile_id-argumentsjson","text":"The CLI allows you to submit jobs for a specific profile_id which is returned by the deploy step. <argument.json> needs to contain valid input data, otherwise the job is rejected. The structure of the JSON file can be found here .","title":"submit &lt;profile_id&gt; &lt;arguments.json&gt;"},{"location":"cli_reference/#fetch-job_id-path","text":"It's also possible to fetch the results from a job with job_id . The job ID is displayed during the submission step. With <path> you specify where the results should be saved.","title":"fetch &lt;job_id&gt; &lt;path&gt;"},{"location":"contributing/","text":"We welcome all contributors and their contributions. All contributors will be listed in the project (if wanted). What to contribute? \u00b6 There are several ways that you can contribute to lmrtfy . Report bugs or suggest features Work on the code: fix a bug, create a feature, ... Improve the documentation Improving the documentation often isn't glamorous work but is highly appreciated by us. If you think something is missing in the documentation or something needs more information, please let us know or add the documentation. Contribute code \u00b6 The process of contributing code is quite easy: Fork the lmrtfy repository. Let us know what you want to work on to ensure that we can merge your code. Best to create an issue . Fix a bug, add a feature, ... Create a pull request . This process is standard in most repositories. If you have any questions or need help contributing, please let us know! Contribute documentation \u00b6 The files required to build the documentation are inside the docs directory. They are plain markdown files that can be easily edited. The documentation is built with mkdocs and the mkdocs-material theme. Everything you need to build the docs locally can be installed by pip install -r docs/requirements.txt . Reading the documentation of mkdocs and the material theme is highly recommend.","title":"Contributing code"},{"location":"contributing/#what-to-contribute","text":"There are several ways that you can contribute to lmrtfy . Report bugs or suggest features Work on the code: fix a bug, create a feature, ... Improve the documentation Improving the documentation often isn't glamorous work but is highly appreciated by us. If you think something is missing in the documentation or something needs more information, please let us know or add the documentation.","title":"What to contribute?"},{"location":"contributing/#contribute-code","text":"The process of contributing code is quite easy: Fork the lmrtfy repository. Let us know what you want to work on to ensure that we can merge your code. Best to create an issue . Fix a bug, add a feature, ... Create a pull request . This process is standard in most repositories. If you have any questions or need help contributing, please let us know!","title":"Contribute code"},{"location":"contributing/#contribute-documentation","text":"The files required to build the documentation are inside the docs directory. They are plain markdown files that can be easily edited. The documentation is built with mkdocs and the mkdocs-material theme. Everything you need to build the docs locally can be installed by pip install -r docs/requirements.txt . Reading the documentation of mkdocs and the material theme is highly recommend.","title":"Contribute documentation"},{"location":"quickstart/","text":"This is our quick start guide. Everything you need to know to get started is here. Installation \u00b6 You can install the lmrtfy package just like any other package: $ pip install lmrtfy If you use conda you need to install pip in your conda environment before you can use LMRTFY. This is likely necessary on Windows. $ conda install pip $ pip install lmrtfy Sign-Up \u00b6 To make full use of LMRTFY you need to sign up with us with lmrtfy login . You can sign up with GitHub to streamline the process. Calling your first remote function \u00b6 Calling a function that is available in the cloud is as easy as calling a native function. IPython listing - Best way to call jobs interactively 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ ipython In [ 1 ]: from lmrtfy.functions import catalog # information regarding the configuration 2022 - 10 - 12 12 : 14 : 28 [ 36627 ] INFO { 'namespaces' : [ 'examples' , '<user_namespace>' ]} # (1)! In [ 2 ]: job = catalog . examples . free_fall_lmrtfy ( time = 100. ) # (2)! 2022 - 10 - 12 12 : 16 : 08 [ 36627 ] INFO Job 8 GXvA6JREr created . Status is ACCEPTED . # (3)! In [ 3 ]: job . ready # (4)! Out [ 3 ]: True In [ 4 ]: job . results # (5)! Out [ 5 ]: { 'velocity' : 981.0 } If you want to know more about namespaces go here . This calls the function free_fall_lmrtfy in the LMRTFY namespace example . The function is executed on a remote resource that has been provided by us. The job is created and that status is ACCEPTED . Sometimes the computation is so quick that it immediately returns RESULTS_READY . job.ready checks if the results are computed and ready to be fetched with job.results job.results gets the results from LMRTFY as JSON: { \"<variable name>\" : <value> , ... } As you can see calling the function free_fall_lmrtfy looks just like calling any other function in Python; however, it is actually executed on a remote server, in this case on our own server as we provide the example. Each call to a deployed function returns a Job object if the submission was successful. If an error occurred None is returned. This way we can check if the submission was successful or not. If you get an error here, please let us know via hello@lmrt.fyi or open an issue . To get the results from the computation we can simply call job.results . Depending on the size of the results this call will take a while. In the example, this call should at most take a few seconds. Deploying your first function \u00b6 Deploying your own function is really easy, too. Let's say you want to calculate how fast a falling object will be after \\(x\\) seconds. The code which can be used with LMRTFY is really simple: free_fall_lmrtfy.py 1 2 3 4 5 6 7 8 9 from lmrtfy.annotation import variable , result # (1)! time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # (2)! standard_gravity = 9.81 velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) velocity = result ( velocity , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) # (3)! variable and result are the imports that are needed to annotate your code to make it work with LMRTFY. variable annotates any inputs of your script. result annotates a result of script. A script can have multiple results. The highlighted lines have been added or changed to let LMRTFY know about the structure of the script. Before we can deploy the function we need to run it through the Python interpreter once to create the annotation profile. $ python free_fall_lmrtfy.py To deploy this function on your laptop you simply run $ lmrtfy deploy free_fall_lmrtfy.py --local This makes the function available in your catalog as catatlog.<user_namespace>.free_fall_lmrtfy and starts a runner on your laptop. Nobody but you can call the function unless you share it with others. If no runner is deployed an error will be returned.","title":"Quick start"},{"location":"quickstart/#installation","text":"You can install the lmrtfy package just like any other package: $ pip install lmrtfy If you use conda you need to install pip in your conda environment before you can use LMRTFY. This is likely necessary on Windows. $ conda install pip $ pip install lmrtfy","title":"Installation"},{"location":"quickstart/#sign-up","text":"To make full use of LMRTFY you need to sign up with us with lmrtfy login . You can sign up with GitHub to streamline the process.","title":"Sign-Up"},{"location":"quickstart/#calling-your-first-remote-function","text":"Calling a function that is available in the cloud is as easy as calling a native function. IPython listing - Best way to call jobs interactively 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ ipython In [ 1 ]: from lmrtfy.functions import catalog # information regarding the configuration 2022 - 10 - 12 12 : 14 : 28 [ 36627 ] INFO { 'namespaces' : [ 'examples' , '<user_namespace>' ]} # (1)! In [ 2 ]: job = catalog . examples . free_fall_lmrtfy ( time = 100. ) # (2)! 2022 - 10 - 12 12 : 16 : 08 [ 36627 ] INFO Job 8 GXvA6JREr created . Status is ACCEPTED . # (3)! In [ 3 ]: job . ready # (4)! Out [ 3 ]: True In [ 4 ]: job . results # (5)! Out [ 5 ]: { 'velocity' : 981.0 } If you want to know more about namespaces go here . This calls the function free_fall_lmrtfy in the LMRTFY namespace example . The function is executed on a remote resource that has been provided by us. The job is created and that status is ACCEPTED . Sometimes the computation is so quick that it immediately returns RESULTS_READY . job.ready checks if the results are computed and ready to be fetched with job.results job.results gets the results from LMRTFY as JSON: { \"<variable name>\" : <value> , ... } As you can see calling the function free_fall_lmrtfy looks just like calling any other function in Python; however, it is actually executed on a remote server, in this case on our own server as we provide the example. Each call to a deployed function returns a Job object if the submission was successful. If an error occurred None is returned. This way we can check if the submission was successful or not. If you get an error here, please let us know via hello@lmrt.fyi or open an issue . To get the results from the computation we can simply call job.results . Depending on the size of the results this call will take a while. In the example, this call should at most take a few seconds.","title":"Calling your first remote function"},{"location":"quickstart/#deploying-your-first-function","text":"Deploying your own function is really easy, too. Let's say you want to calculate how fast a falling object will be after \\(x\\) seconds. The code which can be used with LMRTFY is really simple: free_fall_lmrtfy.py 1 2 3 4 5 6 7 8 9 from lmrtfy.annotation import variable , result # (1)! time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # (2)! standard_gravity = 9.81 velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) velocity = result ( velocity , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) # (3)! variable and result are the imports that are needed to annotate your code to make it work with LMRTFY. variable annotates any inputs of your script. result annotates a result of script. A script can have multiple results. The highlighted lines have been added or changed to let LMRTFY know about the structure of the script. Before we can deploy the function we need to run it through the Python interpreter once to create the annotation profile. $ python free_fall_lmrtfy.py To deploy this function on your laptop you simply run $ lmrtfy deploy free_fall_lmrtfy.py --local This makes the function available in your catalog as catatlog.<user_namespace>.free_fall_lmrtfy and starts a runner on your laptop. Nobody but you can call the function unless you share it with others. If no runner is deployed an error will be returned.","title":"Deploying your first function"},{"location":"report_bugs/","text":"When you find an issue, don't hesitate to create an issue to let us know about the bug. When you create an issue there are 3 options: Create a bug report \u00b6 Reporting bugs is very important. We are very thankful for each bug that is reported and will try to fix them as soon as possible. The clearer the description of the bug is and the more information you provide, the better. Request a feature \u00b6 This is the best choice if there is a feature you are missing. Each feature request will be reviewed. If the feature aligns with our vision of LMRTFY. If we are going to implement the feature it will be added to the roadmap. Ask a question \u00b6 This the best choice if you just want to ask a question about LMRTFY: Is a feature planned? How to do X? *... Open a blank issue \u00b6 This should be the last resort and only be used if your concern does not fit into any of the other categories.","title":"Reporting an issue"},{"location":"report_bugs/#create-a-bug-report","text":"Reporting bugs is very important. We are very thankful for each bug that is reported and will try to fix them as soon as possible. The clearer the description of the bug is and the more information you provide, the better.","title":"Create a bug report"},{"location":"report_bugs/#request-a-feature","text":"This is the best choice if there is a feature you are missing. Each feature request will be reviewed. If the feature aligns with our vision of LMRTFY. If we are going to implement the feature it will be added to the roadmap.","title":"Request a feature"},{"location":"report_bugs/#ask-a-question","text":"This the best choice if you just want to ask a question about LMRTFY: Is a feature planned? How to do X? *...","title":"Ask a question"},{"location":"report_bugs/#open-a-blank-issue","text":"This should be the last resort and only be used if your concern does not fit into any of the other categories.","title":"Open a blank issue"},{"location":"roadmap/","text":"This roadmap should give you an idea of the features we will work on in the future. If you find any feature highly relevant for you, please let us know: hello@lmrt.fyi . LMRTFY Library \u00b6 Annotation \u00b6 annotate resources in your code, e.g. big data sets support for more languages (help us prioritize with your feedback): JavaScript R Matlab C++ Julia ... Catalog (calling cloud functions like native ones) \u00b6 support for more languages (see above) enhance job and results handling (async handling, context) LMRTFY CLI tool \u00b6 cloud deployment via lmrtfy deploy deploy to one of the big cloud service providers (AWS, Azure, GCP, ...) deploy to your own servers (private cloud) LMRTFY web application \u00b6 accounting system for deployed functions (monetize your functions) overview of your functions manage functions see available runners sharing functions with others functions activity graph","title":"Roadmap"},{"location":"roadmap/#lmrtfy-library","text":"","title":"LMRTFY Library"},{"location":"roadmap/#annotation","text":"annotate resources in your code, e.g. big data sets support for more languages (help us prioritize with your feedback): JavaScript R Matlab C++ Julia ...","title":"Annotation"},{"location":"roadmap/#catalog-calling-cloud-functions-like-native-ones","text":"support for more languages (see above) enhance job and results handling (async handling, context)","title":"Catalog (calling cloud functions like native ones)"},{"location":"roadmap/#lmrtfy-cli-tool","text":"cloud deployment via lmrtfy deploy deploy to one of the big cloud service providers (AWS, Azure, GCP, ...) deploy to your own servers (private cloud)","title":"LMRTFY CLI tool"},{"location":"roadmap/#lmrtfy-web-application","text":"accounting system for deployed functions (monetize your functions) overview of your functions manage functions see available runners sharing functions with others functions activity graph","title":"LMRTFY web application"},{"location":"api_reference/annotation/","text":"result ( value , name , min = None , max = None , unit = None ) \u00b6 result defines an input parameter of the script. It is used to create the profile for the code. r1 = result(5*v1, name='res1') creates a result named 'res1' in the profile. If run with the python interpreter r1 is equal to 5*v1 Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric result takes. Currently not checked. None max [optional] Maximum value that a numeric result takes. Currently not checked. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. None Returns: Type Description supported_object_type value variable ( value , name , min = None , max = None , unit = None ) \u00b6 variable defines an input parameter of the script. It is used to create the profile for the code. v1 = variable(5, name='var1') create a variable named 'var1' in the profile. If run with the python interpreter v1 takes the value 5 and can be used in the rest of the code just like any other variable. Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric variable takes. Checked by API. None max [optional] Maximum value that a numeric variable takes. Checked by API. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. None Returns: Type Description supported_object_type value","title":"Annotation"},{"location":"api_reference/annotation/#src.lmrtfy.annotation.result","text":"result defines an input parameter of the script. It is used to create the profile for the code. r1 = result(5*v1, name='res1') creates a result named 'res1' in the profile. If run with the python interpreter r1 is equal to 5*v1 Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric result takes. Currently not checked. None max [optional] Maximum value that a numeric result takes. Currently not checked. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. None Returns: Type Description supported_object_type value","title":"result()"},{"location":"api_reference/annotation/#src.lmrtfy.annotation.variable","text":"variable defines an input parameter of the script. It is used to create the profile for the code. v1 = variable(5, name='var1') create a variable named 'var1' in the profile. If run with the python interpreter v1 takes the value 5 and can be used in the rest of the code just like any other variable. Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric variable takes. Checked by API. None max [optional] Maximum value that a numeric variable takes. Checked by API. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. None Returns: Type Description supported_object_type value","title":"variable()"},{"location":"api_reference/catalog/","text":"Catalog \u00b6 Bases: object The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during from lmrtfy import catalog . If you want to retrieve newly deployed function, call catalog.update() . To run a deployed function from the catalog call catalog.<namespace>.<deployed_function>(*args, **kwargs) . Each function that has been pulled into the catalog is available via the help() command. accept_invite ( invite_id ) \u00b6 Accept an invitation with invite_id . This only works once for each ID. Returns True if acceptance was successful, False otherwise. add_function_to_namespace ( namespace , function ) \u00b6 Add a function to an existing namespace. Example: catalog . add_function_to_namespace ( catalog .< namespace > , catalog .< namespace >.< function > create_namespace ( namespace , name ) \u00b6 Create a unique namespace that collects functions and can be shared. Example: catalog . create_namespace ( catalog .< namespace > , \"new_namespace\" ) Parameters: Name Type Description Default namespace Namespace parent namespace required name str Name of the new namespace. required Returns: Type Description bool Returns the True on success and False in case of an error. delete_namespace ( namespace ) \u00b6 Deletes the entire namespace. Deletion will fail if there are still functions in the namespace. issue_deploy_token ( function , token_type = 'deploy' ) \u00b6 This function creates a deploy token for function . It can only be used for this function. To use the token, you need to set the environment variable LMRTFY_ACCESS_TOKEN with the token before deployment. If you deploy with a token, you need to specify the namespace of the function with the --namespace=\"...\" argument of lmrtfy deploy . Returns a token with a token_id . The token_id is needed for token revocation. issue_submit_token ( function ) \u00b6 This function creates a submit token for function . It can only be used for this function. To use the token, you need to set the environment variable LMRTFY_ACCESS_TOKEN with the token before you submit a job: LMRTFY_ACCESS_TOKEN = \"LMRTFY....\" ipython In [ 0 ] : from lmrtfy.functions import catalog In [ 1 ] : job = catatlog.<namespace>.< function > ( ... ) Returns a token with a token_id . The token_id is needed for token revocation. remove_function_from_namespace ( function ) \u00b6 Delete function from its namespace. revoke_token ( token_id ) \u00b6 With this function you can revoke a token when you don't need it anymore. All you need is the token_id of the token which was returned to you when the token was issued. share_namespace ( namespace , recipient_email ) \u00b6 Share a namespace with someone else by email. The invite email will be sent to recipient_email . This does not have to be the email address associated with the account of the person you want to share the namespace with. Invites only work once. update () \u00b6 Call update to update the catalog with newly deployed functions. Job \u00b6 Bases: object ready () property \u00b6 Returns True if the job has finished successfully and the results are ready to be fetched. results () property \u00b6 Returns the results if they are ready and None otherwise. Results are returned as a dictionary: { \"<results_name>\" : resul t _value } status () property \u00b6 Queries the job status. If it returns JobStatus.UNKNOWN the job is likely in failed state.","title":"Catalog"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog","text":"Bases: object The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during from lmrtfy import catalog . If you want to retrieve newly deployed function, call catalog.update() . To run a deployed function from the catalog call catalog.<namespace>.<deployed_function>(*args, **kwargs) . Each function that has been pulled into the catalog is available via the help() command.","title":"Catalog"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.accept_invite","text":"Accept an invitation with invite_id . This only works once for each ID. Returns True if acceptance was successful, False otherwise.","title":"accept_invite()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.add_function_to_namespace","text":"Add a function to an existing namespace. Example: catalog . add_function_to_namespace ( catalog .< namespace > , catalog .< namespace >.< function >","title":"add_function_to_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.create_namespace","text":"Create a unique namespace that collects functions and can be shared. Example: catalog . create_namespace ( catalog .< namespace > , \"new_namespace\" ) Parameters: Name Type Description Default namespace Namespace parent namespace required name str Name of the new namespace. required Returns: Type Description bool Returns the True on success and False in case of an error.","title":"create_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.delete_namespace","text":"Deletes the entire namespace. Deletion will fail if there are still functions in the namespace.","title":"delete_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.issue_deploy_token","text":"This function creates a deploy token for function . It can only be used for this function. To use the token, you need to set the environment variable LMRTFY_ACCESS_TOKEN with the token before deployment. If you deploy with a token, you need to specify the namespace of the function with the --namespace=\"...\" argument of lmrtfy deploy . Returns a token with a token_id . The token_id is needed for token revocation.","title":"issue_deploy_token()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.issue_submit_token","text":"This function creates a submit token for function . It can only be used for this function. To use the token, you need to set the environment variable LMRTFY_ACCESS_TOKEN with the token before you submit a job: LMRTFY_ACCESS_TOKEN = \"LMRTFY....\" ipython In [ 0 ] : from lmrtfy.functions import catalog In [ 1 ] : job = catatlog.<namespace>.< function > ( ... ) Returns a token with a token_id . The token_id is needed for token revocation.","title":"issue_submit_token()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.remove_function_from_namespace","text":"Delete function from its namespace.","title":"remove_function_from_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.revoke_token","text":"With this function you can revoke a token when you don't need it anymore. All you need is the token_id of the token which was returned to you when the token was issued.","title":"revoke_token()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.share_namespace","text":"Share a namespace with someone else by email. The invite email will be sent to recipient_email . This does not have to be the email address associated with the account of the person you want to share the namespace with. Invites only work once.","title":"share_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.update","text":"Call update to update the catalog with newly deployed functions.","title":"update()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Job","text":"Bases: object","title":"Job"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Job.ready","text":"Returns True if the job has finished successfully and the results are ready to be fetched.","title":"ready()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Job.results","text":"Returns the results if they are ready and None otherwise. Results are returned as a dictionary: { \"<results_name>\" : resul t _value }","title":"results()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Job.status","text":"Queries the job status. If it returns JobStatus.UNKNOWN the job is likely in failed state.","title":"status()"},{"location":"examples/compound_interest/","text":"Example 3: Compound interest \u00b6 The third example calculates the compound interest \\(C\\) starting from a principal value \\(P\\) with annualt interest \\(I\\) after \\(N\\) years: \\[ C = P \\cdot (1 + I)^N - P \\] Very common formula in anything related to finance. Again, we start with the plain code, as you would implement it right away: calc_compound_interest.py 1 2 3 4 5 6 7 8 def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal principal = 10_000 interest = 6 years = 10 ci = compound_interest ( principal , interest , years ) print ( f \"Compound interest after { years } years: { ci } \" ) You can run this example with $ python ci.py and it should print 7908.47 . Which is the compound interest after 10 years if you started with 10000 units that grow by 6% each year. There are several problems with this solution: 1. You need to change the code to run it for other inputs 2. Units are unclear! principal is a currency, but that actually does not matter. The real problem is the interest . Is it decimal or in %? Annotate with lmrtfy \u00b6 Using lmrtfy, you would annotate the script as follows: calc_compound_interest_lmrtfy.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from lmrtfy.annotation import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal principal = 10_000. interest = 6. years = 10 principal = variable ( principal , name = \"principal\" , min = 0 ) annual_interest = variable ( principal , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ) years = variable ( years , name = \"years\" , min = 0 ) ci = compound_interest ( principal , annual_interest , years ) ci = result ( ci , name = \"compound_interest\" ) Now, we run python calc_compound_interest_lmrtfy_1.py to generate the profile. Warning The type annotation are not enforced when run locally. LMRTFY checks the types and units only if jobs are submitted through its API. This guarantees that you can run your code without our service. Deployment \u00b6 After creating the profile we can easily deploy with $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local Call compound_interest from code \u00b6 Similar to the other examples we just need to import the catalog and call the correct function: call_compound_interest.py 1 2 3 4 5 6 7 8 9 10 11 12 from time import sleep from lmrtfy.functions import catalog job = catalog .< your_namespace >. calc_compound_interest_lmrtfy ( 5. , 10. , 5 ) # (1)! if job : print ( job . id , job . status ) while not job . ready : sleep ( 1. ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . Call compound_interest from the CLI \u00b6 The output should be similar to this: INFO Profile_id to be used for requests: <profile_id> The <profile_id> is important to submit jobs. To submit a job you are currently required to save the input parameters as JSON (e.g. input.json ): input.json to calculate compound interest { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } Now, we have everything that is needed to start a job: $ lmrtfy submit <profile_id> input.json The job id for this job is printed to the terminal: INFO Job-id: <job_id> We need the <job_id> later to fetch the results from the computation. Alternative Annotation \u00b6 A more compact but working alternative is to create the result as follows: Alternative annotation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from lmrtfy.annotation import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : ci = result ( compound_interest ( principal = variable ( 10000. , name = \"principal\" , min = 0 ), annual_interest = variable ( 6 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ), years = variable ( 10 , name = \"years\" , min = 0 ) ), name = \"compound_interest\" ) print ( ci ) It's not necessarily prettier to look at, but also works!","title":"Compound interest"},{"location":"examples/compound_interest/#example-3-compound-interest","text":"The third example calculates the compound interest \\(C\\) starting from a principal value \\(P\\) with annualt interest \\(I\\) after \\(N\\) years: \\[ C = P \\cdot (1 + I)^N - P \\] Very common formula in anything related to finance. Again, we start with the plain code, as you would implement it right away: calc_compound_interest.py 1 2 3 4 5 6 7 8 def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal principal = 10_000 interest = 6 years = 10 ci = compound_interest ( principal , interest , years ) print ( f \"Compound interest after { years } years: { ci } \" ) You can run this example with $ python ci.py and it should print 7908.47 . Which is the compound interest after 10 years if you started with 10000 units that grow by 6% each year. There are several problems with this solution: 1. You need to change the code to run it for other inputs 2. Units are unclear! principal is a currency, but that actually does not matter. The real problem is the interest . Is it decimal or in %?","title":"Example 3: Compound interest"},{"location":"examples/compound_interest/#annotate-with-lmrtfy","text":"Using lmrtfy, you would annotate the script as follows: calc_compound_interest_lmrtfy.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from lmrtfy.annotation import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal principal = 10_000. interest = 6. years = 10 principal = variable ( principal , name = \"principal\" , min = 0 ) annual_interest = variable ( principal , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ) years = variable ( years , name = \"years\" , min = 0 ) ci = compound_interest ( principal , annual_interest , years ) ci = result ( ci , name = \"compound_interest\" ) Now, we run python calc_compound_interest_lmrtfy_1.py to generate the profile. Warning The type annotation are not enforced when run locally. LMRTFY checks the types and units only if jobs are submitted through its API. This guarantees that you can run your code without our service.","title":"Annotate with lmrtfy"},{"location":"examples/compound_interest/#deployment","text":"After creating the profile we can easily deploy with $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local","title":"Deployment"},{"location":"examples/compound_interest/#call-compound_interest-from-code","text":"Similar to the other examples we just need to import the catalog and call the correct function: call_compound_interest.py 1 2 3 4 5 6 7 8 9 10 11 12 from time import sleep from lmrtfy.functions import catalog job = catalog .< your_namespace >. calc_compound_interest_lmrtfy ( 5. , 10. , 5 ) # (1)! if job : print ( job . id , job . status ) while not job . ready : sleep ( 1. ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() .","title":"Call compound_interest from code"},{"location":"examples/compound_interest/#call-compound_interest-from-the-cli","text":"The output should be similar to this: INFO Profile_id to be used for requests: <profile_id> The <profile_id> is important to submit jobs. To submit a job you are currently required to save the input parameters as JSON (e.g. input.json ): input.json to calculate compound interest { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } Now, we have everything that is needed to start a job: $ lmrtfy submit <profile_id> input.json The job id for this job is printed to the terminal: INFO Job-id: <job_id> We need the <job_id> later to fetch the results from the computation.","title":"Call compound_interest from the CLI"},{"location":"examples/compound_interest/#alternative-annotation","text":"A more compact but working alternative is to create the result as follows: Alternative annotation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from lmrtfy.annotation import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : ci = result ( compound_interest ( principal = variable ( 10000. , name = \"principal\" , min = 0 ), annual_interest = variable ( 6 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ), years = variable ( 10 , name = \"years\" , min = 0 ) ), name = \"compound_interest\" ) print ( ci ) It's not necessarily prettier to look at, but also works!","title":"Alternative Annotation"},{"location":"examples/free_fall/","text":"Example 2: Velocity due to gravtity in free fall \u00b6 The second example calculates the velocity of an object falling from the sky (without air resistance). The standard gravity on earth is 9.81 m*s^(-2). Multiplicated by the fall time, we will get the velocity of the object after that time. If you like equations more, you might recognize these from your physics class: $$ v = g \\cdot t $$ In regular python code that you run locally it would look like this: free_fall.py 1 2 3 4 5 standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now we want to be able to share that functionality via the lmrtfy web API. All we have to do is decide which variables are considered to be an input and a result of the computation: free_fall_lmrtfy.py 1 2 3 4 5 6 7 from lmrtfy import variable , result standard_gravity = 9.81 time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) velocity = result ( standard_gravity * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m/s\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now run $ python examples/velocity_from_gravity/calc_velocity.py to generate the required profile. This way you can also check if your code is actually working the way you expect it. Deploying the script \u00b6 To deploy, you simply run $ lmrtfy deploy examples/velocity_from_gravity/calc_velocity.py --local . Do not stop that process, because than you will not be able to submit a job. Calling from code \u00b6 Calling free_fall_lmrtfy by code as easy as it was for the first example . calc_free_fall.py 1 2 3 4 5 6 7 8 9 10 import time from lmrtfy.functions import catalog job = catalog .< your_namespace >. free_fall_lmrtfy ( time = 100.0 ) # (1)! if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . Note You can also run help(free_fall_lmrtfy) to see the corresponding help. Right now, only the function signature is shown but in the future you will also be able to see the docstrings. Calling from CLI \u00b6 Open a new terminal in the same directory and run $ lmrtfy submit <profile_id> . The profile_id has been printed in the lmrtfy deploy step. This does not work right out of the box, because you need to specify a JSON file that contains the input parameters for your job. A template for that JSON should have been printed in the CLI. Create such a JSON file and name it input.json and put values of the correct type into the values (no type conversion is happening in the API, so if float is required, you cannot input an int ). Alternatively, use the provided input.json in examples/free_fall/input.json : { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } Now run $ lmrtfy submit <profile_id> examples/free_fall/input.json . You will receive a job_id which we will shortly need to fetch the results after they are computed. After your job has run, you can get the results by running $ lmrtfy fetch <job_id> <path to store results> . The results are downloaded and stored inside the specified path within a directory that has the job_id as its name.","title":"Free fall"},{"location":"examples/free_fall/#example-2-velocity-due-to-gravtity-in-free-fall","text":"The second example calculates the velocity of an object falling from the sky (without air resistance). The standard gravity on earth is 9.81 m*s^(-2). Multiplicated by the fall time, we will get the velocity of the object after that time. If you like equations more, you might recognize these from your physics class: $$ v = g \\cdot t $$ In regular python code that you run locally it would look like this: free_fall.py 1 2 3 4 5 standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now we want to be able to share that functionality via the lmrtfy web API. All we have to do is decide which variables are considered to be an input and a result of the computation: free_fall_lmrtfy.py 1 2 3 4 5 6 7 from lmrtfy import variable , result standard_gravity = 9.81 time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) velocity = result ( standard_gravity * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m/s\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now run $ python examples/velocity_from_gravity/calc_velocity.py to generate the required profile. This way you can also check if your code is actually working the way you expect it.","title":"Example 2: Velocity due to gravtity in free fall"},{"location":"examples/free_fall/#deploying-the-script","text":"To deploy, you simply run $ lmrtfy deploy examples/velocity_from_gravity/calc_velocity.py --local . Do not stop that process, because than you will not be able to submit a job.","title":"Deploying the script"},{"location":"examples/free_fall/#calling-from-code","text":"Calling free_fall_lmrtfy by code as easy as it was for the first example . calc_free_fall.py 1 2 3 4 5 6 7 8 9 10 import time from lmrtfy.functions import catalog job = catalog .< your_namespace >. free_fall_lmrtfy ( time = 100.0 ) # (1)! if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . Note You can also run help(free_fall_lmrtfy) to see the corresponding help. Right now, only the function signature is shown but in the future you will also be able to see the docstrings.","title":"Calling from code"},{"location":"examples/free_fall/#calling-from-cli","text":"Open a new terminal in the same directory and run $ lmrtfy submit <profile_id> . The profile_id has been printed in the lmrtfy deploy step. This does not work right out of the box, because you need to specify a JSON file that contains the input parameters for your job. A template for that JSON should have been printed in the CLI. Create such a JSON file and name it input.json and put values of the correct type into the values (no type conversion is happening in the API, so if float is required, you cannot input an int ). Alternatively, use the provided input.json in examples/free_fall/input.json : { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } Now run $ lmrtfy submit <profile_id> examples/free_fall/input.json . You will receive a job_id which we will shortly need to fetch the results after they are computed. After your job has run, you can get the results by running $ lmrtfy fetch <job_id> <path to store results> . The results are downloaded and stored inside the specified path within a directory that has the job_id as its name.","title":"Calling from CLI"},{"location":"examples/starting_example/","text":"Example 1: Simple annotation \u00b6 This is a simple example to showcase the general usage of lmrtfy. It can be found in examples/starting_example/example1.py . The two core concepts are the variable and result functions which annotate the inputs and outputs of the script. They are needed to create the profile which is used to create the API. example1.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from lmrtfy.annotation import variable , result # (1)! x = variable ( 5 , name = \"x\" , min = 1 , max = 10 ) # (2)! y = variable ( np . linspace ( 0. , 1. , 101 , dtype = np . float64 ), name = \"y\" , min =- 1. , max = 11. , unit = \"m\" ) # (3)! z = variable ( \"abc\" , name = \"z\" ) z1 = variable ([ \"abc\" , \"def\" ], name = \"z1\" ) # (4)! z2 = variable ([ \"abc\" , 1 , 1.1 ], name = \"z2\" ) z3 = variable ({ 'a' : \"abc\" , 'b' : 1 }, name = \"z3\" ) a = result ( x * y , name = \"a\" ) # (5)! b = result ( x * z , name = \"b\" ) The functions need to be imported from the lmrtfy library The variable x has the local value 5 and can be between 1 and 10. You can have numpy arrays as inputs Lists and dictionaries work, too! Results are similar to variables. They have a name and an expression that they will become. Run python examples/starting/example1.py to create the profile needed for the deployment. Deployment \u00b6 To deploy the script run lmrtfy deploy examples/starting/example1.py --local Call example1 from code \u00b6 Now you can simply call catalog.example1() with the correct arguments, and you are good to go: call_example1.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import time from lmrtfy.functions import catalog job = catalog .< your_namespace > example1 ( x = 1 , # (1)! y = [ 1 , 2.0 , 3.0 ], z = \"foobar\" , z1 = [ \"bar\" , \"foo\" ], z2 = [ \"foo\" , 1 , 42 ], z3 = { \"foo\" : \"bar\" , \"bar\" : \"foo\" } ) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . if job: is currently required to ensure that you actually got a job object back from the function which would not be the case if the submission failed. Calling example1 from the CLI \u00b6 Note We encourage you to use code to submit jobs and get results. During the deployment you should have received a seven letter profile_id : Profile_id to be used for requests: CgHUejl We need the profile_id to submit a job from the CLI: lmrtfy submit CgHUejL examples/starting/example1.json If the JSON file has the correct inputs, in a valid range with correct units you will see that the job submission was successful. You will receive a ten-letter job ID. INFO Job submission successful. INFO Job-id: HgaUbcTFah With this job_id you can now get the job results: lmrtfy fetch HgaUbcTFah . That's all that is to it. Happy Hacking!","title":"Starting example"},{"location":"examples/starting_example/#example-1-simple-annotation","text":"This is a simple example to showcase the general usage of lmrtfy. It can be found in examples/starting_example/example1.py . The two core concepts are the variable and result functions which annotate the inputs and outputs of the script. They are needed to create the profile which is used to create the API. example1.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from lmrtfy.annotation import variable , result # (1)! x = variable ( 5 , name = \"x\" , min = 1 , max = 10 ) # (2)! y = variable ( np . linspace ( 0. , 1. , 101 , dtype = np . float64 ), name = \"y\" , min =- 1. , max = 11. , unit = \"m\" ) # (3)! z = variable ( \"abc\" , name = \"z\" ) z1 = variable ([ \"abc\" , \"def\" ], name = \"z1\" ) # (4)! z2 = variable ([ \"abc\" , 1 , 1.1 ], name = \"z2\" ) z3 = variable ({ 'a' : \"abc\" , 'b' : 1 }, name = \"z3\" ) a = result ( x * y , name = \"a\" ) # (5)! b = result ( x * z , name = \"b\" ) The functions need to be imported from the lmrtfy library The variable x has the local value 5 and can be between 1 and 10. You can have numpy arrays as inputs Lists and dictionaries work, too! Results are similar to variables. They have a name and an expression that they will become. Run python examples/starting/example1.py to create the profile needed for the deployment.","title":"Example 1: Simple annotation"},{"location":"examples/starting_example/#deployment","text":"To deploy the script run lmrtfy deploy examples/starting/example1.py --local","title":"Deployment"},{"location":"examples/starting_example/#call-example1-from-code","text":"Now you can simply call catalog.example1() with the correct arguments, and you are good to go: call_example1.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import time from lmrtfy.functions import catalog job = catalog .< your_namespace > example1 ( x = 1 , # (1)! y = [ 1 , 2.0 , 3.0 ], z = \"foobar\" , z1 = [ \"bar\" , \"foo\" ], z2 = [ \"foo\" , 1 , 42 ], z3 = { \"foo\" : \"bar\" , \"bar\" : \"foo\" } ) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . if job: is currently required to ensure that you actually got a job object back from the function which would not be the case if the submission failed.","title":"Call example1 from code"},{"location":"examples/starting_example/#calling-example1-from-the-cli","text":"Note We encourage you to use code to submit jobs and get results. During the deployment you should have received a seven letter profile_id : Profile_id to be used for requests: CgHUejl We need the profile_id to submit a job from the CLI: lmrtfy submit CgHUejL examples/starting/example1.json If the JSON file has the correct inputs, in a valid range with correct units you will see that the job submission was successful. You will receive a ten-letter job ID. INFO Job submission successful. INFO Job-id: HgaUbcTFah With this job_id you can now get the job results: lmrtfy fetch HgaUbcTFah . That's all that is to it. Happy Hacking!","title":"Calling example1 from the CLI"},{"location":"rtd/","text":"The official documentation can be found here: Documenation \u00b6 LMRTFY stands for Let Me Run That For You . \u2705 Create functions that run in the cloud , on your servers or even on your laptop . \u2705 Call them from code that runs somewhere else, just like a regular function . \u2705 Share functions with friends and colleagues, track their usage and monetize their usage. \u2705 Works with Python, but more languages will be added in the future. Quickstart Guide Tutorial Examples API Reference How to report issues and how to contribute Introduction \u00b6 LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possible to use LMRTFY Call deployed function like any other function provided by a local library Quickstart - TL;DR \u00b6 install with pip install lmrtfy login/sign up with lmrtfy login annotate your code's inputs with variable and its outputs with result deploy the script: lmrtfy deploy examples/deployment/calc_compound_interest.py --local Use the deployed function (from another terminal, or another computer!): open examples/calling_cloud_functions/call_function.py run python examples/call_deployed_function.py to call the deployed function and get the results. As you can see in step 5, it's as simple as calling a regular function from any other library you have installed locally. Examples \u00b6 The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one! License \u00b6","title":"Index"},{"location":"rtd/#the-official-documentation-can-be-found-here-documenation","text":"LMRTFY stands for Let Me Run That For You . \u2705 Create functions that run in the cloud , on your servers or even on your laptop . \u2705 Call them from code that runs somewhere else, just like a regular function . \u2705 Share functions with friends and colleagues, track their usage and monetize their usage. \u2705 Works with Python, but more languages will be added in the future. Quickstart Guide Tutorial Examples API Reference How to report issues and how to contribute","title":"The official documentation can be found here: Documenation"},{"location":"rtd/#introduction","text":"LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possible to use LMRTFY Call deployed function like any other function provided by a local library","title":"Introduction"},{"location":"rtd/#quickstart-tldr","text":"install with pip install lmrtfy login/sign up with lmrtfy login annotate your code's inputs with variable and its outputs with result deploy the script: lmrtfy deploy examples/deployment/calc_compound_interest.py --local Use the deployed function (from another terminal, or another computer!): open examples/calling_cloud_functions/call_function.py run python examples/call_deployed_function.py to call the deployed function and get the results. As you can see in step 5, it's as simple as calling a regular function from any other library you have installed locally.","title":"Quickstart - TL;DR"},{"location":"rtd/#examples","text":"The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one!","title":"Examples"},{"location":"rtd/#license","text":"","title":"License"},{"location":"user_guide/installation/","text":"There are two ways to install the lmrtfy . We recommend the usage of virtual environments at the moment due to the frequent updates and changes of lmrtfy. Linux and MacOS \u00b6 We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy Windows \u00b6 On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy This way you will always have the most recent release of lmrtfy . Install from Source \u00b6 You can also install lmrtfy from source which is the best way to use the nightly features. Be aware that this might not always work. Issues are to be expected. Clone the git repository and install manually: $ git clone --branch main https://github.com/lmrtfy/lmrtfy.git $ cd lmrtfy $ pip install . The main branch is the release branch and should always work with the LMRTFY API. Alternatively, you can use the develop branch. This should be the most up-to-date branch in the repository, but things might break. So be careful while using the develop branch.","title":"Installation"},{"location":"user_guide/installation/#linux-and-macos","text":"We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy","title":"Linux and MacOS"},{"location":"user_guide/installation/#windows","text":"On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy This way you will always have the most recent release of lmrtfy .","title":"Windows"},{"location":"user_guide/installation/#install-from-source","text":"You can also install lmrtfy from source which is the best way to use the nightly features. Be aware that this might not always work. Issues are to be expected. Clone the git repository and install manually: $ git clone --branch main https://github.com/lmrtfy/lmrtfy.git $ cd lmrtfy $ pip install . The main branch is the release branch and should always work with the LMRTFY API. Alternatively, you can use the develop branch. This should be the most up-to-date branch in the repository, but things might break. So be careful while using the develop branch.","title":"Install from Source"},{"location":"user_guide/login/","text":"To use LMRTFY we need you to sign up with us to authenticate with our API. This step is necessary to prevent malicious activity. You can use one of our social logins or just sign up with email and password. With your account you can to the following things: deploy scripts as a callable function submit jobs and call cloud functions access the LMRTFY Management Board Sign Up \u00b6 Before you can login the first time you need to sign up. Currently, you have three options to do that: email address + password GitHub account Google account If you sign up with email/password you will be required to verify your email address. Login \u00b6 When you run an LMRTFY actions that require a token you will automatically be asked to login. Each token is valid for 10 hours before you need to login again. If you allow cookies our authentication provider Auth0 will recognize you. If you use a headless setup (server, google colab or other web-based tools) or if you have long-term deployments you should use our token-based authentication for deployment and job submission","title":"Sign Up/Login"},{"location":"user_guide/login/#sign-up","text":"Before you can login the first time you need to sign up. Currently, you have three options to do that: email address + password GitHub account Google account If you sign up with email/password you will be required to verify your email address.","title":"Sign Up"},{"location":"user_guide/login/#login","text":"When you run an LMRTFY actions that require a token you will automatically be asked to login. Each token is valid for 10 hours before you need to login again. If you allow cookies our authentication provider Auth0 will recognize you. If you use a headless setup (server, google colab or other web-based tools) or if you have long-term deployments you should use our token-based authentication for deployment and job submission","title":"Login"},{"location":"user_guide/namespaces/","text":"In version 0.0.10 lrmtfy introduced namespaces to give users more flexibility and to make sharing easier. Every user has a default namespace that corresponds to their username and a short suffix depending on the kind of login that you use. You can see available namespace when you import the catalog. The best way to manage namespaces is to start an ipython session. For me the output is the following: Importing catalog 1 2 3 4 5 In [ 1 ]: from lmrtfy.functions import catalog 2022 - 10 - 11 16 : 57 : 03 [ 58002 ] INFO Available namespaces : [ 'examples' , 'orgarten_gh' ] 2022 - 10 - 11 16 : 57 : 04 [ 58002 ] INFO Added function : catalog . examples . free_fall_lmrtfy 2022 - 10 - 11 16 : 57 : 05 [ 58002 ] INFO Added function : catalog . orgarten_gh . calc_compound_interest 2022 - 10 - 11 16 : 57 : 05 [ 58002 ] INFO Added function : catalog . orgarten_gh . free_fall_lmrtfy Namespaces available to me are my private one orgarten_gh and the example namespace examples . Also shown are the available functions in my catalog. To use a deployed function inside the namespace I would simply call Using a function in a namespace 1 catalog . orgarten_gh .< function > ( ... ) If the code editor or IDE of your choice supports auto-completion, the available functions should be suggested. Create namespaces \u00b6 You can create new namespaces inside your own user namespace: Creating new namespaces 1 2 3 In [ 2 ]: catalog . create_namespace ( catalog . orgarten_gh , \"shared\" ) 2022 - 10 - 11 18 : 27 : 27 [ 58002 ] INFO { 'namespaces' : [ 'orgarten_gh' , 'orgarten_gh/shared' ]} To add a function inside a namespace you just run add_function_to_namespace . Add a function to a namespace 1 2 In [ 3 ]: catalog . add_function_to_namespace ( catalog . orgarten_gh . shared , ... : catalog . orgarten_gh . calc_compound_interest ) Now you can share the namespace with other users!","title":"Namespaces"},{"location":"user_guide/namespaces/#create-namespaces","text":"You can create new namespaces inside your own user namespace: Creating new namespaces 1 2 3 In [ 2 ]: catalog . create_namespace ( catalog . orgarten_gh , \"shared\" ) 2022 - 10 - 11 18 : 27 : 27 [ 58002 ] INFO { 'namespaces' : [ 'orgarten_gh' , 'orgarten_gh/shared' ]} To add a function inside a namespace you just run add_function_to_namespace . Add a function to a namespace 1 2 In [ 3 ]: catalog . add_function_to_namespace ( catalog . orgarten_gh . shared , ... : catalog . orgarten_gh . calc_compound_interest ) Now you can share the namespace with other users!","title":"Create namespaces"},{"location":"user_guide/troubleshooting/","text":"If you encounter any problems you can always contact us via hello@lmrt.fyi . Alternatively, you can also open an issue on GitHub . How can I get debug output? \u00b6 Set LMRTFY_DEBUG = \"1\" before running any lmrtfy commands. My deployed function does not receive any jobs that I submit! \u00b6 There are a multiple possible explanations depending on what exactly you see I can retrieve results, even though my deployed has not done anything \u00b6 Please check if you ran lmrtfy deploy for the same file in another session. If multiple runners serve your function only one of them will receive the same call. The next call to the same function will be routed to the next runner in the line. The job status is reported UNKNOWN \u00b6 If that is the case, please let us know at hello@lmrt.fyi or in the GitHub issues. This is likely a problem on our side. I need help to implement a specific use case \u00b6 Please don't hesitate to contact us via hello@lmrt.fyi . We are always interested in new use cases.","title":"Troubleshooting"},{"location":"user_guide/troubleshooting/#how-can-i-get-debug-output","text":"Set LMRTFY_DEBUG = \"1\" before running any lmrtfy commands.","title":"How can I get debug output?"},{"location":"user_guide/troubleshooting/#my-deployed-function-does-not-receive-any-jobs-that-i-submit","text":"There are a multiple possible explanations depending on what exactly you see","title":"My deployed function does not receive any jobs that I submit!"},{"location":"user_guide/troubleshooting/#i-can-retrieve-results-even-though-my-deployed-has-not-done-anything","text":"Please check if you ran lmrtfy deploy for the same file in another session. If multiple runners serve your function only one of them will receive the same call. The next call to the same function will be routed to the next runner in the line.","title":"I can retrieve results, even though my deployed has not done anything"},{"location":"user_guide/troubleshooting/#the-job-status-is-reported-unknown","text":"If that is the case, please let us know at hello@lmrt.fyi or in the GitHub issues. This is likely a problem on our side.","title":"The job status is reported UNKNOWN"},{"location":"user_guide/troubleshooting/#i-need-help-to-implement-a-specific-use-case","text":"Please don't hesitate to contact us via hello@lmrt.fyi . We are always interested in new use cases.","title":"I need help to implement a specific use case"},{"location":"user_guide/calling_functions/fetch_results/","text":"Fetching results in your code \u00b6 When you call a function from your code, the results are part of the job object created when calling the function . You can only get the results when they are ready by using job.results : while not job . ready : sleep ( 1. ) print ( job . results ) Currently, the while loop is necessary to wait for the results. If the job has not been submitted for some reason the job.ready query triggers automatic resubmission. This could easily become a future (in the sense of concurrent programming) later on. We are also looking for feedback, what would work best for you. Get results with the CLI \u00b6 LMRTFY also provides a way to download the results of the computation. All you need is the <job_id> that you received when you submitted the job. Then, you simply run $ lmrtfy fetch <job_id> <save_path> The results will be saved in <save_path>/<job_id>/.. . Each result is currently saved as a JSON file with the following format: { \"<var_name>\" : < value > } Each variable has its own file.","title":"Get results"},{"location":"user_guide/calling_functions/fetch_results/#fetching-results-in-your-code","text":"When you call a function from your code, the results are part of the job object created when calling the function . You can only get the results when they are ready by using job.results : while not job . ready : sleep ( 1. ) print ( job . results ) Currently, the while loop is necessary to wait for the results. If the job has not been submitted for some reason the job.ready query triggers automatic resubmission. This could easily become a future (in the sense of concurrent programming) later on. We are also looking for feedback, what would work best for you.","title":"Fetching results in your code"},{"location":"user_guide/calling_functions/fetch_results/#get-results-with-the-cli","text":"LMRTFY also provides a way to download the results of the computation. All you need is the <job_id> that you received when you submitted the job. Then, you simply run $ lmrtfy fetch <job_id> <save_path> The results will be saved in <save_path>/<job_id>/.. . Each result is currently saved as a JSON file with the following format: { \"<var_name>\" : < value > } Each variable has its own file.","title":"Get results with the CLI"},{"location":"user_guide/calling_functions/submission/","text":"Calling a function from code \u00b6 Our goal is to provide an interface to deployed functions that works just like any other function in any other library that you have locally installed. We provide an example call_free_fall.py in the same directory as the free_fall_lmrtfy.py script. As you can see, calling a remote function via LMRTFY feels just like calling a native function. call_free_fall.py 1 2 3 4 5 6 7 8 9 10 11 12 from time import sleep from lmrtfy import catalog # (1)! job = catalog . examples . free_fall_lmrtfy ( time = 100. ) # (2)! if job : print ( job . id , job . status ) while not job . ready : # (3)! sleep ( 1. ) print ( job . results ) # (4)! Importing the catalog triggers the catalog update to get newly deployed functions every time you run the code. Our examples are all part of the catalog in the catalog.examples namespace. The function free_fall_lmrtfy is now part of the catalog and can be called just like a normal function in your code. The function is not executed in the same context as your Python interpreter. Instead it is run in one of the runners. Loop until the job is ready. In this context ready means that the results are ready to be fetched. Fetching the results is as simple as calling job.results . The return value is a dictionary with the keys corresponding to the names of the results and the values are the actual values of the result Run the script with your local python interpreter python call_free_fall.py . It runs just like a regular script but calls a remote function inside. The output of the script looks like this: INFO Validating auth token. INFO Auth token accepted. INFO Valid access token found. Login not necessary. INFO Updated function catalog. INFO Added function free_fall_lmrtfy. INFO Job CLuz1ZrpR7 created. Status is RUNNING. # (1)! CLuz1ZrpR7 JobStatus.RUNNING { 'velocity' : 981 .0 } # (2)! The job ID is always a 10-character long ID. The reported status is sometimes UNKNOWN which usually means that the LMRTFY platform has not processed the job yet. The result of the computation is a dictionary with the variable names as keys and the actual value as values. The ID of the job is going to be different from the one shown in the example output. Job IDs are always 10 characters long. Using the CLI \u00b6 LMRTFY also provides a way to submit jobs with the lmrtfy CLI tool. All you need for this is a profile_id (7 characters long) which is provided by you during the deployment and a JSON file that contains the input parameters. Attention This is a good way to call deployed scripts from another language as you can always build the JSON file and call the lmrtfy CLI. If you are using it this way, please contact us. We want to provide more native-feeling interfaces to languages other than python as well but would love to hear what you use to prioritize. For the example calculating the compound interest, the JSON file would look like this: { \"argument_values\" : { \"time\" : 100 }, \"argument_units\" : { \"time\" : \"s\" } } argument_values and argument_units contain a key-value pair each for each of the inputs in the annotation profile. The types need to match exactly. No implicit type casting in performed during the submission. The unit also has to match exactly. Save the JSON file es input.json and run: $ lmrtfy submit <profile_id> input.json Info Later on, we might perform automatic conversion in case of a unit mismatch, e.g. if the profile requires s (as in seconds) but the input is given as h (as in hours). There will be an option to enable/disable the function. If you have any opinions about that, please let us know When you submit your job you will receive a job_id which is needed to fetch the results as you will see in the next part of this guide.","title":"Call a function"},{"location":"user_guide/calling_functions/submission/#calling-a-function-from-code","text":"Our goal is to provide an interface to deployed functions that works just like any other function in any other library that you have locally installed. We provide an example call_free_fall.py in the same directory as the free_fall_lmrtfy.py script. As you can see, calling a remote function via LMRTFY feels just like calling a native function. call_free_fall.py 1 2 3 4 5 6 7 8 9 10 11 12 from time import sleep from lmrtfy import catalog # (1)! job = catalog . examples . free_fall_lmrtfy ( time = 100. ) # (2)! if job : print ( job . id , job . status ) while not job . ready : # (3)! sleep ( 1. ) print ( job . results ) # (4)! Importing the catalog triggers the catalog update to get newly deployed functions every time you run the code. Our examples are all part of the catalog in the catalog.examples namespace. The function free_fall_lmrtfy is now part of the catalog and can be called just like a normal function in your code. The function is not executed in the same context as your Python interpreter. Instead it is run in one of the runners. Loop until the job is ready. In this context ready means that the results are ready to be fetched. Fetching the results is as simple as calling job.results . The return value is a dictionary with the keys corresponding to the names of the results and the values are the actual values of the result Run the script with your local python interpreter python call_free_fall.py . It runs just like a regular script but calls a remote function inside. The output of the script looks like this: INFO Validating auth token. INFO Auth token accepted. INFO Valid access token found. Login not necessary. INFO Updated function catalog. INFO Added function free_fall_lmrtfy. INFO Job CLuz1ZrpR7 created. Status is RUNNING. # (1)! CLuz1ZrpR7 JobStatus.RUNNING { 'velocity' : 981 .0 } # (2)! The job ID is always a 10-character long ID. The reported status is sometimes UNKNOWN which usually means that the LMRTFY platform has not processed the job yet. The result of the computation is a dictionary with the variable names as keys and the actual value as values. The ID of the job is going to be different from the one shown in the example output. Job IDs are always 10 characters long.","title":"Calling a function from code"},{"location":"user_guide/calling_functions/submission/#using-the-cli","text":"LMRTFY also provides a way to submit jobs with the lmrtfy CLI tool. All you need for this is a profile_id (7 characters long) which is provided by you during the deployment and a JSON file that contains the input parameters. Attention This is a good way to call deployed scripts from another language as you can always build the JSON file and call the lmrtfy CLI. If you are using it this way, please contact us. We want to provide more native-feeling interfaces to languages other than python as well but would love to hear what you use to prioritize. For the example calculating the compound interest, the JSON file would look like this: { \"argument_values\" : { \"time\" : 100 }, \"argument_units\" : { \"time\" : \"s\" } } argument_values and argument_units contain a key-value pair each for each of the inputs in the annotation profile. The types need to match exactly. No implicit type casting in performed during the submission. The unit also has to match exactly. Save the JSON file es input.json and run: $ lmrtfy submit <profile_id> input.json Info Later on, we might perform automatic conversion in case of a unit mismatch, e.g. if the profile requires s (as in seconds) but the input is given as h (as in hours). There will be an option to enable/disable the function. If you have any opinions about that, please let us know When you submit your job you will receive a job_id which is needed to fetch the results as you will see in the next part of this guide.","title":"Using the CLI"},{"location":"user_guide/calling_functions/submission_tokens/","text":"If you run lmrtfy with a headless setup (Raspberry Pi, server, google colab, ..) you cannot use lmrtfy login to authenticate. If you are running a web frontend that calls an LMRTFY function you can use The following listing creates a token for a <funtion> that is available in <namespace> . Submit tokens can be issued for any function that you can call. IPython listing 1 2 3 4 5 6 7 In [ 1 ]: from lmrtfy.functions import catalog Out [ 2 ]: In [ 2 ]: catalog . issue_submit_token ( catatlog .< namespace >.< function > ) Out [ 2 ]: { 'token_id' : '4HLTgo9bKhaK6kvv2dYA' , 'token' : 'LMRTFY...' } The token always starts with LMRTFY . The token_id is needed to revoke the deployment key when you don't want to use it anymore. Keep the deployment token secret. In order to use deployment key you need to set the LMRTFY_ACCESS_TOKEN environment variable: Using the deployment token $ LMRTFY_ACCESS_TOKEN = \"LMRTFY...\" ipython In [ 1 ] : from lmrtfy.functions import catalog Out [ 1 ] : ... In [ 2 ] : job = catalog.<namespace>.< function > ( ... ) This way you can use lmrtfy on any platform, that does not allow a regular login. To revoke the submit key, you just need to do the following: Revoke the key 1 2 In [ 3 ]: catalog . revoke_token ( < token_id > ) Out [ 3 ]: True If the revocation failed, it will raise an error. Note Tokens will be available in the web app.","title":"Using a submit token"},{"location":"user_guide/creating_functions/annotation/","text":"Now, we have seen how we can call a deployed function from our code, but how do we deploy something ourselves? This requires two main steps: You need to annotate your script to let LMRTFY know about the input variables and the results of your script. The actual deployment of your script. Annotate your script \u00b6 The annotation of your script tells the lmrtfy tool which python variables are considered inputs and outputs, which is done via the variable and results functions. This step is important, because lmrtfy traces the calls to variable and result to create a profile for the code. This profile includes the inputs and outputs as well as the additional meta information ( min , max , unit , and possibly more in the future). Let's assume that you have create a script to calculate the velocity of an object after a certain time: free_fall.py 1 2 3 4 5 standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now, if you want to recalculate for a different time, you would edit the script and run it again. While this might work for a small script like this, this becomes tedious if you have different input variables and want others to use your script easily, too. Let's change the script in such a way that lmrtfy can create a profile which can be used to deploy the function and make it available to other users: free_fall_lmrtfy.py 1 2 3 4 5 6 7 8 9 from lmrtfy.annotation import variable , result # (1)! time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # (2)! standard_gravity = 9.81 velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) velocity = result ( velocity , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) # (3)! variable and result are the imports that are needed to annotate your code to make it work with LMRTFY. variable annotates any inputs of your script. result annotates a result of script. A script can have multiple results. If you run python free_fall_lmrtfy.py you get the exact same result as before. During the run, lmrtfy created the profile for free_fall_lmrtfy.py which will be needed to deploy the function. Create the annotation profile \u00b6 It is required to run your script at least once with the regular python interpreter to create the annotation profile which will be used to generate the API. $ python <script.py> The profile is currently saved under ~/.lmrtfy/profiles . The profile is necessary for the deployment, and is human-readable; however, there should be no need to check the created profile unless for troubleshooting","title":"Annotation"},{"location":"user_guide/creating_functions/annotation/#annotate-your-script","text":"The annotation of your script tells the lmrtfy tool which python variables are considered inputs and outputs, which is done via the variable and results functions. This step is important, because lmrtfy traces the calls to variable and result to create a profile for the code. This profile includes the inputs and outputs as well as the additional meta information ( min , max , unit , and possibly more in the future). Let's assume that you have create a script to calculate the velocity of an object after a certain time: free_fall.py 1 2 3 4 5 standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now, if you want to recalculate for a different time, you would edit the script and run it again. While this might work for a small script like this, this becomes tedious if you have different input variables and want others to use your script easily, too. Let's change the script in such a way that lmrtfy can create a profile which can be used to deploy the function and make it available to other users: free_fall_lmrtfy.py 1 2 3 4 5 6 7 8 9 from lmrtfy.annotation import variable , result # (1)! time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # (2)! standard_gravity = 9.81 velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) velocity = result ( velocity , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) # (3)! variable and result are the imports that are needed to annotate your code to make it work with LMRTFY. variable annotates any inputs of your script. result annotates a result of script. A script can have multiple results. If you run python free_fall_lmrtfy.py you get the exact same result as before. During the run, lmrtfy created the profile for free_fall_lmrtfy.py which will be needed to deploy the function.","title":"Annotate your script"},{"location":"user_guide/creating_functions/annotation/#create-the-annotation-profile","text":"It is required to run your script at least once with the regular python interpreter to create the annotation profile which will be used to generate the API. $ python <script.py> The profile is currently saved under ~/.lmrtfy/profiles . The profile is necessary for the deployment, and is human-readable; however, there should be no need to check the created profile unless for troubleshooting","title":"Create the annotation profile"},{"location":"user_guide/creating_functions/deployment/","text":"Deploying the function (local runner) \u00b6 Now you can deploy the function and make it available via the LMRTFY API. This is simply done by running $ lmrtfy deploy <path_to_script.py> --local The --local flag means that the script will run locally on your computer and waits for jobs from the outside. The LMRTFY API only allows job submissions that fit your deployed annotation profile. In the future you will be able to deploy directly to the cloud. Then, you do not have to host the runner yourself. Warning Don't change the script after you have deployed it. The current advice would be to copy and rename the script before deployment. In later versions, this will be taken care of by the lmrtfy tool. Example \u00b6 Let's assume that you want to deploy the example script free_fall_lmrtfy.py provided in examples/free_fall/free_fall_lmrtfy.py . This is simple done by running $ cd examples/free_fall $ python free_fall_lmrtfy.py # (1)! $ lmrtfy deploy free_fall_lmrtfy.py --local This step is necessary to create the annotation profile, but testing your script locally is likely part of your development process. Therefore, often this step is not needed. Running lmrtfy deploy ... --local does two things: Register the function with name free_fall_lmrtfy in your LMRTFY catalog . If the function already exists it will be replaced if the input and output signatures have not changed. Otherwise, the profile will be rejected. You can force the replacement with the --force-replacement flag. Start a runner on your local system that takes the submitted jobs and executes them. When you stop the runner you won't receive any results from calling the function. When a job is submitted the types of the job's input parameters are checked by the LMRTFY API. Furthermore, they are also checked for their bounds and their units. This way, only jobs that can be run successfully with your script will get executed by it. Deploying to the cloud \u00b6 LMRTFY is currently not able to deploy your scripts directly to the cloud; however, you can do this manually by using lmrtfy inside of a docker container. The docker container can be run on your laptop, one of your own servers or in the cloud. The current workaround would be to use lmrtfy on a server or inside a docker container which can be hosted in the cloud. Inside the docker container you run the same command as if you were to run locally. Easy cloud deployment will be added in the future.","title":"Deployment"},{"location":"user_guide/creating_functions/deployment/#deploying-the-function-local-runner","text":"Now you can deploy the function and make it available via the LMRTFY API. This is simply done by running $ lmrtfy deploy <path_to_script.py> --local The --local flag means that the script will run locally on your computer and waits for jobs from the outside. The LMRTFY API only allows job submissions that fit your deployed annotation profile. In the future you will be able to deploy directly to the cloud. Then, you do not have to host the runner yourself. Warning Don't change the script after you have deployed it. The current advice would be to copy and rename the script before deployment. In later versions, this will be taken care of by the lmrtfy tool.","title":"Deploying the function (local runner)"},{"location":"user_guide/creating_functions/deployment/#example","text":"Let's assume that you want to deploy the example script free_fall_lmrtfy.py provided in examples/free_fall/free_fall_lmrtfy.py . This is simple done by running $ cd examples/free_fall $ python free_fall_lmrtfy.py # (1)! $ lmrtfy deploy free_fall_lmrtfy.py --local This step is necessary to create the annotation profile, but testing your script locally is likely part of your development process. Therefore, often this step is not needed. Running lmrtfy deploy ... --local does two things: Register the function with name free_fall_lmrtfy in your LMRTFY catalog . If the function already exists it will be replaced if the input and output signatures have not changed. Otherwise, the profile will be rejected. You can force the replacement with the --force-replacement flag. Start a runner on your local system that takes the submitted jobs and executes them. When you stop the runner you won't receive any results from calling the function. When a job is submitted the types of the job's input parameters are checked by the LMRTFY API. Furthermore, they are also checked for their bounds and their units. This way, only jobs that can be run successfully with your script will get executed by it.","title":"Example"},{"location":"user_guide/creating_functions/deployment/#deploying-to-the-cloud","text":"LMRTFY is currently not able to deploy your scripts directly to the cloud; however, you can do this manually by using lmrtfy inside of a docker container. The docker container can be run on your laptop, one of your own servers or in the cloud. The current workaround would be to use lmrtfy on a server or inside a docker container which can be hosted in the cloud. Inside the docker container you run the same command as if you were to run locally. Easy cloud deployment will be added in the future.","title":"Deploying to the cloud"},{"location":"user_guide/creating_functions/deployment_tokens/","text":"If you run lmrtfy with a headless setup (Raspberry Pi, server, google colab, ..) you cannot use lmrtfy login to authenticate. To mitigate this issue you can create a deployment token from the catalog. The deployment token is valid for the function that has been specified. The following listing creates a token for <funtion> in <namespace> . This can be any function available owned by you. IPython listing 1 2 3 4 5 6 7 In [ 1 ]: from lmrtfy.functions import catalog Out [ 2 ]: In [ 2 ]: catalog . issue_deploy_token ( catatlog .< namespace >.< function > ) Out [ 2 ]: { 'token_id' : '4HLTgo9bKhaK6kvv2dYA' , 'token' : 'LMRTFY...' } The token always starts with LMRTFY . The token_id is needed to revoke the deployment key when you don't want to use it anymore. Keep the deployment token secret. In order to use deployment key you need to set the LMRTFY_ACCESS_TOKEN environment variable: Using the deployment token $ LMRTFY_ACCESS_TOKEN = \"LMRTFY...\" lmrtfy deploy <script.py> --local --namespace = \"<namespace>\" This way you can use lmrtfy on any platform, that does not allow a regular login. <namespace> is the same namespace that is used in the first listing. To revoke the deployment key, you just need to do the following: Revoke the key 1 2 In [ 3 ]: catalog . revoke_token ( < token_id > ) Out [ 3 ]: True Note Tokens will be available in the web app.","title":"Using deploy tokens"},{"location":"user_guide/creating_functions/variables_and_results/","text":"The API reference with the corresponding function signatures for variable and result can be found here . In this part we want to take a closer look at some details. Transparency \u00b6 The annotation functions variable and result are transparent for the Python interpreter. Let's say we have the following code: 1 2 3 4 5 x = variable ( 5. , name = \"x\" ) tmp = heavy_computation ( x ) y = result ( tmp , name = \"y\" ) To the Python interpreter the code looks more or less like this: 1 2 3 4 5 x = 5 tmp = heavy_computation ( x ) y = tmp The calls to variable and result always return the value itself. In the background, these calls create the annotation which is necessary for the LMRTFY platform. When a function is deployed the variable functions injects the input arguments of the job received through the LMRTFY platform into the script. The result function on the other hand funnels the result back to the LMRTFY platform which makes it available to the caller of the function. Checking the validity of input arguments \u00b6 If you checked the API reference you may have noticed that variable and result have some additional parameters: min , max , and unit . Even better, through the first argument, the actual value, you also create type information that allows us to thoroughly check incoming job submissions. Type checking \u00b6 The input type is inferred from the value that is used in the variable function call. function call inferred type of x accepted type with LMRTFY x = variable ( 5 , name = \"x\" ) int int x = variable ( 5. , name = \"x\" ) float float x = variable ([ 5 , 2 ], name = \"x\" ) int_array list [ int ], ndarray [ int ] x = variable ([ 5. , 2. ], name = \"x\" ) float_array list [ float ], ndarray [ float ] x = variable ( \"abc\" , name = \"x\" ) str str x = variable ([ \"abc\" , \"def\" ], name = \"x\" ) str_array list [ str ] x = variable ([ \"abd\" , 1 , 1.1 ], name = \"x\" ) json list [ any ] x = variable ({ \"a\" : \"a\" , \"b\" : 1 }, name = \"x\" ) json dict If the input arguments submitted via the catalog do not match the accepted type, we reject the job and tell the caller to fix their types. We chose a strict type checking because a type has its meaning which it might lose during automatic conversion. Bounds checking \u00b6 The min and max parameters limit the range in which numeric input variables are seen as valid. This is especially useful if your algorithm has known and well-defined limitations. If it only works between \\(0\\) and \\(1000\\) you can simply specify variable ( 500 , name = \"a\" , min = 0 , max = 1000 ) . When a job is submitted via the LMRTFY platform, these bounds are checked. If the input variable a is out of bounds we notify the caller and reject the job. Unit checking \u00b6 Another useful thing is the annotation with an actual unit for the variable. Currently, this is done via str but we will switch to pint units in later releases. Similar to the numeric bounds the units are checked during the job submission and the job is rejected if the unit do not match. Example In 1999 the NASA lost its Mars Climate Orbiter due to a navigation error, which was caused by a failure to convert units from the imperial system to the metric system .","title":"Variables and results"},{"location":"user_guide/creating_functions/variables_and_results/#transparency","text":"The annotation functions variable and result are transparent for the Python interpreter. Let's say we have the following code: 1 2 3 4 5 x = variable ( 5. , name = \"x\" ) tmp = heavy_computation ( x ) y = result ( tmp , name = \"y\" ) To the Python interpreter the code looks more or less like this: 1 2 3 4 5 x = 5 tmp = heavy_computation ( x ) y = tmp The calls to variable and result always return the value itself. In the background, these calls create the annotation which is necessary for the LMRTFY platform. When a function is deployed the variable functions injects the input arguments of the job received through the LMRTFY platform into the script. The result function on the other hand funnels the result back to the LMRTFY platform which makes it available to the caller of the function.","title":"Transparency"},{"location":"user_guide/creating_functions/variables_and_results/#checking-the-validity-of-input-arguments","text":"If you checked the API reference you may have noticed that variable and result have some additional parameters: min , max , and unit . Even better, through the first argument, the actual value, you also create type information that allows us to thoroughly check incoming job submissions.","title":"Checking the validity of input arguments"},{"location":"user_guide/creating_functions/variables_and_results/#type-checking","text":"The input type is inferred from the value that is used in the variable function call. function call inferred type of x accepted type with LMRTFY x = variable ( 5 , name = \"x\" ) int int x = variable ( 5. , name = \"x\" ) float float x = variable ([ 5 , 2 ], name = \"x\" ) int_array list [ int ], ndarray [ int ] x = variable ([ 5. , 2. ], name = \"x\" ) float_array list [ float ], ndarray [ float ] x = variable ( \"abc\" , name = \"x\" ) str str x = variable ([ \"abc\" , \"def\" ], name = \"x\" ) str_array list [ str ] x = variable ([ \"abd\" , 1 , 1.1 ], name = \"x\" ) json list [ any ] x = variable ({ \"a\" : \"a\" , \"b\" : 1 }, name = \"x\" ) json dict If the input arguments submitted via the catalog do not match the accepted type, we reject the job and tell the caller to fix their types. We chose a strict type checking because a type has its meaning which it might lose during automatic conversion.","title":"Type checking"},{"location":"user_guide/creating_functions/variables_and_results/#bounds-checking","text":"The min and max parameters limit the range in which numeric input variables are seen as valid. This is especially useful if your algorithm has known and well-defined limitations. If it only works between \\(0\\) and \\(1000\\) you can simply specify variable ( 500 , name = \"a\" , min = 0 , max = 1000 ) . When a job is submitted via the LMRTFY platform, these bounds are checked. If the input variable a is out of bounds we notify the caller and reject the job.","title":"Bounds checking"},{"location":"user_guide/creating_functions/variables_and_results/#unit-checking","text":"Another useful thing is the annotation with an actual unit for the variable. Currently, this is done via str but we will switch to pint units in later releases. Similar to the numeric bounds the units are checked during the job submission and the job is rejected if the unit do not match. Example In 1999 the NASA lost its Mars Climate Orbiter due to a navigation error, which was caused by a failure to convert units from the imperial system to the metric system .","title":"Unit checking"},{"location":"user_guide/sharing/sharing/","text":"You can share your functions with others, and they can simply use it from their code just as simply as you can. All you need is the email address of the person you want to share with. We take care of the rest. Sharing is implemented on a namespace level . Sharing via code \u00b6 Using code is the easiest way to share a namespace. There are 3 steps to share a function: Create a namespace that you want to share Add the function you want to share to the namespace Share the namespace via e-mail In code it looks just like this: Sharing a function 1 2 3 4 5 6 7 8 9 from lmrtfy.functions import catalog catalog . create_namespace ( catalog .< user_namespace > , \"new_namespace\" ) catalog . add_function_to_namespace ( catalog .< user_namespace >. new_namespace , catatlog .< user_namespace >.< function > ) catalog . share_namespace ( catalog .< user_namespace >. new_namespace , \"someone@somewhere.com\" ) <user_namespace> is the namespace that is bound to your user and is <username>_a0 if you use a username/password combination for your account. If you use a social login the username will be taken from there. To ensure uniqueness we add a suffix to the username to create the namespace: for Auth0 (username/password): <username>_a0 for GitHub: <username>_gh for Google: <username>_gl <function> is any function that is available in your catalog and owned by you. The output of th script above is an invite ID. The invite is sent via email to the specified recipient. The invite is not bound to this email address. If the invitee uses another email for LMRTFY they can just login with their regular account to accept the invite. Alternatively, the invitation can be accepted via code as well: Accepting an invite in code catalog . accept_invite ( \"<invite_id>\" )","title":"Sharing a function"},{"location":"user_guide/sharing/sharing/#sharing-via-code","text":"Using code is the easiest way to share a namespace. There are 3 steps to share a function: Create a namespace that you want to share Add the function you want to share to the namespace Share the namespace via e-mail In code it looks just like this: Sharing a function 1 2 3 4 5 6 7 8 9 from lmrtfy.functions import catalog catalog . create_namespace ( catalog .< user_namespace > , \"new_namespace\" ) catalog . add_function_to_namespace ( catalog .< user_namespace >. new_namespace , catatlog .< user_namespace >.< function > ) catalog . share_namespace ( catalog .< user_namespace >. new_namespace , \"someone@somewhere.com\" ) <user_namespace> is the namespace that is bound to your user and is <username>_a0 if you use a username/password combination for your account. If you use a social login the username will be taken from there. To ensure uniqueness we add a suffix to the username to create the namespace: for Auth0 (username/password): <username>_a0 for GitHub: <username>_gh for Google: <username>_gl <function> is any function that is available in your catalog and owned by you. The output of th script above is an invite ID. The invite is sent via email to the specified recipient. The invite is not bound to this email address. If the invitee uses another email for LMRTFY they can just login with their regular account to accept the invite. Alternatively, the invitation can be accepted via code as well: Accepting an invite in code catalog . accept_invite ( \"<invite_id>\" )","title":"Sharing via code"},{"location":"user_guide/web_app/catalog/","text":"Coming soon...","title":"Catalog"},{"location":"user_guide/web_app/overview/","text":"Our web app provides you with information regarding your deployed functions, your runners and your jobs. The app is work in progress and under constant development. More features will be added. Currently, you can do the following things: Get an overview of the job activities of your deployed functions. Check your submitted jobs, their status and how long they ran. Check your function catalog. Check your deployed runners and their status The navigation is at the top. (1) The logo leads to our website (2) The main navigation is located directly next to the logo. (3) On the right side, you find a drop-down menu for profile management. Activity view \u00b6 The activity view is the standard view that you will see when you open the app: (1) Links to the documentation and our contact in case you need anything! (2) A list of functions you deployed at least once. Sorted by usage. (3) The actual activity graph showing how often a function has been used. Note If you haven't deployed any functions, you will not see the activity graph. Jobs view \u00b6 The jobs view allows you to see all that you have submitted. You can limit the visible jobs to jobs in a certain time frame, and also filter by name and status. (1) Filter jobs by time. (2) The job ID is always 10 characters long. If you experience any problems with your jobs we will always ask for the job ID. (3) Each job is tied to a specific function name which is displayed in this view. (4) The job status is displayed in the last column. If it is RESULTS_READY the results can be fetched Functions view \u00b6 This view shows all the functions that are available in your catalog. Currently, this serves as a view into the catalog, but you cannot share, rename or remove yet. What you can do, is to check the details of a function. Just click the function and then click on the Details button. (1) Function actions. Currently, Details works but the others don't. (2) These are the function names that will be available in your catalog and can be called with that. (3) The namespaces are currently not displayed correctly. (4) The profile IDs are 7 characters long and unique for a specific function. (5) The last columns indicates whether a function has been shared or not. (under construction) Runners view \u00b6 This view shows all runner that you have ever deployed. Each one has a unique runner ID which was bound to a specific function. You can check the runner status here, which is useful if you deployed on a remote machine. (1) The unique runner ID for each runner that has been deployed (2) The function that was served by a specfific runner (3) Check whether a runner is active or not. If a runner has not been active for more than a few minutes it's likely in an erroneous state and should be restarted.","title":"Using the LMRTFY web app \ud83c\udd95"},{"location":"user_guide/web_app/overview/#activity-view","text":"The activity view is the standard view that you will see when you open the app: (1) Links to the documentation and our contact in case you need anything! (2) A list of functions you deployed at least once. Sorted by usage. (3) The actual activity graph showing how often a function has been used. Note If you haven't deployed any functions, you will not see the activity graph.","title":"Activity view"},{"location":"user_guide/web_app/overview/#jobs-view","text":"The jobs view allows you to see all that you have submitted. You can limit the visible jobs to jobs in a certain time frame, and also filter by name and status. (1) Filter jobs by time. (2) The job ID is always 10 characters long. If you experience any problems with your jobs we will always ask for the job ID. (3) Each job is tied to a specific function name which is displayed in this view. (4) The job status is displayed in the last column. If it is RESULTS_READY the results can be fetched","title":"Jobs view"},{"location":"user_guide/web_app/overview/#functions-view","text":"This view shows all the functions that are available in your catalog. Currently, this serves as a view into the catalog, but you cannot share, rename or remove yet. What you can do, is to check the details of a function. Just click the function and then click on the Details button. (1) Function actions. Currently, Details works but the others don't. (2) These are the function names that will be available in your catalog and can be called with that. (3) The namespaces are currently not displayed correctly. (4) The profile IDs are 7 characters long and unique for a specific function. (5) The last columns indicates whether a function has been shared or not. (under construction)","title":"Functions view"},{"location":"user_guide/web_app/overview/#runners-view","text":"This view shows all runner that you have ever deployed. Each one has a unique runner ID which was bound to a specific function. You can check the runner status here, which is useful if you deployed on a remote machine. (1) The unique runner ID for each runner that has been deployed (2) The function that was served by a specfific runner (3) Check whether a runner is active or not. If a runner has not been active for more than a few minutes it's likely in an erroneous state and should be restarted.","title":"Runners view"}]}