{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","tags":["Introduction"],"text":"LMRTFY stands for Let Me Run That For You . \u2705 Create functions that run in the cloud , on your servers or even on your laptop . \u2705 Call them from code that runs somewhere else, just like a regular function . \u2705 Share functions with friends and colleagues, track their usage and monetize their usage. \u2705 Works with Python, but more languages will be added in the future. Quickstart Guide Tutorial Examples API Reference How to report issues and how to contribute Introduction \u00b6 LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possible to use LMRTFY Call deployed function like any other function provided by a local library Warning LMRTFY is currently in an early phase. Things will likely change in future releases. Quickstart - TL;DR \u00b6 install with pip install lmrtfy login/sign up with lmrtfy login annotate your code's inputs with variable and its outputs with result deploy the script: lmrtfy deploy examples/deployment/calc_compound_interest.py --local Use the deployed function (from another terminal, or another computer!): open examples/calling_cloud_functions/call_function.py run python examples/call_deployed_function.py to call the deployed function and get the results. As you can see in step 5, it's as simple as calling a regular function from any other library you have installed locally. Examples \u00b6 The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one!","title":"Introduction"},{"location":"#introduction","text":"LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possible to use LMRTFY Call deployed function like any other function provided by a local library Warning LMRTFY is currently in an early phase. Things will likely change in future releases.","title":"Introduction"},{"location":"#quickstart-tldr","text":"install with pip install lmrtfy login/sign up with lmrtfy login annotate your code's inputs with variable and its outputs with result deploy the script: lmrtfy deploy examples/deployment/calc_compound_interest.py --local Use the deployed function (from another terminal, or another computer!): open examples/calling_cloud_functions/call_function.py run python examples/call_deployed_function.py to call the deployed function and get the results. As you can see in step 5, it's as simple as calling a regular function from any other library you have installed locally.","title":"Quickstart - TL;DR"},{"location":"#examples","text":"The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one!","title":"Examples"},{"location":"CHANGELOG/","text":"v0.0.8 - 16/sep/2022 \u00b6 fixed missing dependencies for installation v0.0.7 - 16/sep/2022 \u00b6 introduced catalog feature to call 'cloud' functions directly from code. enhanced documentation (API reference, links, quickstart, structure, ...) general bug fixes for better stability v0.0.6 - 08/sep/2022 \u00b6 changed input formats for json as follows: { \"profile_id\" : \"<profile_id>\" , \"job_parameters\" : { \"time\" : 200.0 }, \"parameter_units\" : { \"time\" : \"s\" } } to { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } modified readme to match docs added check if job_id is valid UUID v0.0.5 - 06/sep/2022 \u00b6 included version check changed listener to remove profile collisions v0.0.4 - 05/sep/2022 \u00b6 First release This creates the following commands: * lmrtfy login => get token * lmrtfy deploy <script.py> --local => get profile_id for deployed script profile * lmrtfy submit <profile_id> <input.json> => get job_id for submitted job * lmrtfy fetch <job_id> <path_to_save> => get results for finished job","title":"Changelog"},{"location":"CHANGELOG/#v008-16sep2022","text":"fixed missing dependencies for installation","title":"v0.0.8 - 16/sep/2022"},{"location":"CHANGELOG/#v007-16sep2022","text":"introduced catalog feature to call 'cloud' functions directly from code. enhanced documentation (API reference, links, quickstart, structure, ...) general bug fixes for better stability","title":"v0.0.7 - 16/sep/2022"},{"location":"CHANGELOG/#v006-08sep2022","text":"changed input formats for json as follows: { \"profile_id\" : \"<profile_id>\" , \"job_parameters\" : { \"time\" : 200.0 }, \"parameter_units\" : { \"time\" : \"s\" } } to { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } modified readme to match docs added check if job_id is valid UUID","title":"v0.0.6 - 08/sep/2022"},{"location":"CHANGELOG/#v005-06sep2022","text":"included version check changed listener to remove profile collisions","title":"v0.0.5 - 06/sep/2022"},{"location":"CHANGELOG/#v004-05sep2022","text":"First release This creates the following commands: * lmrtfy login => get token * lmrtfy deploy <script.py> --local => get profile_id for deployed script profile * lmrtfy submit <profile_id> <input.json> => get job_id for submitted job * lmrtfy fetch <job_id> <path_to_save> => get results for finished job","title":"v0.0.4 - 05/sep/2022"},{"location":"about/","text":"lmrtfy is a project that is currently work in progress. Things will change. If you have feature requests please let us know!","title":"About"},{"location":"cli_reference/","text":"The lmrtfy CLI tools uses the following schema: lmrtfy [ COMMAND ] [ COMMAND_OPTIONS ] where [COMMAND] is one of the following: login , logouot , deploy , submit , and fetch . Commands \u00b6 login \u00b6 The usage of the LMRTFY services is connected to a user accouunt. With lmrtfy login you can directly trigger the login process. This is usually not necessary, because each command that needs a valid authorization token will trigger the login if needed. logout \u00b6 Logout of the currently active account. This should only be necessary in special cases, e.g. if you have more than one account for some reason (private and work account for example). deploy <scipt> [OPTIONS] \u00b6 This command deploys the <script> and makes it available via the LMRTFY platform. It's now ready to take jobs. Note Currently, only local deployment via the --local flag works. If you want to deploy to the cloud you need to manually do this and run lmrtfy deploy <script> --local on the remote resource. Option Description --local Deploy locally on the current system. --run_as_daemon Run the deployment as daemon in the background. submit <profile_id> <arguments.json> \u00b6 The CLI allows you to submit jobs for a specific profile_id which is returned by the deploy step. <argument.json> needs to contain valid input data, otherwise the job is rejected. The structure of the JSON file can be found here . fetch <job_id> <path> \u00b6 It's also possible to fetch the results from a job with job_id . The job ID is displayed during the submission step. With <path> you specify where the results should be saved.","title":"CLI reference"},{"location":"cli_reference/#commands","text":"","title":"Commands"},{"location":"cli_reference/#login","text":"The usage of the LMRTFY services is connected to a user accouunt. With lmrtfy login you can directly trigger the login process. This is usually not necessary, because each command that needs a valid authorization token will trigger the login if needed.","title":"login"},{"location":"cli_reference/#logout","text":"Logout of the currently active account. This should only be necessary in special cases, e.g. if you have more than one account for some reason (private and work account for example).","title":"logout"},{"location":"cli_reference/#deploy-scipt-options","text":"This command deploys the <script> and makes it available via the LMRTFY platform. It's now ready to take jobs. Note Currently, only local deployment via the --local flag works. If you want to deploy to the cloud you need to manually do this and run lmrtfy deploy <script> --local on the remote resource. Option Description --local Deploy locally on the current system. --run_as_daemon Run the deployment as daemon in the background.","title":"deploy &lt;scipt&gt; [OPTIONS]"},{"location":"cli_reference/#submit-profile_id-argumentsjson","text":"The CLI allows you to submit jobs for a specific profile_id which is returned by the deploy step. <argument.json> needs to contain valid input data, otherwise the job is rejected. The structure of the JSON file can be found here .","title":"submit &lt;profile_id&gt; &lt;arguments.json&gt;"},{"location":"cli_reference/#fetch-job_id-path","text":"It's also possible to fetch the results from a job with job_id . The job ID is displayed during the submission step. With <path> you specify where the results should be saved.","title":"fetch &lt;job_id&gt; &lt;path&gt;"},{"location":"contributing/","text":"We welcome all contributors and their contributions. All contributors will be listed in the project (if wanted). What to contribute? \u00b6 There are several ways that you can contribute to lmrtfy . Report bugs or suggest features Work on the code: fix a bug, create a feature, ... Improve the documentation Improving the documentation often isn't glamorous work but is highly appreciated by us. If you think something is missing in the documentation or something needs more information, please let us know or add the documentation. Contribute code \u00b6 The process of contributing code is quite easy: Fork the lmrtfy repository. Let us know what you want to work on to ensure that we can merge your code. Best to create an issue . Fix a bug, add a feature, ... Create a pull request . This process is standard in most repositories. If you have any questions or need help contributing, please let us know! Contribute documentation \u00b6 The files required to build the documentation are inside the docs directory. They are plain markdown files that can be easily edited. The documentation is built with mkdocs and the mkdocs-material theme. Everything you need to build the docs locally can be installed by pip install -r docs/requirements.txt . Reading the documentation of mkdocs and the material theme is highly recommend.","title":"Contributing code"},{"location":"contributing/#what-to-contribute","text":"There are several ways that you can contribute to lmrtfy . Report bugs or suggest features Work on the code: fix a bug, create a feature, ... Improve the documentation Improving the documentation often isn't glamorous work but is highly appreciated by us. If you think something is missing in the documentation or something needs more information, please let us know or add the documentation.","title":"What to contribute?"},{"location":"contributing/#contribute-code","text":"The process of contributing code is quite easy: Fork the lmrtfy repository. Let us know what you want to work on to ensure that we can merge your code. Best to create an issue . Fix a bug, add a feature, ... Create a pull request . This process is standard in most repositories. If you have any questions or need help contributing, please let us know!","title":"Contribute code"},{"location":"contributing/#contribute-documentation","text":"The files required to build the documentation are inside the docs directory. They are plain markdown files that can be easily edited. The documentation is built with mkdocs and the mkdocs-material theme. Everything you need to build the docs locally can be installed by pip install -r docs/requirements.txt . Reading the documentation of mkdocs and the material theme is highly recommend.","title":"Contribute documentation"},{"location":"quickstart/","text":"Installation: \u00b6 We provide a PyPI package Note We recommend installing LMRTFY into a virtual environment to keep your system clean. Linux and MacOS \u00b6 We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy Windows \u00b6 On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy First login \u00b6 Login/sign up to receive access token: $ lmrtfy login . The token is saved in ~/.lmrtfy/auth/token but you should not need to manually open the token. Create code annotations \u00b6 Annotate the inputs and outputs of your script with variable and results and save it as script.py : # import the required things from lmrtfy from lmrtfy import variable , result # annotate an input time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) speed = result ( 9.81 * time , name = \"speed\" , min = 0 , max = 9810 , unit = \"m/s\" ) This calculates the velocity of an object in free fall after time seconds. Run python script.py to create the profile. This always works, even without access to lmrtfy. Deploy the script to accept jobs from the generated web API \u00b6 Run lmrtfy deploy script.py --local to generate the API and start the runner to listen to jobs for your script. You can find the profile id to submit a job in the logs: INFO Starting deployment of examples/velocity_from_gravity/calc_velocity.py WARNING Deploying locally. INFO Profile_id to be used for requests: 7ff68a0cfd8c61122cfdaf0a835c7cd1f94e7db9 Submit jobs with CLI \u00b6 Open a new terminal and submit a new job ( <profile_id> is the profile id you received during the deployment ( 7ff... in the example above)): $ lmrtfy submit <profile_id> <input.json> For the example above save the following in your input.json : { \"argument_values\" : { \"time\" : 200.0 }, \"argument_units\" : { \"time\" : \"s\" } } You will receive a job_id which you will need to fetch the results later on: INFO Job-id: 14584640 -778c-4f91-a288-03cffc2b9c7a Fetch results \u00b6 Get the results by calling $ lmrtfy fetch <job_id> <path> . Currently, the results will be saved in <path>/<job_id> as JSON files.","title":"Quick start"},{"location":"quickstart/#installation","text":"We provide a PyPI package Note We recommend installing LMRTFY into a virtual environment to keep your system clean.","title":"Installation:"},{"location":"quickstart/#linux-and-macos","text":"We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy","title":"Linux and MacOS"},{"location":"quickstart/#windows","text":"On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy","title":"Windows"},{"location":"quickstart/#first-login","text":"Login/sign up to receive access token: $ lmrtfy login . The token is saved in ~/.lmrtfy/auth/token but you should not need to manually open the token.","title":"First login"},{"location":"quickstart/#create-code-annotations","text":"Annotate the inputs and outputs of your script with variable and results and save it as script.py : # import the required things from lmrtfy from lmrtfy import variable , result # annotate an input time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) speed = result ( 9.81 * time , name = \"speed\" , min = 0 , max = 9810 , unit = \"m/s\" ) This calculates the velocity of an object in free fall after time seconds. Run python script.py to create the profile. This always works, even without access to lmrtfy.","title":"Create code annotations"},{"location":"quickstart/#deploy-the-script-to-accept-jobs-from-the-generated-web-api","text":"Run lmrtfy deploy script.py --local to generate the API and start the runner to listen to jobs for your script. You can find the profile id to submit a job in the logs: INFO Starting deployment of examples/velocity_from_gravity/calc_velocity.py WARNING Deploying locally. INFO Profile_id to be used for requests: 7ff68a0cfd8c61122cfdaf0a835c7cd1f94e7db9","title":"Deploy the script to accept jobs from the generated web API"},{"location":"quickstart/#submit-jobs-with-cli","text":"Open a new terminal and submit a new job ( <profile_id> is the profile id you received during the deployment ( 7ff... in the example above)): $ lmrtfy submit <profile_id> <input.json> For the example above save the following in your input.json : { \"argument_values\" : { \"time\" : 200.0 }, \"argument_units\" : { \"time\" : \"s\" } } You will receive a job_id which you will need to fetch the results later on: INFO Job-id: 14584640 -778c-4f91-a288-03cffc2b9c7a","title":"Submit jobs with CLI"},{"location":"quickstart/#fetch-results","text":"Get the results by calling $ lmrtfy fetch <job_id> <path> . Currently, the results will be saved in <path>/<job_id> as JSON files.","title":"Fetch results"},{"location":"report_bugs/","text":"When you find an issue, don't hesitate to create an issue to let us know about the bug. When you create an issue there are 3 options: Create a bug report \u00b6 Reporting bugs is very important. We are very thankful for each bug that is reported and will try to fix them as soon as possible. The clearer the description of the bug is and the more information you provide, the better. Request a feature \u00b6 This is the best choice if there is a feature you are missing. Each feature request will be reviewed. If the feature aligns with our vision of LMRTFY. If we are going to implement the feature it will be added to the roadmap. Ask a question \u00b6 This the best choice if you just want to ask a question about LMRTFY: Is a feature planned? How to do X? *... Open a blank issue \u00b6 This should be the last resort and only be used if your concern does not fit into any of the other categories.","title":"Reporting an issue"},{"location":"report_bugs/#create-a-bug-report","text":"Reporting bugs is very important. We are very thankful for each bug that is reported and will try to fix them as soon as possible. The clearer the description of the bug is and the more information you provide, the better.","title":"Create a bug report"},{"location":"report_bugs/#request-a-feature","text":"This is the best choice if there is a feature you are missing. Each feature request will be reviewed. If the feature aligns with our vision of LMRTFY. If we are going to implement the feature it will be added to the roadmap.","title":"Request a feature"},{"location":"report_bugs/#ask-a-question","text":"This the best choice if you just want to ask a question about LMRTFY: Is a feature planned? How to do X? *...","title":"Ask a question"},{"location":"report_bugs/#open-a-blank-issue","text":"This should be the last resort and only be used if your concern does not fit into any of the other categories.","title":"Open a blank issue"},{"location":"roadmap/","text":"","title":"Roadmap"},{"location":"vision/","text":"","title":"Our vision"},{"location":"api_reference/annotation/","text":"result ( value , name , min = None , max = None , unit = None ) \u00b6 result defines an input parameter of the script. It is used to create the profile for the code. r1 = result(5*v1, name='res1') creates a result named 'res1' in the profile. If run with the python interpreter r1 is equal to 5*v1 Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric result takes. Currently not checked. None max [optional] Maximum value that a numeric result takes. Currently not checked. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `result` defines an input parameter of the script. It is used to create the profile for the code. `r1 = result(5*v1, name='res1')` creates a result named 'res1' in the profile. If run with the python interpreter `r1` is equal to `5*v1` :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric result takes. Currently not checked. :param max: [optional] Maximum value that a numeric result takes. Currently not checked. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'result' , dtype = _get_type ( value ), unit = None , min = min , max = max ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with ( open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_result_ { name } .json' )), 'w' )) as f : json . dump ({ name : value }, f , cls = NumpyEncoder ) logging . info ( f \"Running: Saved result ' { name } ' with value ' { value } '.\" ) return value variable ( value , name , min = None , max = None , unit = None ) \u00b6 variable defines an input parameter of the script. It is used to create the profile for the code. v1 = variable(5, name='var1') create a variable named 'var1' in the profile. If run with the python interpreter v1 takes the value 5 and can be used in the rest of the code just like any other variable. Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric variable takes. Checked by API. None max [optional] Maximum value that a numeric variable takes. Checked by API. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `variable` defines an input parameter of the script. It is used to create the profile for the code. `v1 = variable(5, name='var1')` create a variable named 'var1' in the profile. If run with the python interpreter `v1` takes the value 5 and can be used in the rest of the code just like any other variable. :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric variable takes. Checked by API. :param max: [optional] Maximum value that a numeric variable takes. Checked by API. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'variable' , dtype = _get_type ( value ), min = min , max = max , unit = unit ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_variable_ { name } .json' )), 'r' ) as f : tmp = json . load ( f ) # TODO: Make it work for numpy dtypes. dtype = _inverse_type_map [ type ( value )] value = dtype ( tmp [ name ]) logging . info ( f \"Running: Loaded variable ' { name } ' of type ' { dtype } ' and value ' { value } '.\" ) return value","title":"Annotation"},{"location":"api_reference/annotation/#src.lmrtfy.annotation.result","text":"result defines an input parameter of the script. It is used to create the profile for the code. r1 = result(5*v1, name='res1') creates a result named 'res1' in the profile. If run with the python interpreter r1 is equal to 5*v1 Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric result takes. Currently not checked. None max [optional] Maximum value that a numeric result takes. Currently not checked. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `result` defines an input parameter of the script. It is used to create the profile for the code. `r1 = result(5*v1, name='res1')` creates a result named 'res1' in the profile. If run with the python interpreter `r1` is equal to `5*v1` :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric result takes. Currently not checked. :param max: [optional] Maximum value that a numeric result takes. Currently not checked. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'result' , dtype = _get_type ( value ), unit = None , min = min , max = max ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with ( open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_result_ { name } .json' )), 'w' )) as f : json . dump ({ name : value }, f , cls = NumpyEncoder ) logging . info ( f \"Running: Saved result ' { name } ' with value ' { value } '.\" ) return value","title":"result()"},{"location":"api_reference/annotation/#src.lmrtfy.annotation.variable","text":"variable defines an input parameter of the script. It is used to create the profile for the code. v1 = variable(5, name='var1') create a variable named 'var1' in the profile. If run with the python interpreter v1 takes the value 5 and can be used in the rest of the code just like any other variable. Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric variable takes. Checked by API. None max [optional] Maximum value that a numeric variable takes. Checked by API. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `variable` defines an input parameter of the script. It is used to create the profile for the code. `v1 = variable(5, name='var1')` create a variable named 'var1' in the profile. If run with the python interpreter `v1` takes the value 5 and can be used in the rest of the code just like any other variable. :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric variable takes. Checked by API. :param max: [optional] Maximum value that a numeric variable takes. Checked by API. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'variable' , dtype = _get_type ( value ), min = min , max = max , unit = unit ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_variable_ { name } .json' )), 'r' ) as f : tmp = json . load ( f ) # TODO: Make it work for numpy dtypes. dtype = _inverse_type_map [ type ( value )] value = dtype ( tmp [ name ]) logging . info ( f \"Running: Loaded variable ' { name } ' of type ' { dtype } ' and value ' { value } '.\" ) return value","title":"variable()"},{"location":"api_reference/catalog/","text":"Catalog \u00b6 Bases: object The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during from lmrtfy import catalog . If you want to retrieve newly deployed function, call catalog.update() . To run a deployed function from the catalog call catalog.<deployed_function>(*args, **kwargs) . Each function that has been pulled into the catalog is available via the help() command. Source code in src/lmrtfy/functions.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 class Catalog ( object ): \"\"\" The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during `from lmrtfy import catalog`. If you want to retrieve newly deployed function, call `catalog.update()`. To run a deployed function from the catalog call `catalog.<deployed_function>(*args, **kwargs)`. Each function that has been pulled into the catalog is available via the `help()` command. \"\"\" def __init__ ( self ): h = LoginHandler () if h . login (): h . get_token () self . config = get_cliconfig () logging . info ( self . config ) self . token = load_token_data ()[ 'access_token' ] self . headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { self . token } \" } self . profiles = None self . update () def __add_function ( self , namespace , name , sig , res_ann , pid ): template = fetch_template ( pid ) def f ( ** kwargs ) -> Job : f . pid = pid for p in kwargs : template [ 'argument_values' ][ p ] = kwargs [ p ] r = requests . post ( self . config [ 'api_submit_url' ] + f '/ { pid } ' , data = json . dumps ( template , cls = NumpyEncoder ), headers = self . headers ) if r . status_code == 200 : return Job ( r . json ()[ 'job_id' ]) if r . status_code == 400 : logging . error ( f 'Input Error: { r . json () } ' ) elif r . status_code == 404 : logging . error ( f \" { r . json () } \" ) setattr ( namespace , name , create_function ( sig , f , func_name = name , qualname = f \" { namespace . __name__ } - { pid } \" )) def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" # TODO: get list of namespaces r = requests . get ( self . config [ 'api_namespaces_url' ], headers = self . headers ) logging . info ( r . json ()) namespaces = r . json ()[ 'namespaces' ] for n in namespaces : names = n . split ( '/' ) o = self nsn = \"\" for name in names : nsn += f \" { name } /\" if not hasattr ( o , name ): setattr ( o , name , Namespace ( nsn [: - 1 ])) o = getattr ( o , name ) try : r = requests . get ( f \" { self . config [ 'api_namespaces_url' ] } / { n . replace ( '/' , '-' ) } \" , headers = self . headers ) functions = r . json ()[ 'functions' ] for func in functions : func_name = unique_name ( o , functions [ func ][ 'name' ]) self . __add_function ( o , func_name , * signature_from_profile ( functions [ func ]), pid = func ) except : # TODO: Except clause too broad! logging . error ( f \"Could not namespace { n } function catalog.\" ) def create_namespace ( self , namespace , name : str ) -> bool : \"\"\" Create a unique namespace that collects functions and can be shared. :param namespace: parent namespace :param name: Name of the new namespace. :return: Returns the Namespace object on success and None in case of an error. \"\"\" try : new_name = f \" { namespace . __name__ } / { name } \" . replace ( '/' , '-' ) r = requests . post ( f \" { self . config [ 'api_namespaces_url' ] } \" , data = json . dumps ({ \"namespace\" : new_name }), headers = self . headers ) if r . status_code == 201 : self . update () return True except : pass logging . error ( f \"Could not create namespace { name } .\" ) return False def add_function_to_namespace ( self , namespace , function ): # PUT r = requests . put ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False def delete_namespace ( self , namespace ) -> bool : # DELETE r = requests . delete ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False def remove_function_from_namespace ( self , function ) -> bool : # PATCH namespace = function . __qualname__ . split ( '-' )[ 0 ] r = requests . patch ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False def share_namespace ( self , namespace , recipient_email ) -> Optional [ str ]: id_token = load_token_data ()[ 'id_token' ] sender_email = jwt . decode ( id_token , options = { \"verify_signature\" : False })[ \"email\" ] r = requests . post ( f \" { self . config [ 'api_invites_url' ] } \" , data = json . dumps ({ \"namespace\" : namespace . __name__ , \"recipient_email\" : recipient_email , \"sender_email\" : sender_email }), headers = self . headers ) if r . status_code == 202 : return r . json ()[ \"invite_id\" ] def accept_invite ( self , invite_id ) -> bool : r = requests . get ( f \" { self . config [ 'api_invites_url' ] } / { invite_id } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False create_namespace ( namespace , name ) \u00b6 Create a unique namespace that collects functions and can be shared. Parameters: Name Type Description Default namespace parent namespace required name str Name of the new namespace. required Returns: Type Description bool Returns the Namespace object on success and None in case of an error. Source code in src/lmrtfy/functions.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def create_namespace ( self , namespace , name : str ) -> bool : \"\"\" Create a unique namespace that collects functions and can be shared. :param namespace: parent namespace :param name: Name of the new namespace. :return: Returns the Namespace object on success and None in case of an error. \"\"\" try : new_name = f \" { namespace . __name__ } / { name } \" . replace ( '/' , '-' ) r = requests . post ( f \" { self . config [ 'api_namespaces_url' ] } \" , data = json . dumps ({ \"namespace\" : new_name }), headers = self . headers ) if r . status_code == 201 : self . update () return True except : pass logging . error ( f \"Could not create namespace { name } .\" ) return False update () \u00b6 Call update to update the catalog with newly deployed functions. Source code in src/lmrtfy/functions.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" # TODO: get list of namespaces r = requests . get ( self . config [ 'api_namespaces_url' ], headers = self . headers ) logging . info ( r . json ()) namespaces = r . json ()[ 'namespaces' ] for n in namespaces : names = n . split ( '/' ) o = self nsn = \"\" for name in names : nsn += f \" { name } /\" if not hasattr ( o , name ): setattr ( o , name , Namespace ( nsn [: - 1 ])) o = getattr ( o , name ) try : r = requests . get ( f \" { self . config [ 'api_namespaces_url' ] } / { n . replace ( '/' , '-' ) } \" , headers = self . headers ) functions = r . json ()[ 'functions' ] for func in functions : func_name = unique_name ( o , functions [ func ][ 'name' ]) self . __add_function ( o , func_name , * signature_from_profile ( functions [ func ]), pid = func ) except : # TODO: Except clause too broad! logging . error ( f \"Could not namespace { n } function catalog.\" )","title":"Catalog"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog","text":"Bases: object The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during from lmrtfy import catalog . If you want to retrieve newly deployed function, call catalog.update() . To run a deployed function from the catalog call catalog.<deployed_function>(*args, **kwargs) . Each function that has been pulled into the catalog is available via the help() command. Source code in src/lmrtfy/functions.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 class Catalog ( object ): \"\"\" The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during `from lmrtfy import catalog`. If you want to retrieve newly deployed function, call `catalog.update()`. To run a deployed function from the catalog call `catalog.<deployed_function>(*args, **kwargs)`. Each function that has been pulled into the catalog is available via the `help()` command. \"\"\" def __init__ ( self ): h = LoginHandler () if h . login (): h . get_token () self . config = get_cliconfig () logging . info ( self . config ) self . token = load_token_data ()[ 'access_token' ] self . headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { self . token } \" } self . profiles = None self . update () def __add_function ( self , namespace , name , sig , res_ann , pid ): template = fetch_template ( pid ) def f ( ** kwargs ) -> Job : f . pid = pid for p in kwargs : template [ 'argument_values' ][ p ] = kwargs [ p ] r = requests . post ( self . config [ 'api_submit_url' ] + f '/ { pid } ' , data = json . dumps ( template , cls = NumpyEncoder ), headers = self . headers ) if r . status_code == 200 : return Job ( r . json ()[ 'job_id' ]) if r . status_code == 400 : logging . error ( f 'Input Error: { r . json () } ' ) elif r . status_code == 404 : logging . error ( f \" { r . json () } \" ) setattr ( namespace , name , create_function ( sig , f , func_name = name , qualname = f \" { namespace . __name__ } - { pid } \" )) def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" # TODO: get list of namespaces r = requests . get ( self . config [ 'api_namespaces_url' ], headers = self . headers ) logging . info ( r . json ()) namespaces = r . json ()[ 'namespaces' ] for n in namespaces : names = n . split ( '/' ) o = self nsn = \"\" for name in names : nsn += f \" { name } /\" if not hasattr ( o , name ): setattr ( o , name , Namespace ( nsn [: - 1 ])) o = getattr ( o , name ) try : r = requests . get ( f \" { self . config [ 'api_namespaces_url' ] } / { n . replace ( '/' , '-' ) } \" , headers = self . headers ) functions = r . json ()[ 'functions' ] for func in functions : func_name = unique_name ( o , functions [ func ][ 'name' ]) self . __add_function ( o , func_name , * signature_from_profile ( functions [ func ]), pid = func ) except : # TODO: Except clause too broad! logging . error ( f \"Could not namespace { n } function catalog.\" ) def create_namespace ( self , namespace , name : str ) -> bool : \"\"\" Create a unique namespace that collects functions and can be shared. :param namespace: parent namespace :param name: Name of the new namespace. :return: Returns the Namespace object on success and None in case of an error. \"\"\" try : new_name = f \" { namespace . __name__ } / { name } \" . replace ( '/' , '-' ) r = requests . post ( f \" { self . config [ 'api_namespaces_url' ] } \" , data = json . dumps ({ \"namespace\" : new_name }), headers = self . headers ) if r . status_code == 201 : self . update () return True except : pass logging . error ( f \"Could not create namespace { name } .\" ) return False def add_function_to_namespace ( self , namespace , function ): # PUT r = requests . put ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False def delete_namespace ( self , namespace ) -> bool : # DELETE r = requests . delete ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False def remove_function_from_namespace ( self , function ) -> bool : # PATCH namespace = function . __qualname__ . split ( '-' )[ 0 ] r = requests . patch ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False def share_namespace ( self , namespace , recipient_email ) -> Optional [ str ]: id_token = load_token_data ()[ 'id_token' ] sender_email = jwt . decode ( id_token , options = { \"verify_signature\" : False })[ \"email\" ] r = requests . post ( f \" { self . config [ 'api_invites_url' ] } \" , data = json . dumps ({ \"namespace\" : namespace . __name__ , \"recipient_email\" : recipient_email , \"sender_email\" : sender_email }), headers = self . headers ) if r . status_code == 202 : return r . json ()[ \"invite_id\" ] def accept_invite ( self , invite_id ) -> bool : r = requests . get ( f \" { self . config [ 'api_invites_url' ] } / { invite_id } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False","title":"Catalog"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.create_namespace","text":"Create a unique namespace that collects functions and can be shared. Parameters: Name Type Description Default namespace parent namespace required name str Name of the new namespace. required Returns: Type Description bool Returns the Namespace object on success and None in case of an error. Source code in src/lmrtfy/functions.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def create_namespace ( self , namespace , name : str ) -> bool : \"\"\" Create a unique namespace that collects functions and can be shared. :param namespace: parent namespace :param name: Name of the new namespace. :return: Returns the Namespace object on success and None in case of an error. \"\"\" try : new_name = f \" { namespace . __name__ } / { name } \" . replace ( '/' , '-' ) r = requests . post ( f \" { self . config [ 'api_namespaces_url' ] } \" , data = json . dumps ({ \"namespace\" : new_name }), headers = self . headers ) if r . status_code == 201 : self . update () return True except : pass logging . error ( f \"Could not create namespace { name } .\" ) return False","title":"create_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.update","text":"Call update to update the catalog with newly deployed functions. Source code in src/lmrtfy/functions.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" # TODO: get list of namespaces r = requests . get ( self . config [ 'api_namespaces_url' ], headers = self . headers ) logging . info ( r . json ()) namespaces = r . json ()[ 'namespaces' ] for n in namespaces : names = n . split ( '/' ) o = self nsn = \"\" for name in names : nsn += f \" { name } /\" if not hasattr ( o , name ): setattr ( o , name , Namespace ( nsn [: - 1 ])) o = getattr ( o , name ) try : r = requests . get ( f \" { self . config [ 'api_namespaces_url' ] } / { n . replace ( '/' , '-' ) } \" , headers = self . headers ) functions = r . json ()[ 'functions' ] for func in functions : func_name = unique_name ( o , functions [ func ][ 'name' ]) self . __add_function ( o , func_name , * signature_from_profile ( functions [ func ]), pid = func ) except : # TODO: Except clause too broad! logging . error ( f \"Could not namespace { n } function catalog.\" )","title":"update()"},{"location":"examples/compound_interest/","text":"Example 3: Compound interest \u00b6 The third example calculates the compound interest \\(C\\) starting from a principal value \\(P\\) with annualt interest \\(I\\) after \\(N\\) years: \\[ C = P \\cdot (1 + I)^N - P \\] Very common formula in anything related to finance. Again, we start with the plain code, as you would implement it right away: # file: ci.py def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : principal = 10_000 interest = 6 years = 10 ci = compound_interest ( principal , interest , years ) print ( f \"Compound interest after { years } years: { ci } \" ) You can run this example with $ python ci.py and it should print 7908.47 . Which is the compound interest after 10 years if you started with 10000 units that grow by 6% each year. There are several problems with this solution: 1. You need to change the code to run it for other inputs 2. Units are unclear! principal is a currency, but that actually does not matter. The real problem is the interest . Is it decimal or in %? Annotate with lmrtfy \u00b6 Using lmrtfy, you would annotate the script as follows: # file: calc_compound_interest.py from lmrtfy import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): \"\"\" Compute the compound interest for `years` when starting from `principal` with `annual interest`. compound interest = principal * (1 + annual_interest)^years - principal \"\"\" return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : principal = variable ( 10000. , name = \"principal\" , min = 0 ) annual_interest = variable ( 6.0 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ) years = variable ( 10 , name = \"years\" , min = 0 ) ci = result ( compound_interest ( principal , annual_interest , years ), name = \"compound_interest\" Now, we run python ci_lmrtfy_1.py to generate the profile. Warning The type annotation are not enforced when run locally. LMRTFY checks the types and units only if jobs are submitted through its API. This guarantees that you can run your code without our service. Deployment \u00b6 After creating the profile we can easily deploy with $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local Call compound_interest from code \u00b6 Similar to the other examples we just need to import the catalog and call the correct function: # file: examples/compound_interest/call_compound_interest.py from time import sleep from lmrtfy import catalog job = catalog . calc_compound_interest ( 5. , 10. , 5 ) if job : print ( job . id , job . status ) while not job . ready : sleep ( 1. ) print ( job . results ) Call compound_interest from the CLI \u00b6 The output should be similar to this: INFO Profile_id to be used for requests: <profile_id> The <profile_id> is important to submit jobs. To submit a job you are currently required to save the input parameters as JSON (e.g. input.json ): { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } Now, we have everything that is needed to start a job: $ lmrtfy submit <profile_id> input.json The job id for this job is printed to the terminal: INFO Job-id: <job_id> We need the <job_id> later to fetch the results from the computation. Alternative Annotation \u00b6 A more compact but working alternative is to create the result as follows: from lmrtfy import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : ci = result ( compound_interest ( principal = variable ( 10000. , name = \"principal\" , min = 0 ), annual_interest = variable ( 6 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ), years = variable ( 10 , name = \"years\" , min = 0 ) ), name = \"compound_interest\" ) print ( ci ) It's not necessarily prettier to look at, but also works!","title":"Compound interest"},{"location":"examples/compound_interest/#example-3-compound-interest","text":"The third example calculates the compound interest \\(C\\) starting from a principal value \\(P\\) with annualt interest \\(I\\) after \\(N\\) years: \\[ C = P \\cdot (1 + I)^N - P \\] Very common formula in anything related to finance. Again, we start with the plain code, as you would implement it right away: # file: ci.py def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : principal = 10_000 interest = 6 years = 10 ci = compound_interest ( principal , interest , years ) print ( f \"Compound interest after { years } years: { ci } \" ) You can run this example with $ python ci.py and it should print 7908.47 . Which is the compound interest after 10 years if you started with 10000 units that grow by 6% each year. There are several problems with this solution: 1. You need to change the code to run it for other inputs 2. Units are unclear! principal is a currency, but that actually does not matter. The real problem is the interest . Is it decimal or in %?","title":"Example 3: Compound interest"},{"location":"examples/compound_interest/#annotate-with-lmrtfy","text":"Using lmrtfy, you would annotate the script as follows: # file: calc_compound_interest.py from lmrtfy import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): \"\"\" Compute the compound interest for `years` when starting from `principal` with `annual interest`. compound interest = principal * (1 + annual_interest)^years - principal \"\"\" return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : principal = variable ( 10000. , name = \"principal\" , min = 0 ) annual_interest = variable ( 6.0 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ) years = variable ( 10 , name = \"years\" , min = 0 ) ci = result ( compound_interest ( principal , annual_interest , years ), name = \"compound_interest\" Now, we run python ci_lmrtfy_1.py to generate the profile. Warning The type annotation are not enforced when run locally. LMRTFY checks the types and units only if jobs are submitted through its API. This guarantees that you can run your code without our service.","title":"Annotate with lmrtfy"},{"location":"examples/compound_interest/#deployment","text":"After creating the profile we can easily deploy with $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local","title":"Deployment"},{"location":"examples/compound_interest/#call-compound_interest-from-code","text":"Similar to the other examples we just need to import the catalog and call the correct function: # file: examples/compound_interest/call_compound_interest.py from time import sleep from lmrtfy import catalog job = catalog . calc_compound_interest ( 5. , 10. , 5 ) if job : print ( job . id , job . status ) while not job . ready : sleep ( 1. ) print ( job . results )","title":"Call compound_interest from code"},{"location":"examples/compound_interest/#call-compound_interest-from-the-cli","text":"The output should be similar to this: INFO Profile_id to be used for requests: <profile_id> The <profile_id> is important to submit jobs. To submit a job you are currently required to save the input parameters as JSON (e.g. input.json ): { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } Now, we have everything that is needed to start a job: $ lmrtfy submit <profile_id> input.json The job id for this job is printed to the terminal: INFO Job-id: <job_id> We need the <job_id> later to fetch the results from the computation.","title":"Call compound_interest from the CLI"},{"location":"examples/compound_interest/#alternative-annotation","text":"A more compact but working alternative is to create the result as follows: from lmrtfy import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : ci = result ( compound_interest ( principal = variable ( 10000. , name = \"principal\" , min = 0 ), annual_interest = variable ( 6 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ), years = variable ( 10 , name = \"years\" , min = 0 ) ), name = \"compound_interest\" ) print ( ci ) It's not necessarily prettier to look at, but also works!","title":"Alternative Annotation"},{"location":"examples/free_fall/","text":"Example 2: Velocity due to gravtity in free fall \u00b6 The second example calculates the velocity of an object falling from the sky (without air resistance). The standard gravity on earth is 9.81 m*s^(-2). Multiplicated by the fall time, we will get the velocity of the object after that time. If you like equations more, you might recognize these from your physics class: $$ v = g \\cdot t $$ In regular python code that you run locally it would look like this: standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now we want to be able to share that functionality via the lmrtfy web API. All we have to do is decide which variables are considered to be an input and a result of the computation: # file: examples/free_fall/calc_velocity.py from lmrtfy import variable , result standard_gravity = 9.81 time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) velocity = result ( standard_gravity * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m/s\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now run $ python examples/velocity_from_gravity/calc_velocity.py to generate the required profile. This way you can also check if your code is actually working the way you expect it. Deploying the script \u00b6 To deploy, you simply run $ lmrtfy deploy examples/velocity_from_gravity/calc_velocity.py --local . Do not stop that process, because than you will not be able to submit a job. Calling from code \u00b6 Calling calc_velocity by code as easy as it was for the first example . import time from lmrtfy import catalog job = catalog . calc_velocity ( time = 100.0 ) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) Note You can also run help(calc_velocity) to see the corresponding help. Right now, only the function signature is shown but in the future you will also be able to see the docstrings. Calling from CLI \u00b6 Open a new terminal in the same directory and run $ lmrtfy submit <profile_id> . The profile_id has been printed in the lmrtfy deploy step. This does not work right out of the box, because you need to specify a JSON file that contains the input parameters for your job. A template for that JSON should have been printed in the CLI. Create such a JSON file and name it input.json and put values of the correct type into the values (no type conversion is happening in the API, so if float is required, you cannot input an int ). Alternatively, use the provided input.json in examples/free_fall/input.json : { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } Now run $ lmrtfy submit <profile_id> examples/free_fall/input.json . You will receive a job_id which we will shortly need to fetch the results after they are computed. After your job has run, you can get the results by running $ lmrtfy fetch <job_id> <path to store results> . The results are downloaded and stored inside the specified path within a directory that has the job_id as its name.","title":"Free fall"},{"location":"examples/free_fall/#example-2-velocity-due-to-gravtity-in-free-fall","text":"The second example calculates the velocity of an object falling from the sky (without air resistance). The standard gravity on earth is 9.81 m*s^(-2). Multiplicated by the fall time, we will get the velocity of the object after that time. If you like equations more, you might recognize these from your physics class: $$ v = g \\cdot t $$ In regular python code that you run locally it would look like this: standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now we want to be able to share that functionality via the lmrtfy web API. All we have to do is decide which variables are considered to be an input and a result of the computation: # file: examples/free_fall/calc_velocity.py from lmrtfy import variable , result standard_gravity = 9.81 time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) velocity = result ( standard_gravity * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m/s\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now run $ python examples/velocity_from_gravity/calc_velocity.py to generate the required profile. This way you can also check if your code is actually working the way you expect it.","title":"Example 2: Velocity due to gravtity in free fall"},{"location":"examples/free_fall/#deploying-the-script","text":"To deploy, you simply run $ lmrtfy deploy examples/velocity_from_gravity/calc_velocity.py --local . Do not stop that process, because than you will not be able to submit a job.","title":"Deploying the script"},{"location":"examples/free_fall/#calling-from-code","text":"Calling calc_velocity by code as easy as it was for the first example . import time from lmrtfy import catalog job = catalog . calc_velocity ( time = 100.0 ) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) Note You can also run help(calc_velocity) to see the corresponding help. Right now, only the function signature is shown but in the future you will also be able to see the docstrings.","title":"Calling from code"},{"location":"examples/free_fall/#calling-from-cli","text":"Open a new terminal in the same directory and run $ lmrtfy submit <profile_id> . The profile_id has been printed in the lmrtfy deploy step. This does not work right out of the box, because you need to specify a JSON file that contains the input parameters for your job. A template for that JSON should have been printed in the CLI. Create such a JSON file and name it input.json and put values of the correct type into the values (no type conversion is happening in the API, so if float is required, you cannot input an int ). Alternatively, use the provided input.json in examples/free_fall/input.json : { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } Now run $ lmrtfy submit <profile_id> examples/free_fall/input.json . You will receive a job_id which we will shortly need to fetch the results after they are computed. After your job has run, you can get the results by running $ lmrtfy fetch <job_id> <path to store results> . The results are downloaded and stored inside the specified path within a directory that has the job_id as its name.","title":"Calling from CLI"},{"location":"examples/starting_example/","text":"Example 1: Simple annotation \u00b6 This is a simple example to showcase the general usage of lmrtfy. It can be found in examples/example1/example1.py . The two core concepts are the variable and result functions which annotate the inputs and outputs of the script. They are needed to create the profile which is used to create the API. # file: examples/starting/example1.py import numpy as np from lmrtfy import variable , result # 1 x = variable ( 5 , name = \"x\" , min = 1 , max = 10 ) # 2 y = variable ( np . linspace ( 0. , 1. , 101 , dtype = np . float64 ), name = \"y\" , min =- 1. , max = 11. , unit = \"m\" ) # 3 z = variable ( \"abc\" , name = \"z\" ) z1 = variable ([ \"abc\" , \"def\" ], name = \"z1\" ) # 4 z2 = variable ([ \"abc\" , 1 , 1.1 ], name = \"z2\" ) z3 = variable ({ 'a' : \"abc\" , 'b' : 1 }, name = \"z3\" ) a = result ( x * y , name = \"a\" ) # 5 b = result ( x * z , name = \"b\" ) The functions need to be imported from the lmrtfy library The variable x has the local value 5 and can be between 1 and 10. You can have numpy arrays as inputs Lists and dictionaries work, too! Results are similar to variables. They have a name and an expression that they will become. Run python examples/starting/example1.py to create the profile needed for the deployment. Deployment \u00b6 To deploy the script run lmrtfy deploy examples/starting/example1.py --local Call example1 from code \u00b6 Now you can simply call catalog.example1() with the correct arguments, and you are good to go: #!/usr/bin/env python # -*- coding: utf-8 -*- import time from lmrtfy import catalog job = catalog . example1 ( x = 1 , y = [ 1 , 2.0 , 3.0 ], z = \"foobar\" , z1 = [ \"bar\" , \"foo\" ], z2 = [ \"foo\" , 1 , 42 ], z3 = { \"foo\" : \"bar\" , \"bar\" : \"foo\" } ) #help(catalog.example1) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) if job: is currently required to ensure that you actually got a job object back from the function which would not be the case if the submission failed. Calling example1 from the CLI \u00b6 Note We encourage you to use code to submit jobs and get results. During the deployment you should have received a profile_id : Profile_id to be used for requests: 392b3bf6fb7cf32ba1a10052f138583e1a594354 We need the profile_id to submit a job from the CLI: lmrtfy submit 392b3bf6fb7cf32ba1a10052f138583e1a594354 examples/starting/example1.json If the JSON file has the correct inputs, in a valid range with correct units you will see that the job submission was successful. INFO Job submission successful. INFO Job-id: cfaf7e58-d6fc-4509-96b9-439fb2877f85 With this job_id you can now get the job results: lmrtfy fetch cfaf7e58-d6fc-4509-96b9-439fb2877f85 . That's all that is to it. Happy Hacking!","title":"Starting example"},{"location":"examples/starting_example/#example-1-simple-annotation","text":"This is a simple example to showcase the general usage of lmrtfy. It can be found in examples/example1/example1.py . The two core concepts are the variable and result functions which annotate the inputs and outputs of the script. They are needed to create the profile which is used to create the API. # file: examples/starting/example1.py import numpy as np from lmrtfy import variable , result # 1 x = variable ( 5 , name = \"x\" , min = 1 , max = 10 ) # 2 y = variable ( np . linspace ( 0. , 1. , 101 , dtype = np . float64 ), name = \"y\" , min =- 1. , max = 11. , unit = \"m\" ) # 3 z = variable ( \"abc\" , name = \"z\" ) z1 = variable ([ \"abc\" , \"def\" ], name = \"z1\" ) # 4 z2 = variable ([ \"abc\" , 1 , 1.1 ], name = \"z2\" ) z3 = variable ({ 'a' : \"abc\" , 'b' : 1 }, name = \"z3\" ) a = result ( x * y , name = \"a\" ) # 5 b = result ( x * z , name = \"b\" ) The functions need to be imported from the lmrtfy library The variable x has the local value 5 and can be between 1 and 10. You can have numpy arrays as inputs Lists and dictionaries work, too! Results are similar to variables. They have a name and an expression that they will become. Run python examples/starting/example1.py to create the profile needed for the deployment.","title":"Example 1: Simple annotation"},{"location":"examples/starting_example/#deployment","text":"To deploy the script run lmrtfy deploy examples/starting/example1.py --local","title":"Deployment"},{"location":"examples/starting_example/#call-example1-from-code","text":"Now you can simply call catalog.example1() with the correct arguments, and you are good to go: #!/usr/bin/env python # -*- coding: utf-8 -*- import time from lmrtfy import catalog job = catalog . example1 ( x = 1 , y = [ 1 , 2.0 , 3.0 ], z = \"foobar\" , z1 = [ \"bar\" , \"foo\" ], z2 = [ \"foo\" , 1 , 42 ], z3 = { \"foo\" : \"bar\" , \"bar\" : \"foo\" } ) #help(catalog.example1) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) if job: is currently required to ensure that you actually got a job object back from the function which would not be the case if the submission failed.","title":"Call example1 from code"},{"location":"examples/starting_example/#calling-example1-from-the-cli","text":"Note We encourage you to use code to submit jobs and get results. During the deployment you should have received a profile_id : Profile_id to be used for requests: 392b3bf6fb7cf32ba1a10052f138583e1a594354 We need the profile_id to submit a job from the CLI: lmrtfy submit 392b3bf6fb7cf32ba1a10052f138583e1a594354 examples/starting/example1.json If the JSON file has the correct inputs, in a valid range with correct units you will see that the job submission was successful. INFO Job submission successful. INFO Job-id: cfaf7e58-d6fc-4509-96b9-439fb2877f85 With this job_id you can now get the job results: lmrtfy fetch cfaf7e58-d6fc-4509-96b9-439fb2877f85 . That's all that is to it. Happy Hacking!","title":"Calling example1 from the CLI"},{"location":"tutorial/annotation/","text":"Annotate your script \u00b6 The annotation of your script tells the lmrtfy tool which python variables are considered inputs and outputs, which is done via the variable and results functions. This step is important, because lmrtfy traces the calls to variable and result to create a profile for the code. This profile includes the inputs and outputs as well as the additional meta information ( min , max , unit , and possibly more in the future). Let's assume that you have create a script to calculate the velocity of an object after a certain time: # file: free_fall.py standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now, if you want to recalculate for a different time, you would edit the script and run it again. While this might work for a small script like this, this becomes tedious if you have different input variables and want others to use your script easily, too. Let's change the script in such a way that lmrtfy can create a profile which can be used to deploy the function and make it available to other users: # file: free_fall_lmrtfy.py # import the required things from lmrtfy from lmrtfy import variable , result standard_gravity = 9.81 # annotate an input time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # annotate an output velocity = result ( 9.81 * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) If you run python free_fall_lmrtfy.py you get the exact same result as before. During the run, lmrtfy created the profile for free_fall_lmrtfy.py which will be needed to deploy the function. Create the annotation profile \u00b6 It is required to run your script at least once with the regular python interpreter to create the annotation profile which will be used to generate the API. $ python <script.py> The profile is currently saved under ~/.lmrtfy/profiles which will change in the future to respect XDG directory specifications. Focus: variable and result \u00b6 The API reference for variable and result can be found here . These functions are transparent. That means the assignment a = variable(5, name=\"a\") assigns a the value 5 . This way you can run the script simply with your local python interpreter if lmrtfy is installed in the environment. variable and result do not have any external dependency (e.g. API calls) The functions signatures are as follows. Only the first two arguments are required. Ideally, the name argument should match the name of the variable you assign to, although that is not necessary. It's considered to be a best practice, because it reduces possible errors and fosters a more intuitive understanding of the code. # variable signature: variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type # result signature: result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type The value argument specifies the default argument for the variable and has to be one of the supported types: int, float, complex, bool, np.ndarray, list, dict . name declares the name of the variable that will be used for the API generation. A sensible choice is the same name as the variable's name in the code itself. min and max can be used to specify boundaries for the input and output values in case they are numeric. This might come in handy if the code only works for certain input parameter ranges. If the inputs are outside the specified range the job will be rejected by the generated API. unit is a str that declares the unit of the variable/result. This is especially useful in scientific calculations where units are often not standardized and unclear.","title":"Annotate your script"},{"location":"tutorial/annotation/#annotate-your-script","text":"The annotation of your script tells the lmrtfy tool which python variables are considered inputs and outputs, which is done via the variable and results functions. This step is important, because lmrtfy traces the calls to variable and result to create a profile for the code. This profile includes the inputs and outputs as well as the additional meta information ( min , max , unit , and possibly more in the future). Let's assume that you have create a script to calculate the velocity of an object after a certain time: # file: free_fall.py standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now, if you want to recalculate for a different time, you would edit the script and run it again. While this might work for a small script like this, this becomes tedious if you have different input variables and want others to use your script easily, too. Let's change the script in such a way that lmrtfy can create a profile which can be used to deploy the function and make it available to other users: # file: free_fall_lmrtfy.py # import the required things from lmrtfy from lmrtfy import variable , result standard_gravity = 9.81 # annotate an input time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # annotate an output velocity = result ( 9.81 * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) If you run python free_fall_lmrtfy.py you get the exact same result as before. During the run, lmrtfy created the profile for free_fall_lmrtfy.py which will be needed to deploy the function.","title":"Annotate your script"},{"location":"tutorial/annotation/#create-the-annotation-profile","text":"It is required to run your script at least once with the regular python interpreter to create the annotation profile which will be used to generate the API. $ python <script.py> The profile is currently saved under ~/.lmrtfy/profiles which will change in the future to respect XDG directory specifications.","title":"Create the annotation profile"},{"location":"tutorial/annotation/#focus-variable-and-result","text":"The API reference for variable and result can be found here . These functions are transparent. That means the assignment a = variable(5, name=\"a\") assigns a the value 5 . This way you can run the script simply with your local python interpreter if lmrtfy is installed in the environment. variable and result do not have any external dependency (e.g. API calls) The functions signatures are as follows. Only the first two arguments are required. Ideally, the name argument should match the name of the variable you assign to, although that is not necessary. It's considered to be a best practice, because it reduces possible errors and fosters a more intuitive understanding of the code. # variable signature: variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type # result signature: result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type The value argument specifies the default argument for the variable and has to be one of the supported types: int, float, complex, bool, np.ndarray, list, dict . name declares the name of the variable that will be used for the API generation. A sensible choice is the same name as the variable's name in the code itself. min and max can be used to specify boundaries for the input and output values in case they are numeric. This might come in handy if the code only works for certain input parameter ranges. If the inputs are outside the specified range the job will be rejected by the generated API. unit is a str that declares the unit of the variable/result. This is especially useful in scientific calculations where units are often not standardized and unclear.","title":"Focus: variable and result"},{"location":"tutorial/deployment/","text":"Deploy the function (local runner) \u00b6 Now you can deploy the function and make it available via the LMRTFY API. This is simply done by running $ lmrtfy deploy <path_to_script.py> --local The --local flag means that the script will run locally on your computer and waits for jobs from the outside. The LMRTFY API only allows job submissions that fit your deployed annotation profile. In the future you will be able to deploy directly to the cloud. Then, you do not have to host the runner yourself. Warning Don't change the script after you have deployed it. The current advice would be to copy and rename the script before deployment. In later versions, this will be taken care of by the lmrtfy tool. When a job is submitted the types of the job's input parameters are checked by the lmrtfy API. Futhermore they are also checked for their bounds and their units. This way, only jobs that can be run successfully with the script belonging to the deployed profile. Deploying to the cloud \u00b6 LMRTFY is currently not able to deploy your scripts directly to the cloud; however, you can do this manually by using lmrtfy inside of a docker container. The docker container can be run on your laptop, one of your own servers or in the cloud. The current workaround would be to use lmrtfy on a server or inside a docker container which can be hosted in the cloud. Inside the docker container you run the same command as if you were to run locally. Note In the future, we will provide a sample docker container to simplify that.","title":"Deploy your script/function"},{"location":"tutorial/deployment/#deploy-the-function-local-runner","text":"Now you can deploy the function and make it available via the LMRTFY API. This is simply done by running $ lmrtfy deploy <path_to_script.py> --local The --local flag means that the script will run locally on your computer and waits for jobs from the outside. The LMRTFY API only allows job submissions that fit your deployed annotation profile. In the future you will be able to deploy directly to the cloud. Then, you do not have to host the runner yourself. Warning Don't change the script after you have deployed it. The current advice would be to copy and rename the script before deployment. In later versions, this will be taken care of by the lmrtfy tool. When a job is submitted the types of the job's input parameters are checked by the lmrtfy API. Futhermore they are also checked for their bounds and their units. This way, only jobs that can be run successfully with the script belonging to the deployed profile.","title":"Deploy the function (local runner)"},{"location":"tutorial/deployment/#deploying-to-the-cloud","text":"LMRTFY is currently not able to deploy your scripts directly to the cloud; however, you can do this manually by using lmrtfy inside of a docker container. The docker container can be run on your laptop, one of your own servers or in the cloud. The current workaround would be to use lmrtfy on a server or inside a docker container which can be hosted in the cloud. Inside the docker container you run the same command as if you were to run locally. Note In the future, we will provide a sample docker container to simplify that.","title":"Deploying to the cloud"},{"location":"tutorial/fetch_results/","text":"Fetching results in your code \u00b6 When you call a function from your code, the results are part of the job object created when calling the function . You can only get the results when they are ready by using job.results : while not job . ready : sleep ( 1. ) print ( job . results ) Currently, the while loop is necessary to wait for the results. This isn't the most ergonomic way to do this. This could easily become a future (in the sense of concurrent programming) later on. We are also looking for feedback, what would work best for you. Get results with the CLI \u00b6 LMRTFY also provides a way to download the results of the computation. All you need is the <job_id> that you received when you submitted the job. Then, you simply run $ lmrtfy fetch <job_id> <save_path> The results will be saved in <save_path>/<job_id>/.. . Each result is currently saved as a JSON file with the following format: { \"<var_name>\" : < value > } Each variable has its own file. Warning This will very likely change in the future to be more ergonomic.","title":"Get results"},{"location":"tutorial/fetch_results/#fetching-results-in-your-code","text":"When you call a function from your code, the results are part of the job object created when calling the function . You can only get the results when they are ready by using job.results : while not job . ready : sleep ( 1. ) print ( job . results ) Currently, the while loop is necessary to wait for the results. This isn't the most ergonomic way to do this. This could easily become a future (in the sense of concurrent programming) later on. We are also looking for feedback, what would work best for you.","title":"Fetching results in your code"},{"location":"tutorial/fetch_results/#get-results-with-the-cli","text":"LMRTFY also provides a way to download the results of the computation. All you need is the <job_id> that you received when you submitted the job. Then, you simply run $ lmrtfy fetch <job_id> <save_path> The results will be saved in <save_path>/<job_id>/.. . Each result is currently saved as a JSON file with the following format: { \"<var_name>\" : < value > } Each variable has its own file. Warning This will very likely change in the future to be more ergonomic.","title":"Get results with the CLI"},{"location":"tutorial/installation/","text":"There are two ways to install the lmrtfy . We recommend the usage of virtual environments at the moment due to the frequent updates and changes of lmrtfy. Linux and MacOS \u00b6 We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy Windows \u00b6 On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy This way you will always have the most recent release of lmrtfy . Install from Source \u00b6 You can also install from git which is the best way to use the nightly features. Clone the git repository and install manually: $ git clone --branch main https://github.com/lmrtfy/lmrtfy.git $ cd lmrtfy $ pip install . The main branch is the release branch and should always work with the lmrtfy API. Alternatively, you can use the develop branch. This should be the most up-to-date branch in the repository, but things might break. So be careful while using the develop branch.","title":"Installation"},{"location":"tutorial/installation/#linux-and-macos","text":"We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy","title":"Linux and MacOS"},{"location":"tutorial/installation/#windows","text":"On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy This way you will always have the most recent release of lmrtfy .","title":"Windows"},{"location":"tutorial/installation/#install-from-source","text":"You can also install from git which is the best way to use the nightly features. Clone the git repository and install manually: $ git clone --branch main https://github.com/lmrtfy/lmrtfy.git $ cd lmrtfy $ pip install . The main branch is the release branch and should always work with the lmrtfy API. Alternatively, you can use the develop branch. This should be the most up-to-date branch in the repository, but things might break. So be careful while using the develop branch.","title":"Install from Source"},{"location":"tutorial/login/","text":"To use LMRTFY you need to login to get an API token. The API token is necassary to deploy scripts as a callable function submit jobs as call cloud functions When you run an LMRTFY actions that require a token you will automatically be asked to login. Each token is valid for 24h before you need to relogin. If you allow cookies, you will probably not have to login again because the authentication provider Auth0 recognizes you. Info Tokens are currently valid for 24 hours. After that you will be requested to login again. That also means that you cannot have scripts deployed more than 24 hours right now. This will change soon so that you can deploy scripts longer than that. Just run lmrtfy deploy <script> --local again after 24h and you are fine if you need longer running deployments right now.) Sign-Up \u00b6 Before you can login the first time you need to sign up. Currently, you have two options to do that: email adress + password google account If you would like to sign up with another social login (e.g. GitHub) please let us know so that we can prioritize this issue.","title":"Login"},{"location":"tutorial/login/#sign-up","text":"Before you can login the first time you need to sign up. Currently, you have two options to do that: email adress + password google account If you would like to sign up with another social login (e.g. GitHub) please let us know so that we can prioritize this issue.","title":"Sign-Up"},{"location":"tutorial/submission/","text":"Calling a Deployed Function \u00b6 Let's assume that you have just deployed the script calc_compound_interest.py from the examples provided in examples/compound_interest/calc_compound_interest.py by running $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local Do not close that terminal as there won't be a runner to receive jobs in that case. Calling a Function from Code \u00b6 Our goal is to provide an interface to deployed functions that works just like any other function in any other library that you have locally installed. We provide an example call_compound_interest.py' in the same directory as the calc_compound_interest.py` script. As you can see, it's very close to a native function already. # file: examples/compound_interest/call_compound_interest.py from time import sleep from lmrtfy import catalog #1 job = catalog . calc_compound_interest ( 5. , 10. , 5 ) #2 if job : print ( job . id , job . status ) while not job . ready : #3 sleep ( 1. ) print ( job . results ) #4 Run the script with your local python interpreter python examples/compound_interest/call_compound_interest.py to make use of the deployed script. The output of the script looks like this: INFO Validating auth token. INFO Auth token accepted. INFO Valid access token found. Login not necessary. INFO Updated function catalog. INFO Added function calc_compound_interest. INFO Job 78b69e45-3d51-4734-92db-a4901ee6d02b created. Status is RUNNING. 78b69e45-3d51-4734-92db-a4901ee6d02b JobStatus.RUNNING { 'compound_interest' : 2 .7628156250000035 } The ID of the job is going to be different from the one shown in the example output. Let's discuss some aspects of the code a little closer: Importing the catalog triggers the catalog update to get newly deployed functions every time you run the code The function calc_compount_interest is now part of the catalog and can be called just like a normal function in your code, however this function is not executed locally and run wherever the corresponding runner is deployed. Loop until the job is ready. In this context ready means that the results are ready to be fetched. which can simply be done by calling job.results . The return value is a dictionary with the keys corresponding to the names of the results and the values are the actual values of the result. Using the CLI \u00b6 LMRTFY also provides a way to submit jobs with the lmrtfy CLI tool. All you need for this is a profile_id which is provided by you during the deployment and a JSON file that contains the input parameters. Attention This is a good way to call deployed scripts from another language as you can always build the JSON file and call the lmrtfy CLI. If you are using it this we, please contact us. We want to provide more native-feeling interfaces to languages other than python as well but would love to hear what you use to priortize. For the example calculating the compound interest, the JSON file would look like this: { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } argument_values and argument_units contain a key-value pair each for each of the inputs in the annotation profile. The types need to match exactly. No implicit type casting in performed during the submission. The unit also has to match exactly. Save the JSON file es input.json and run: $ lmrtfy submit <profile_id> input.json Info Later on, we might perform automatic conversion in case of a unit mismatch, e.g. if the profile requires s (as in seconds) but the input is given as h (as in hours). There will be an option to enable/disable the function. If you have any opinions about that, please let us know When you submit your job you will receive a job_id which is needed to fetch the results as you will see in the next part of this guide.","title":"Submit a job"},{"location":"tutorial/submission/#calling-a-deployed-function","text":"Let's assume that you have just deployed the script calc_compound_interest.py from the examples provided in examples/compound_interest/calc_compound_interest.py by running $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local Do not close that terminal as there won't be a runner to receive jobs in that case.","title":"Calling a Deployed Function"},{"location":"tutorial/submission/#calling-a-function-from-code","text":"Our goal is to provide an interface to deployed functions that works just like any other function in any other library that you have locally installed. We provide an example call_compound_interest.py' in the same directory as the calc_compound_interest.py` script. As you can see, it's very close to a native function already. # file: examples/compound_interest/call_compound_interest.py from time import sleep from lmrtfy import catalog #1 job = catalog . calc_compound_interest ( 5. , 10. , 5 ) #2 if job : print ( job . id , job . status ) while not job . ready : #3 sleep ( 1. ) print ( job . results ) #4 Run the script with your local python interpreter python examples/compound_interest/call_compound_interest.py to make use of the deployed script. The output of the script looks like this: INFO Validating auth token. INFO Auth token accepted. INFO Valid access token found. Login not necessary. INFO Updated function catalog. INFO Added function calc_compound_interest. INFO Job 78b69e45-3d51-4734-92db-a4901ee6d02b created. Status is RUNNING. 78b69e45-3d51-4734-92db-a4901ee6d02b JobStatus.RUNNING { 'compound_interest' : 2 .7628156250000035 } The ID of the job is going to be different from the one shown in the example output. Let's discuss some aspects of the code a little closer: Importing the catalog triggers the catalog update to get newly deployed functions every time you run the code The function calc_compount_interest is now part of the catalog and can be called just like a normal function in your code, however this function is not executed locally and run wherever the corresponding runner is deployed. Loop until the job is ready. In this context ready means that the results are ready to be fetched. which can simply be done by calling job.results . The return value is a dictionary with the keys corresponding to the names of the results and the values are the actual values of the result.","title":"Calling a Function from Code"},{"location":"tutorial/submission/#using-the-cli","text":"LMRTFY also provides a way to submit jobs with the lmrtfy CLI tool. All you need for this is a profile_id which is provided by you during the deployment and a JSON file that contains the input parameters. Attention This is a good way to call deployed scripts from another language as you can always build the JSON file and call the lmrtfy CLI. If you are using it this we, please contact us. We want to provide more native-feeling interfaces to languages other than python as well but would love to hear what you use to priortize. For the example calculating the compound interest, the JSON file would look like this: { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } argument_values and argument_units contain a key-value pair each for each of the inputs in the annotation profile. The types need to match exactly. No implicit type casting in performed during the submission. The unit also has to match exactly. Save the JSON file es input.json and run: $ lmrtfy submit <profile_id> input.json Info Later on, we might perform automatic conversion in case of a unit mismatch, e.g. if the profile requires s (as in seconds) but the input is given as h (as in hours). There will be an option to enable/disable the function. If you have any opinions about that, please let us know When you submit your job you will receive a job_id which is needed to fetch the results as you will see in the next part of this guide.","title":"Using the CLI"}]}