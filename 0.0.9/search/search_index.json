{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","tags":["Introduction"],"text":"Let Me Run That For You. \u2705 Create functions that run in the cloud , on your servers or even on your laptop . \u2705 Call them from code that runs somewhere else, just like a regular function . \u2705 Share functions with friends and colleagues, track their usage and monetize their usage. \u2705 Works with Python, but more languages will be added in the future. Quickstart Guide Tutorial Examples API Reference How to report issues and how to contribute Introduction \u00b6 LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possible to use LMRTFY Call deployed function like any other function provided by a local library Info LMRTFY is currently in an early phase. Things will likely change in future releases. Quickstart - TL;DR \u00b6 install with pip install lmrtfy login/sign up with lmrtfy login annotate your code's inputs with variable and its outputs with result deploy the script: lmrtfy deploy examples/deployment/calc_compound_interest.py --local Use the deployed function (from another terminal, or another computer!): open examples/calling_cloud_functions/call_function.py run python examples/call_deployed_function.py to call the deployed function and get the results. As you can see in step 5, it's as simple as calling a regular function from any other library you have installed locally. Examples \u00b6 The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one!","title":"Introduction"},{"location":"#introduction","text":"LMRTFY is a tool to share scripts via the cloud. Your scripts can run on your laptop, on your server or in the cloud. You and everybody you shared your deployed script with can call the function straight from their own code using the lmrtfy package We strive to provide a frictionless developer experience: Change as little code as possible to use LMRTFY Call deployed function like any other function provided by a local library Info LMRTFY is currently in an early phase. Things will likely change in future releases.","title":"Introduction"},{"location":"#quickstart-tldr","text":"install with pip install lmrtfy login/sign up with lmrtfy login annotate your code's inputs with variable and its outputs with result deploy the script: lmrtfy deploy examples/deployment/calc_compound_interest.py --local Use the deployed function (from another terminal, or another computer!): open examples/calling_cloud_functions/call_function.py run python examples/call_deployed_function.py to call the deployed function and get the results. As you can see in step 5, it's as simple as calling a regular function from any other library you have installed locally.","title":"Quickstart - TL;DR"},{"location":"#examples","text":"The examples are provided in the examples/ directory. They are work in progress . As lmrtfy matures, more and more examples will be added. If you miss an example for a specific use case, please let us know, and we will add one!","title":"Examples"},{"location":"CHANGELOG/","text":"v0.0.9 - 10/oct/2022 \u00b6 short UUIDs for jobs and profiles examples to use bug fixes revised documentation (structure, content, ...) sharing v0.0.8 - 16/sep/2022 \u00b6 fixed missing dependencies for installation v0.0.7 - 16/sep/2022 \u00b6 introduced catalog feature to call 'cloud' functions directly from code. enhanced documentation (API reference, links, quickstart, structure, ...) general bug fixes for better stability v0.0.6 - 08/sep/2022 \u00b6 changed input formats for json as follows: { \"profile_id\" : \"<profile_id>\" , \"job_parameters\" : { \"time\" : 200.0 }, \"parameter_units\" : { \"time\" : \"s\" } } to { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } modified readme to match docs added check if job_id is valid UUID v0.0.5 - 06/sep/2022 \u00b6 included version check changed listener to remove profile collisions v0.0.4 - 05/sep/2022 \u00b6 First release This creates the following commands: * lmrtfy login => get token * lmrtfy deploy <script.py> --local => get profile_id for deployed script profile * lmrtfy submit <profile_id> <input.json> => get job_id for submitted job * lmrtfy fetch <job_id> <path_to_save> => get results for finished job","title":"Changelog"},{"location":"CHANGELOG/#v009-10oct2022","text":"short UUIDs for jobs and profiles examples to use bug fixes revised documentation (structure, content, ...) sharing","title":"v0.0.9 - 10/oct/2022"},{"location":"CHANGELOG/#v008-16sep2022","text":"fixed missing dependencies for installation","title":"v0.0.8 - 16/sep/2022"},{"location":"CHANGELOG/#v007-16sep2022","text":"introduced catalog feature to call 'cloud' functions directly from code. enhanced documentation (API reference, links, quickstart, structure, ...) general bug fixes for better stability","title":"v0.0.7 - 16/sep/2022"},{"location":"CHANGELOG/#v006-08sep2022","text":"changed input formats for json as follows: { \"profile_id\" : \"<profile_id>\" , \"job_parameters\" : { \"time\" : 200.0 }, \"parameter_units\" : { \"time\" : \"s\" } } to { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } modified readme to match docs added check if job_id is valid UUID","title":"v0.0.6 - 08/sep/2022"},{"location":"CHANGELOG/#v005-06sep2022","text":"included version check changed listener to remove profile collisions","title":"v0.0.5 - 06/sep/2022"},{"location":"CHANGELOG/#v004-05sep2022","text":"First release This creates the following commands: * lmrtfy login => get token * lmrtfy deploy <script.py> --local => get profile_id for deployed script profile * lmrtfy submit <profile_id> <input.json> => get job_id for submitted job * lmrtfy fetch <job_id> <path_to_save> => get results for finished job","title":"v0.0.4 - 05/sep/2022"},{"location":"cli_reference/","text":"The lmrtfy CLI tools uses the following schema: lmrtfy [ COMMAND ] [ COMMAND_OPTIONS ] where [COMMAND] is one of the following: login , logouot , deploy , submit , and fetch . Commands \u00b6 login \u00b6 The usage of the LMRTFY services is connected to a user accouunt. With lmrtfy login you can directly trigger the login process. This is usually not necessary, because each command that needs a valid authorization token will trigger the login if needed. logout \u00b6 Logout of the currently active account. This should only be necessary in special cases, e.g. if you have more than one account for some reason (private and work account for example). deploy <scipt> [OPTIONS] \u00b6 This command deploys the <script> and makes it available via the LMRTFY platform. It's now ready to take jobs. Note Currently, only local deployment via the --local flag works. If you want to deploy to the cloud you need to manually do this and run lmrtfy deploy <script> --local on the remote resource. Option Description --local Deploy locally on the current system. --run_as_daemon Run the deployment as daemon in the background. submit <profile_id> <arguments.json> \u00b6 The CLI allows you to submit jobs for a specific profile_id which is returned by the deploy step. <argument.json> needs to contain valid input data, otherwise the job is rejected. The structure of the JSON file can be found here . fetch <job_id> <path> \u00b6 It's also possible to fetch the results from a job with job_id . The job ID is displayed during the submission step. With <path> you specify where the results should be saved.","title":"CLI reference"},{"location":"cli_reference/#commands","text":"","title":"Commands"},{"location":"cli_reference/#login","text":"The usage of the LMRTFY services is connected to a user accouunt. With lmrtfy login you can directly trigger the login process. This is usually not necessary, because each command that needs a valid authorization token will trigger the login if needed.","title":"login"},{"location":"cli_reference/#logout","text":"Logout of the currently active account. This should only be necessary in special cases, e.g. if you have more than one account for some reason (private and work account for example).","title":"logout"},{"location":"cli_reference/#deploy-scipt-options","text":"This command deploys the <script> and makes it available via the LMRTFY platform. It's now ready to take jobs. Note Currently, only local deployment via the --local flag works. If you want to deploy to the cloud you need to manually do this and run lmrtfy deploy <script> --local on the remote resource. Option Description --local Deploy locally on the current system. --run_as_daemon Run the deployment as daemon in the background.","title":"deploy &lt;scipt&gt; [OPTIONS]"},{"location":"cli_reference/#submit-profile_id-argumentsjson","text":"The CLI allows you to submit jobs for a specific profile_id which is returned by the deploy step. <argument.json> needs to contain valid input data, otherwise the job is rejected. The structure of the JSON file can be found here .","title":"submit &lt;profile_id&gt; &lt;arguments.json&gt;"},{"location":"cli_reference/#fetch-job_id-path","text":"It's also possible to fetch the results from a job with job_id . The job ID is displayed during the submission step. With <path> you specify where the results should be saved.","title":"fetch &lt;job_id&gt; &lt;path&gt;"},{"location":"contributing/","text":"We welcome all contributors and their contributions. All contributors will be listed in the project (if wanted). What to contribute? \u00b6 There are several ways that you can contribute to lmrtfy . Report bugs or suggest features Work on the code: fix a bug, create a feature, ... Improve the documentation Improving the documentation often isn't glamorous work but is highly appreciated by us. If you think something is missing in the documentation or something needs more information, please let us know or add the documentation. Contribute code \u00b6 The process of contributing code is quite easy: Fork the lmrtfy repository. Let us know what you want to work on to ensure that we can merge your code. Best to create an issue . Fix a bug, add a feature, ... Create a pull request . This process is standard in most repositories. If you have any questions or need help contributing, please let us know! Contribute documentation \u00b6 The files required to build the documentation are inside the docs directory. They are plain markdown files that can be easily edited. The documentation is built with mkdocs and the mkdocs-material theme. Everything you need to build the docs locally can be installed by pip install -r docs/requirements.txt . Reading the documentation of mkdocs and the material theme is highly recommend.","title":"Contributing code"},{"location":"contributing/#what-to-contribute","text":"There are several ways that you can contribute to lmrtfy . Report bugs or suggest features Work on the code: fix a bug, create a feature, ... Improve the documentation Improving the documentation often isn't glamorous work but is highly appreciated by us. If you think something is missing in the documentation or something needs more information, please let us know or add the documentation.","title":"What to contribute?"},{"location":"contributing/#contribute-code","text":"The process of contributing code is quite easy: Fork the lmrtfy repository. Let us know what you want to work on to ensure that we can merge your code. Best to create an issue . Fix a bug, add a feature, ... Create a pull request . This process is standard in most repositories. If you have any questions or need help contributing, please let us know!","title":"Contribute code"},{"location":"contributing/#contribute-documentation","text":"The files required to build the documentation are inside the docs directory. They are plain markdown files that can be easily edited. The documentation is built with mkdocs and the mkdocs-material theme. Everything you need to build the docs locally can be installed by pip install -r docs/requirements.txt . Reading the documentation of mkdocs and the material theme is highly recommend.","title":"Contribute documentation"},{"location":"quickstart/","text":"This is our quick start guide. Everything you need to know to get started is here. Installation \u00b6 You can install the lmrtfy package just like any other package: $ pip install lmrtfy If you use conda you need to install pip in your conda environment before you can use LMRTFY. This is likely necessary on Windows. $ conda install pip $ pip install lmrtfy Sign-Up \u00b6 To make full use of LMRTFY you need to sign up with us with lmrtfy login . Besides an e-mail-based sign up we also provide social logins via GitHub and Google for ease of use. Calling your first remote function \u00b6 Calling a function that is available in the cloud is as easy as calling a native function. call_example1.py 1 2 3 4 5 6 7 8 9 10 from time import sleep from lmrtfy.functions import catalog job = catalog . examples . example1 ( args ) # (1)! while job and not job . ready : # (2)! sleep ( 1. ) if job : print ( job . results ) # (3)! This calls the function example1 in the LMRTFY namespace example . The function is executed on a remote resource. If job is not valid the call did not succeed. job.ready queries the job status and becomes true once the results are ready. job.results gets the results from LMRTFY as JSON: { \"<variable name>\" : <value> , ... } As you can see calling the function example1 looks just like calling any other function in Python; however, it is actually eexcuted on a remote server, in this case on our own server as we provide the example. Each call to a deployed function returns a Job object if the submission was successful. If an error occurred None is returned. This way we can check if the submission was successful or not. To get the results from the computation we can simply call job.results . Depending on the size of the results this call will take a while. In the example, this call should at most take a few seconds. Deploying your first function \u00b6 Deploying your own function is really easy, too. Let's say you want to calculate how fast a falling object will be after \\(x\\) seconds. The code which can be used with LMRTFY is really simple: free_fall_lmrtfy.py 1 2 3 4 5 6 7 8 9 from lmrtfy.annotation import variable , result # (1)! time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # (2)! standard_gravity = 9.81 velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) velocity = result ( velocity , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) # (3)! variable and result are the imports that are needed to annotate your code to make it work with LMRTFY. variable annotates any inputs of your script. result annotates a result of script. A script can have multiple results. The highlighted lines have been added or changed to let LMRTFY know about the structure of the script. Before we can deploy the function we need to run it through the Python interpreter once to create the annotation profile. $ python free_fall_lmrtfy.py To deploy this function on your laptop you simply run $ lmrtfy deploy free_fall_lmrtfy.py --local This makes the function available in your catalog as free_fall_lmrtfy and starts a runner on your laptop. Nobody but you can call the function unless you share it with others. When you stop the runner calling this function will not yield any results. Calling the deployed function becomes as simple as catalog . free_fall_lmrtfy ( time = 200. ) .","title":"Quick start"},{"location":"quickstart/#installation","text":"You can install the lmrtfy package just like any other package: $ pip install lmrtfy If you use conda you need to install pip in your conda environment before you can use LMRTFY. This is likely necessary on Windows. $ conda install pip $ pip install lmrtfy","title":"Installation"},{"location":"quickstart/#sign-up","text":"To make full use of LMRTFY you need to sign up with us with lmrtfy login . Besides an e-mail-based sign up we also provide social logins via GitHub and Google for ease of use.","title":"Sign-Up"},{"location":"quickstart/#calling-your-first-remote-function","text":"Calling a function that is available in the cloud is as easy as calling a native function. call_example1.py 1 2 3 4 5 6 7 8 9 10 from time import sleep from lmrtfy.functions import catalog job = catalog . examples . example1 ( args ) # (1)! while job and not job . ready : # (2)! sleep ( 1. ) if job : print ( job . results ) # (3)! This calls the function example1 in the LMRTFY namespace example . The function is executed on a remote resource. If job is not valid the call did not succeed. job.ready queries the job status and becomes true once the results are ready. job.results gets the results from LMRTFY as JSON: { \"<variable name>\" : <value> , ... } As you can see calling the function example1 looks just like calling any other function in Python; however, it is actually eexcuted on a remote server, in this case on our own server as we provide the example. Each call to a deployed function returns a Job object if the submission was successful. If an error occurred None is returned. This way we can check if the submission was successful or not. To get the results from the computation we can simply call job.results . Depending on the size of the results this call will take a while. In the example, this call should at most take a few seconds.","title":"Calling your first remote function"},{"location":"quickstart/#deploying-your-first-function","text":"Deploying your own function is really easy, too. Let's say you want to calculate how fast a falling object will be after \\(x\\) seconds. The code which can be used with LMRTFY is really simple: free_fall_lmrtfy.py 1 2 3 4 5 6 7 8 9 from lmrtfy.annotation import variable , result # (1)! time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # (2)! standard_gravity = 9.81 velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) velocity = result ( velocity , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) # (3)! variable and result are the imports that are needed to annotate your code to make it work with LMRTFY. variable annotates any inputs of your script. result annotates a result of script. A script can have multiple results. The highlighted lines have been added or changed to let LMRTFY know about the structure of the script. Before we can deploy the function we need to run it through the Python interpreter once to create the annotation profile. $ python free_fall_lmrtfy.py To deploy this function on your laptop you simply run $ lmrtfy deploy free_fall_lmrtfy.py --local This makes the function available in your catalog as free_fall_lmrtfy and starts a runner on your laptop. Nobody but you can call the function unless you share it with others. When you stop the runner calling this function will not yield any results. Calling the deployed function becomes as simple as catalog . free_fall_lmrtfy ( time = 200. ) .","title":"Deploying your first function"},{"location":"report_bugs/","text":"When you find an issue, don't hesitate to create an issue to let us know about the bug. When you create an issue there are 3 options: Create a bug report \u00b6 Reporting bugs is very important. We are very thankful for each bug that is reported and will try to fix them as soon as possible. The clearer the description of the bug is and the more information you provide, the better. Request a feature \u00b6 This is the best choice if there is a feature you are missing. Each feature request will be reviewed. If the feature aligns with our vision of LMRTFY. If we are going to implement the feature it will be added to the roadmap. Ask a question \u00b6 This the best choice if you just want to ask a question about LMRTFY: Is a feature planned? How to do X? *... Open a blank issue \u00b6 This should be the last resort and only be used if your concern does not fit into any of the other categories.","title":"Reporting an issue"},{"location":"report_bugs/#create-a-bug-report","text":"Reporting bugs is very important. We are very thankful for each bug that is reported and will try to fix them as soon as possible. The clearer the description of the bug is and the more information you provide, the better.","title":"Create a bug report"},{"location":"report_bugs/#request-a-feature","text":"This is the best choice if there is a feature you are missing. Each feature request will be reviewed. If the feature aligns with our vision of LMRTFY. If we are going to implement the feature it will be added to the roadmap.","title":"Request a feature"},{"location":"report_bugs/#ask-a-question","text":"This the best choice if you just want to ask a question about LMRTFY: Is a feature planned? How to do X? *...","title":"Ask a question"},{"location":"report_bugs/#open-a-blank-issue","text":"This should be the last resort and only be used if your concern does not fit into any of the other categories.","title":"Open a blank issue"},{"location":"roadmap/","text":"This roadmap should give you an idea of the features we will work on in the future. If you find any feature highly relevant for you, please let us know: hello@lmrt.fyi . LMRTFY Library \u00b6 Annotation \u00b6 annotate resources in your code, e.g. big data sets support for more languages (help us prioritize with your feedback): JavaScript R Matlab C++ Julia ... Catalog (calling cloud functions like native ones) \u00b6 support for more languages (see above) enhance job and results handling (async handling, context) LMRTFY CLI tool \u00b6 cloud deployment via lmrtfy deploy deploy to one of the big cloud service providers (AWS, Azure, GCP, ...) deploy to your own servers (private cloud) LMRTFY web application \u00b6 accounting system for deployed functions (monetize your functions) overview of your functions manage functions see available runners sharing functions with others functions activity graph","title":"Roadmap"},{"location":"roadmap/#lmrtfy-library","text":"","title":"LMRTFY Library"},{"location":"roadmap/#annotation","text":"annotate resources in your code, e.g. big data sets support for more languages (help us prioritize with your feedback): JavaScript R Matlab C++ Julia ...","title":"Annotation"},{"location":"roadmap/#catalog-calling-cloud-functions-like-native-ones","text":"support for more languages (see above) enhance job and results handling (async handling, context)","title":"Catalog (calling cloud functions like native ones)"},{"location":"roadmap/#lmrtfy-cli-tool","text":"cloud deployment via lmrtfy deploy deploy to one of the big cloud service providers (AWS, Azure, GCP, ...) deploy to your own servers (private cloud)","title":"LMRTFY CLI tool"},{"location":"roadmap/#lmrtfy-web-application","text":"accounting system for deployed functions (monetize your functions) overview of your functions manage functions see available runners sharing functions with others functions activity graph","title":"LMRTFY web application"},{"location":"api_reference/annotation/","text":"result ( value , name , min = None , max = None , unit = None ) \u00b6 result defines an input parameter of the script. It is used to create the profile for the code. r1 = result(5*v1, name='res1') creates a result named 'res1' in the profile. If run with the python interpreter r1 is equal to 5*v1 Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric result takes. Currently not checked. None max [optional] Maximum value that a numeric result takes. Currently not checked. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `result` defines an input parameter of the script. It is used to create the profile for the code. `r1 = result(5*v1, name='res1')` creates a result named 'res1' in the profile. If run with the python interpreter `r1` is equal to `5*v1` :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric result takes. Currently not checked. :param max: [optional] Maximum value that a numeric result takes. Currently not checked. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'result' , dtype = _get_type ( value ), unit = None , min = min , max = max ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with ( open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_result_ { name } .json' )), 'w' )) as f : json . dump ({ name : value }, f , cls = NumpyEncoder ) logging . info ( f \"Running: Saved result ' { name } ' with value ' { value } '.\" ) return value variable ( value , name , min = None , max = None , unit = None ) \u00b6 variable defines an input parameter of the script. It is used to create the profile for the code. v1 = variable(5, name='var1') create a variable named 'var1' in the profile. If run with the python interpreter v1 takes the value 5 and can be used in the rest of the code just like any other variable. Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric variable takes. Checked by API. None max [optional] Maximum value that a numeric variable takes. Checked by API. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `variable` defines an input parameter of the script. It is used to create the profile for the code. `v1 = variable(5, name='var1')` create a variable named 'var1' in the profile. If run with the python interpreter `v1` takes the value 5 and can be used in the rest of the code just like any other variable. :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric variable takes. Checked by API. :param max: [optional] Maximum value that a numeric variable takes. Checked by API. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'variable' , dtype = _get_type ( value ), min = min , max = max , unit = unit ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_variable_ { name } .json' )), 'r' ) as f : tmp = json . load ( f ) # TODO: Make it work for numpy dtypes. dtype = _inverse_type_map [ type ( value )] value = dtype ( tmp [ name ]) logging . info ( f \"Running: Loaded variable ' { name } ' of type ' { dtype } ' and value ' { value } '.\" ) return value","title":"Annotation"},{"location":"api_reference/annotation/#src.lmrtfy.annotation.result","text":"result defines an input parameter of the script. It is used to create the profile for the code. r1 = result(5*v1, name='res1') creates a result named 'res1' in the profile. If run with the python interpreter r1 is equal to 5*v1 Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric result takes. Currently not checked. None max [optional] Maximum value that a numeric result takes. Currently not checked. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def result ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `result` defines an input parameter of the script. It is used to create the profile for the code. `r1 = result(5*v1, name='res1')` creates a result named 'res1' in the profile. If run with the python interpreter `r1` is equal to `5*v1` :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric result takes. Currently not checked. :param max: [optional] Maximum value that a numeric result takes. Currently not checked. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. This is currently not checked and saved but will be used in the future. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'result' , dtype = _get_type ( value ), unit = None , min = min , max = max ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with ( open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_result_ { name } .json' )), 'w' )) as f : json . dump ({ name : value }, f , cls = NumpyEncoder ) logging . info ( f \"Running: Saved result ' { name } ' with value ' { value } '.\" ) return value","title":"result()"},{"location":"api_reference/annotation/#src.lmrtfy.annotation.variable","text":"variable defines an input parameter of the script. It is used to create the profile for the code. v1 = variable(5, name='var1') create a variable named 'var1' in the profile. If run with the python interpreter v1 takes the value 5 and can be used in the rest of the code just like any other variable. Parameters: Name Type Description Default value supported_object_type Value that is used if script is run with the python interpreter. Can be any expression. required name str Name of the variable used by the API for job submission. required min [optional] Minimum value that a numeric variable takes. Checked by API. None max [optional] Maximum value that a numeric variable takes. Checked by API. None unit str [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. None Returns: Type Description supported_object_type value Source code in src/lmrtfy/annotation/__init__.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def variable ( value : supported_object_type , name : str , min = None , max = None , unit : str = None ) -> supported_object_type : \"\"\" `variable` defines an input parameter of the script. It is used to create the profile for the code. `v1 = variable(5, name='var1')` create a variable named 'var1' in the profile. If run with the python interpreter `v1` takes the value 5 and can be used in the rest of the code just like any other variable. :param value: Value that is used if script is run with the python interpreter. Can be any expression. :param name: Name of the variable used by the API for job submission. :param min: [optional] Minimum value that a numeric variable takes. Checked by API. :param max: [optional] Maximum value that a numeric variable takes. Checked by API. :param unit: [optional] A string declaring the unit. This will likely change to support \u00b4pynt` units. :return: `value` \"\"\" _add_to_api_definition ( name , kind = 'variable' , dtype = _get_type ( value ), min = min , max = max , unit = unit ) if _run_deployed and _tmp_dir and _script_path : # TODO: warn and except if something fails. with open ( str ( _tmp_dir . joinpath ( f 'lmrtfy_variable_ { name } .json' )), 'r' ) as f : tmp = json . load ( f ) # TODO: Make it work for numpy dtypes. dtype = _inverse_type_map [ type ( value )] value = dtype ( tmp [ name ]) logging . info ( f \"Running: Loaded variable ' { name } ' of type ' { dtype } ' and value ' { value } '.\" ) return value","title":"variable()"},{"location":"api_reference/catalog/","text":"Catalog \u00b6 Bases: object The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during from lmrtfy import catalog . If you want to retrieve newly deployed function, call catalog.update() . To run a deployed function from the catalog call catalog.<namespace>.<deployed_function>(*args, **kwargs) . Each function that has been pulled into the catalog is available via the help() command. Source code in src/lmrtfy/functions.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 class Catalog ( object ): \"\"\" The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during `from lmrtfy import catalog`. If you want to retrieve newly deployed function, call `catalog.update()`. To run a deployed function from the catalog call `catalog.<namespace>.<deployed_function>(*args, **kwargs)`. Each function that has been pulled into the catalog is available via the `help()` command. \"\"\" def __init__ ( self ): h = LoginHandler () if h . login (): h . get_token () self . config = get_cliconfig () logging . info ( self . config ) self . token = load_token_data ()[ 'access_token' ] self . headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { self . token } \" } self . profiles = None self . update () def __add_function ( self , namespace , name , sig , res_ann , pid , template ): template = fetch_template ( pid ) def f ( ** kwargs ) -> Job : f . pid = pid for p in kwargs : template [ 'argument_values' ][ p ] = kwargs [ p ] r = requests . post ( self . config [ 'api_submit_url' ] + f '/ { pid } ' , data = json . dumps ( template , cls = NumpyEncoder ), headers = self . headers ) if r . status_code == 200 : return Job ( r . json ()[ 'job_id' ]) if r . status_code == 400 : logging . error ( f 'Input Error: { r . json () } ' ) elif r . status_code == 404 : logging . error ( f \" { r . json () } \" ) setattr ( namespace , name , create_function ( sig , f , func_name = name , qualname = f \" { namespace . __name__ } - { pid } \" )) def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" r = requests . get ( self . config [ 'api_namespaces_url' ], headers = self . headers ) logging . info ( r . json ()) namespaces = r . json ()[ 'namespaces' ] for n in namespaces : try : delattr ( self , n . split ( '/' )[ 0 ]) except : pass for n in namespaces : names = n . split ( '/' ) o = self nsn = \"\" for name in names : nsn += f \" { name } /\" if not hasattr ( o , name ): setattr ( o , name , Namespace ( nsn [: - 1 ])) o = getattr ( o , name ) try : r = requests . get ( f \" { self . config [ 'api_namespaces_url' ] } / { n . replace ( '/' , '-' ) } \" , headers = self . headers ) functions = r . json ()[ 'functions' ] for func in functions : func_name = functions [ func ][ 'name' ] if not hasattr ( o , func_name ): self . __add_function ( o , func_name , * signature_from_profile ( functions [ func ]), pid = func , template = functions [ func ]) except : # TODO: Except clause too broad! logging . error ( f \"Could not namespace { n } function catalog.\" ) def create_namespace ( self , namespace : Namespace , name : str ) -> bool : \"\"\" Create a unique namespace that collects functions and can be shared. Example: ```python catalog.create_namespace(catalog.<namespace>, \"new_namespace\") ``` :param namespace: parent namespace :param name: Name of the new namespace. :return: Returns the `True` on success and `False` in case of an error. \"\"\" try : new_name = f \" { namespace . __name__ } / { name } \" . replace ( '/' , '-' ) r = requests . post ( f \" { self . config [ 'api_namespaces_url' ] } \" , data = json . dumps ({ \"namespace\" : new_name }), headers = self . headers ) if r . status_code == 201 : self . update () return True except : pass logging . error ( f \"Could not create namespace { name } .\" ) return False def add_function_to_namespace ( self , namespace : Namespace , function : Function ): \"\"\" Add a function to an existing namespace. Example: ```python catalog.add_function_to_namespace(catalog.<namespace>, catalog.<namespace>.<function> ``` \"\"\" # PUT r = requests . put ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False def delete_namespace ( self , namespace : Namespace ) -> bool : \"\"\" Deletes the entire namespace. Deletion will fail if there are still functions in the namespace. \"\"\" # DELETE r = requests . delete ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False def remove_function_from_namespace ( self , function : Function ) -> bool : \"\"\" Delete `function` from its namespace. \"\"\" # PATCH namespace = function . __qualname__ . split ( '-' )[ 0 ] r = requests . patch ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False def share_namespace ( self , namespace : Namespace , recipient_email : str ) -> Optional [ str ]: \"\"\" Share a namespace with someone else by email. The invite email will be sent to `recipient_email`. This does not have to be the email address associated with the account of the person you want to share the namespace with. Invites only work once. \"\"\" id_token = load_token_data ()[ 'id_token' ] sender_email = jwt . decode ( id_token , options = { \"verify_signature\" : False })[ \"email\" ] r = requests . post ( f \" { self . config [ 'api_invites_url' ] } \" , data = json . dumps ({ \"namespace\" : namespace . __name__ , \"recipient_email\" : recipient_email , \"sender_email\" : sender_email }), headers = self . headers ) if r . status_code == 202 : return r . json ()[ \"invite_id\" ] def accept_invite ( self , invite_id ) -> bool : r = requests . get ( f \" { self . config [ 'api_invites_url' ] } / { invite_id } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False add_function_to_namespace ( namespace , function ) \u00b6 Add a function to an existing namespace. Example: catalog . add_function_to_namespace ( catalog .< namespace > , catalog .< namespace >.< function > Source code in src/lmrtfy/functions.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def add_function_to_namespace ( self , namespace : Namespace , function : Function ): \"\"\" Add a function to an existing namespace. Example: ```python catalog.add_function_to_namespace(catalog.<namespace>, catalog.<namespace>.<function> ``` \"\"\" # PUT r = requests . put ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False create_namespace ( namespace , name ) \u00b6 Create a unique namespace that collects functions and can be shared. Example: catalog . create_namespace ( catalog .< namespace > , \"new_namespace\" ) Parameters: Name Type Description Default namespace Namespace parent namespace required name str Name of the new namespace. required Returns: Type Description bool Returns the True on success and False in case of an error. Source code in src/lmrtfy/functions.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def create_namespace ( self , namespace : Namespace , name : str ) -> bool : \"\"\" Create a unique namespace that collects functions and can be shared. Example: ```python catalog.create_namespace(catalog.<namespace>, \"new_namespace\") ``` :param namespace: parent namespace :param name: Name of the new namespace. :return: Returns the `True` on success and `False` in case of an error. \"\"\" try : new_name = f \" { namespace . __name__ } / { name } \" . replace ( '/' , '-' ) r = requests . post ( f \" { self . config [ 'api_namespaces_url' ] } \" , data = json . dumps ({ \"namespace\" : new_name }), headers = self . headers ) if r . status_code == 201 : self . update () return True except : pass logging . error ( f \"Could not create namespace { name } .\" ) return False delete_namespace ( namespace ) \u00b6 Deletes the entire namespace. Deletion will fail if there are still functions in the namespace. Source code in src/lmrtfy/functions.py 267 268 269 270 271 272 273 274 275 276 277 278 279 def delete_namespace ( self , namespace : Namespace ) -> bool : \"\"\" Deletes the entire namespace. Deletion will fail if there are still functions in the namespace. \"\"\" # DELETE r = requests . delete ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False remove_function_from_namespace ( function ) \u00b6 Delete function from its namespace. Source code in src/lmrtfy/functions.py 281 282 283 284 285 286 287 288 289 290 291 292 293 def remove_function_from_namespace ( self , function : Function ) -> bool : \"\"\" Delete `function` from its namespace. \"\"\" # PATCH namespace = function . __qualname__ . split ( '-' )[ 0 ] r = requests . patch ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False share_namespace ( namespace , recipient_email ) \u00b6 Share a namespace with someone else by email. The invite email will be sent to recipient_email . This does not have to be the email address associated with the account of the person you want to share the namespace with. Invites only work once. Source code in src/lmrtfy/functions.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def share_namespace ( self , namespace : Namespace , recipient_email : str ) -> Optional [ str ]: \"\"\" Share a namespace with someone else by email. The invite email will be sent to `recipient_email`. This does not have to be the email address associated with the account of the person you want to share the namespace with. Invites only work once. \"\"\" id_token = load_token_data ()[ 'id_token' ] sender_email = jwt . decode ( id_token , options = { \"verify_signature\" : False })[ \"email\" ] r = requests . post ( f \" { self . config [ 'api_invites_url' ] } \" , data = json . dumps ({ \"namespace\" : namespace . __name__ , \"recipient_email\" : recipient_email , \"sender_email\" : sender_email }), headers = self . headers ) if r . status_code == 202 : return r . json ()[ \"invite_id\" ] update () \u00b6 Call update to update the catalog with newly deployed functions. Source code in src/lmrtfy/functions.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" r = requests . get ( self . config [ 'api_namespaces_url' ], headers = self . headers ) logging . info ( r . json ()) namespaces = r . json ()[ 'namespaces' ] for n in namespaces : try : delattr ( self , n . split ( '/' )[ 0 ]) except : pass for n in namespaces : names = n . split ( '/' ) o = self nsn = \"\" for name in names : nsn += f \" { name } /\" if not hasattr ( o , name ): setattr ( o , name , Namespace ( nsn [: - 1 ])) o = getattr ( o , name ) try : r = requests . get ( f \" { self . config [ 'api_namespaces_url' ] } / { n . replace ( '/' , '-' ) } \" , headers = self . headers ) functions = r . json ()[ 'functions' ] for func in functions : func_name = functions [ func ][ 'name' ] if not hasattr ( o , func_name ): self . __add_function ( o , func_name , * signature_from_profile ( functions [ func ]), pid = func , template = functions [ func ]) except : # TODO: Except clause too broad! logging . error ( f \"Could not namespace { n } function catalog.\" ) Job \u00b6 Bases: object Source code in src/lmrtfy/functions.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Job ( object ): def __init__ ( self , job_id ): self . id = job_id logging . info ( f \"Job { self . id } created. Status is { self . status } .\" ) @property def status ( self ): \"\"\" Queries the job status. If it returns `JobStatus.UNKNOWN` the job is likely in failed state. \"\"\" try : token = load_token_data ()[ 'access_token' ] headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { token } \" } r = requests . get ( get_cliconfig ()[ 'api_job_url' ] + f '/ { self . id } ' , headers = headers ) status = JobStatus ( r . json ()) except Exception as e : logging . error ( e ) status = JobStatus . UNKNOWN try : with open ( _lmrtfy_job_dir . joinpath ( f \" { self . id } .job\" ), \"w\" ) as f : json . dump ({ \"id\" : self . id , \"status\" : status }, f ) except : logging . error ( f \"Could not write local job-file for job { self . id } .\" ) return status @property def results ( self ) -> Optional [ dict ]: \"\"\" Returns the results if they are ready and `None` otherwise. Results are returned as a dictionary: ```json { \"<results_name>\": result_value } ``` \"\"\" return fetch_results ( self . id ) @property def ready ( self ): \"\"\" Returns `True` if the job has finished successfully and the results are ready to be fetched. \"\"\" if self . status == JobStatus . RESULTS_READY : return True return False ready () property \u00b6 Returns True if the job has finished successfully and the results are ready to be fetched. Source code in src/lmrtfy/functions.py 88 89 90 91 92 93 94 95 @property def ready ( self ): \"\"\" Returns `True` if the job has finished successfully and the results are ready to be fetched. \"\"\" if self . status == JobStatus . RESULTS_READY : return True return False results () property \u00b6 Returns the results if they are ready and None otherwise. Results are returned as a dictionary: { \"<results_name>\" : resul t _value } Source code in src/lmrtfy/functions.py 75 76 77 78 79 80 81 82 83 84 85 86 @property def results ( self ) -> Optional [ dict ]: \"\"\" Returns the results if they are ready and `None` otherwise. Results are returned as a dictionary: ```json { \"<results_name>\": result_value } ``` \"\"\" return fetch_results ( self . id ) status () property \u00b6 Queries the job status. If it returns JobStatus.UNKNOWN the job is likely in failed state. Source code in src/lmrtfy/functions.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @property def status ( self ): \"\"\" Queries the job status. If it returns `JobStatus.UNKNOWN` the job is likely in failed state. \"\"\" try : token = load_token_data ()[ 'access_token' ] headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { token } \" } r = requests . get ( get_cliconfig ()[ 'api_job_url' ] + f '/ { self . id } ' , headers = headers ) status = JobStatus ( r . json ()) except Exception as e : logging . error ( e ) status = JobStatus . UNKNOWN try : with open ( _lmrtfy_job_dir . joinpath ( f \" { self . id } .job\" ), \"w\" ) as f : json . dump ({ \"id\" : self . id , \"status\" : status }, f ) except : logging . error ( f \"Could not write local job-file for job { self . id } .\" ) return status","title":"Catalog"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog","text":"Bases: object The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during from lmrtfy import catalog . If you want to retrieve newly deployed function, call catalog.update() . To run a deployed function from the catalog call catalog.<namespace>.<deployed_function>(*args, **kwargs) . Each function that has been pulled into the catalog is available via the help() command. Source code in src/lmrtfy/functions.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 class Catalog ( object ): \"\"\" The Catalog object provides an interface to deployed functions that you can run from your code. Cloud functions are pulled into the catalog by the constructor, which happens during `from lmrtfy import catalog`. If you want to retrieve newly deployed function, call `catalog.update()`. To run a deployed function from the catalog call `catalog.<namespace>.<deployed_function>(*args, **kwargs)`. Each function that has been pulled into the catalog is available via the `help()` command. \"\"\" def __init__ ( self ): h = LoginHandler () if h . login (): h . get_token () self . config = get_cliconfig () logging . info ( self . config ) self . token = load_token_data ()[ 'access_token' ] self . headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { self . token } \" } self . profiles = None self . update () def __add_function ( self , namespace , name , sig , res_ann , pid , template ): template = fetch_template ( pid ) def f ( ** kwargs ) -> Job : f . pid = pid for p in kwargs : template [ 'argument_values' ][ p ] = kwargs [ p ] r = requests . post ( self . config [ 'api_submit_url' ] + f '/ { pid } ' , data = json . dumps ( template , cls = NumpyEncoder ), headers = self . headers ) if r . status_code == 200 : return Job ( r . json ()[ 'job_id' ]) if r . status_code == 400 : logging . error ( f 'Input Error: { r . json () } ' ) elif r . status_code == 404 : logging . error ( f \" { r . json () } \" ) setattr ( namespace , name , create_function ( sig , f , func_name = name , qualname = f \" { namespace . __name__ } - { pid } \" )) def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" r = requests . get ( self . config [ 'api_namespaces_url' ], headers = self . headers ) logging . info ( r . json ()) namespaces = r . json ()[ 'namespaces' ] for n in namespaces : try : delattr ( self , n . split ( '/' )[ 0 ]) except : pass for n in namespaces : names = n . split ( '/' ) o = self nsn = \"\" for name in names : nsn += f \" { name } /\" if not hasattr ( o , name ): setattr ( o , name , Namespace ( nsn [: - 1 ])) o = getattr ( o , name ) try : r = requests . get ( f \" { self . config [ 'api_namespaces_url' ] } / { n . replace ( '/' , '-' ) } \" , headers = self . headers ) functions = r . json ()[ 'functions' ] for func in functions : func_name = functions [ func ][ 'name' ] if not hasattr ( o , func_name ): self . __add_function ( o , func_name , * signature_from_profile ( functions [ func ]), pid = func , template = functions [ func ]) except : # TODO: Except clause too broad! logging . error ( f \"Could not namespace { n } function catalog.\" ) def create_namespace ( self , namespace : Namespace , name : str ) -> bool : \"\"\" Create a unique namespace that collects functions and can be shared. Example: ```python catalog.create_namespace(catalog.<namespace>, \"new_namespace\") ``` :param namespace: parent namespace :param name: Name of the new namespace. :return: Returns the `True` on success and `False` in case of an error. \"\"\" try : new_name = f \" { namespace . __name__ } / { name } \" . replace ( '/' , '-' ) r = requests . post ( f \" { self . config [ 'api_namespaces_url' ] } \" , data = json . dumps ({ \"namespace\" : new_name }), headers = self . headers ) if r . status_code == 201 : self . update () return True except : pass logging . error ( f \"Could not create namespace { name } .\" ) return False def add_function_to_namespace ( self , namespace : Namespace , function : Function ): \"\"\" Add a function to an existing namespace. Example: ```python catalog.add_function_to_namespace(catalog.<namespace>, catalog.<namespace>.<function> ``` \"\"\" # PUT r = requests . put ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False def delete_namespace ( self , namespace : Namespace ) -> bool : \"\"\" Deletes the entire namespace. Deletion will fail if there are still functions in the namespace. \"\"\" # DELETE r = requests . delete ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False def remove_function_from_namespace ( self , function : Function ) -> bool : \"\"\" Delete `function` from its namespace. \"\"\" # PATCH namespace = function . __qualname__ . split ( '-' )[ 0 ] r = requests . patch ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False def share_namespace ( self , namespace : Namespace , recipient_email : str ) -> Optional [ str ]: \"\"\" Share a namespace with someone else by email. The invite email will be sent to `recipient_email`. This does not have to be the email address associated with the account of the person you want to share the namespace with. Invites only work once. \"\"\" id_token = load_token_data ()[ 'id_token' ] sender_email = jwt . decode ( id_token , options = { \"verify_signature\" : False })[ \"email\" ] r = requests . post ( f \" { self . config [ 'api_invites_url' ] } \" , data = json . dumps ({ \"namespace\" : namespace . __name__ , \"recipient_email\" : recipient_email , \"sender_email\" : sender_email }), headers = self . headers ) if r . status_code == 202 : return r . json ()[ \"invite_id\" ] def accept_invite ( self , invite_id ) -> bool : r = requests . get ( f \" { self . config [ 'api_invites_url' ] } / { invite_id } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False","title":"Catalog"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.add_function_to_namespace","text":"Add a function to an existing namespace. Example: catalog . add_function_to_namespace ( catalog .< namespace > , catalog .< namespace >.< function > Source code in src/lmrtfy/functions.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def add_function_to_namespace ( self , namespace : Namespace , function : Function ): \"\"\" Add a function to an existing namespace. Example: ```python catalog.add_function_to_namespace(catalog.<namespace>, catalog.<namespace>.<function> ``` \"\"\" # PUT r = requests . put ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False","title":"add_function_to_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.create_namespace","text":"Create a unique namespace that collects functions and can be shared. Example: catalog . create_namespace ( catalog .< namespace > , \"new_namespace\" ) Parameters: Name Type Description Default namespace Namespace parent namespace required name str Name of the new namespace. required Returns: Type Description bool Returns the True on success and False in case of an error. Source code in src/lmrtfy/functions.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def create_namespace ( self , namespace : Namespace , name : str ) -> bool : \"\"\" Create a unique namespace that collects functions and can be shared. Example: ```python catalog.create_namespace(catalog.<namespace>, \"new_namespace\") ``` :param namespace: parent namespace :param name: Name of the new namespace. :return: Returns the `True` on success and `False` in case of an error. \"\"\" try : new_name = f \" { namespace . __name__ } / { name } \" . replace ( '/' , '-' ) r = requests . post ( f \" { self . config [ 'api_namespaces_url' ] } \" , data = json . dumps ({ \"namespace\" : new_name }), headers = self . headers ) if r . status_code == 201 : self . update () return True except : pass logging . error ( f \"Could not create namespace { name } .\" ) return False","title":"create_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.delete_namespace","text":"Deletes the entire namespace. Deletion will fail if there are still functions in the namespace. Source code in src/lmrtfy/functions.py 267 268 269 270 271 272 273 274 275 276 277 278 279 def delete_namespace ( self , namespace : Namespace ) -> bool : \"\"\" Deletes the entire namespace. Deletion will fail if there are still functions in the namespace. \"\"\" # DELETE r = requests . delete ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . __name__ . replace ( '/' , '-' ) } \" , headers = self . headers ) if r . status_code == 202 : self . update () return True return False","title":"delete_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.remove_function_from_namespace","text":"Delete function from its namespace. Source code in src/lmrtfy/functions.py 281 282 283 284 285 286 287 288 289 290 291 292 293 def remove_function_from_namespace ( self , function : Function ) -> bool : \"\"\" Delete `function` from its namespace. \"\"\" # PATCH namespace = function . __qualname__ . split ( '-' )[ 0 ] r = requests . patch ( f \" { self . config [ 'api_namespaces_url' ] } / { namespace . replace ( '/' , '-' ) } \" , data = json . dumps ({ 'function' : function . __qualname__ . split ( '-' )[ - 1 ]}), headers = self . headers ) if r . status_code == 202 : self . update () return True return False","title":"remove_function_from_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.share_namespace","text":"Share a namespace with someone else by email. The invite email will be sent to recipient_email . This does not have to be the email address associated with the account of the person you want to share the namespace with. Invites only work once. Source code in src/lmrtfy/functions.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def share_namespace ( self , namespace : Namespace , recipient_email : str ) -> Optional [ str ]: \"\"\" Share a namespace with someone else by email. The invite email will be sent to `recipient_email`. This does not have to be the email address associated with the account of the person you want to share the namespace with. Invites only work once. \"\"\" id_token = load_token_data ()[ 'id_token' ] sender_email = jwt . decode ( id_token , options = { \"verify_signature\" : False })[ \"email\" ] r = requests . post ( f \" { self . config [ 'api_invites_url' ] } \" , data = json . dumps ({ \"namespace\" : namespace . __name__ , \"recipient_email\" : recipient_email , \"sender_email\" : sender_email }), headers = self . headers ) if r . status_code == 202 : return r . json ()[ \"invite_id\" ]","title":"share_namespace()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Catalog.update","text":"Call update to update the catalog with newly deployed functions. Source code in src/lmrtfy/functions.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def update ( self ): \"\"\" Call `update` to update the catalog with newly deployed functions. \"\"\" r = requests . get ( self . config [ 'api_namespaces_url' ], headers = self . headers ) logging . info ( r . json ()) namespaces = r . json ()[ 'namespaces' ] for n in namespaces : try : delattr ( self , n . split ( '/' )[ 0 ]) except : pass for n in namespaces : names = n . split ( '/' ) o = self nsn = \"\" for name in names : nsn += f \" { name } /\" if not hasattr ( o , name ): setattr ( o , name , Namespace ( nsn [: - 1 ])) o = getattr ( o , name ) try : r = requests . get ( f \" { self . config [ 'api_namespaces_url' ] } / { n . replace ( '/' , '-' ) } \" , headers = self . headers ) functions = r . json ()[ 'functions' ] for func in functions : func_name = functions [ func ][ 'name' ] if not hasattr ( o , func_name ): self . __add_function ( o , func_name , * signature_from_profile ( functions [ func ]), pid = func , template = functions [ func ]) except : # TODO: Except clause too broad! logging . error ( f \"Could not namespace { n } function catalog.\" )","title":"update()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Job","text":"Bases: object Source code in src/lmrtfy/functions.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class Job ( object ): def __init__ ( self , job_id ): self . id = job_id logging . info ( f \"Job { self . id } created. Status is { self . status } .\" ) @property def status ( self ): \"\"\" Queries the job status. If it returns `JobStatus.UNKNOWN` the job is likely in failed state. \"\"\" try : token = load_token_data ()[ 'access_token' ] headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { token } \" } r = requests . get ( get_cliconfig ()[ 'api_job_url' ] + f '/ { self . id } ' , headers = headers ) status = JobStatus ( r . json ()) except Exception as e : logging . error ( e ) status = JobStatus . UNKNOWN try : with open ( _lmrtfy_job_dir . joinpath ( f \" { self . id } .job\" ), \"w\" ) as f : json . dump ({ \"id\" : self . id , \"status\" : status }, f ) except : logging . error ( f \"Could not write local job-file for job { self . id } .\" ) return status @property def results ( self ) -> Optional [ dict ]: \"\"\" Returns the results if they are ready and `None` otherwise. Results are returned as a dictionary: ```json { \"<results_name>\": result_value } ``` \"\"\" return fetch_results ( self . id ) @property def ready ( self ): \"\"\" Returns `True` if the job has finished successfully and the results are ready to be fetched. \"\"\" if self . status == JobStatus . RESULTS_READY : return True return False","title":"Job"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Job.ready","text":"Returns True if the job has finished successfully and the results are ready to be fetched. Source code in src/lmrtfy/functions.py 88 89 90 91 92 93 94 95 @property def ready ( self ): \"\"\" Returns `True` if the job has finished successfully and the results are ready to be fetched. \"\"\" if self . status == JobStatus . RESULTS_READY : return True return False","title":"ready()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Job.results","text":"Returns the results if they are ready and None otherwise. Results are returned as a dictionary: { \"<results_name>\" : resul t _value } Source code in src/lmrtfy/functions.py 75 76 77 78 79 80 81 82 83 84 85 86 @property def results ( self ) -> Optional [ dict ]: \"\"\" Returns the results if they are ready and `None` otherwise. Results are returned as a dictionary: ```json { \"<results_name>\": result_value } ``` \"\"\" return fetch_results ( self . id )","title":"results()"},{"location":"api_reference/catalog/#src.lmrtfy.functions.Job.status","text":"Queries the job status. If it returns JobStatus.UNKNOWN the job is likely in failed state. Source code in src/lmrtfy/functions.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @property def status ( self ): \"\"\" Queries the job status. If it returns `JobStatus.UNKNOWN` the job is likely in failed state. \"\"\" try : token = load_token_data ()[ 'access_token' ] headers = { 'Content-type' : 'application/json' , 'Accept' : 'text/plain' , \"Authorization\" : f \"Bearer { token } \" } r = requests . get ( get_cliconfig ()[ 'api_job_url' ] + f '/ { self . id } ' , headers = headers ) status = JobStatus ( r . json ()) except Exception as e : logging . error ( e ) status = JobStatus . UNKNOWN try : with open ( _lmrtfy_job_dir . joinpath ( f \" { self . id } .job\" ), \"w\" ) as f : json . dump ({ \"id\" : self . id , \"status\" : status }, f ) except : logging . error ( f \"Could not write local job-file for job { self . id } .\" ) return status","title":"status()"},{"location":"examples/compound_interest/","text":"Example 3: Compound interest \u00b6 The third example calculates the compound interest \\(C\\) starting from a principal value \\(P\\) with annualt interest \\(I\\) after \\(N\\) years: \\[ C = P \\cdot (1 + I)^N - P \\] Very common formula in anything related to finance. Again, we start with the plain code, as you would implement it right away: calc_compound_interest.py 1 2 3 4 5 6 7 8 def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal principal = 10_000 interest = 6 years = 10 ci = compound_interest ( principal , interest , years ) print ( f \"Compound interest after { years } years: { ci } \" ) You can run this example with $ python ci.py and it should print 7908.47 . Which is the compound interest after 10 years if you started with 10000 units that grow by 6% each year. There are several problems with this solution: 1. You need to change the code to run it for other inputs 2. Units are unclear! principal is a currency, but that actually does not matter. The real problem is the interest . Is it decimal or in %? Annotate with lmrtfy \u00b6 Using lmrtfy, you would annotate the script as follows: calc_compound_interest_lmrtfy.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from lmrtfy.annotation import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal principal = 10_000. interest = 6. years = 10 principal = variable ( principal , name = \"principal\" , min = 0 ) annual_interest = variable ( principal , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ) years = variable ( years , name = \"years\" , min = 0 ) ci = compound_interest ( principal , annual_interest , years ) ci = result ( ci , name = \"compound_interest\" ) Now, we run python calc_compound_interest_lmrtfy_1.py to generate the profile. Warning The type annotation are not enforced when run locally. LMRTFY checks the types and units only if jobs are submitted through its API. This guarantees that you can run your code without our service. Deployment \u00b6 After creating the profile we can easily deploy with $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local Call compound_interest from code \u00b6 Similar to the other examples we just need to import the catalog and call the correct function: call_compound_interest.py 1 2 3 4 5 6 7 8 9 10 11 12 from time import sleep from lmrtfy.functions import catalog job = catalog .< your_namespace >. calc_compound_interest_lmrtfy ( 5. , 10. , 5 ) # (1)! if job : print ( job . id , job . status ) while not job . ready : sleep ( 1. ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . Call compound_interest from the CLI \u00b6 The output should be similar to this: INFO Profile_id to be used for requests: <profile_id> The <profile_id> is important to submit jobs. To submit a job you are currently required to save the input parameters as JSON (e.g. input.json ): input.json to calculate compound interest { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } Now, we have everything that is needed to start a job: $ lmrtfy submit <profile_id> input.json The job id for this job is printed to the terminal: INFO Job-id: <job_id> We need the <job_id> later to fetch the results from the computation. Alternative Annotation \u00b6 A more compact but working alternative is to create the result as follows: Alternative annotation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from lmrtfy.annotation import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : ci = result ( compound_interest ( principal = variable ( 10000. , name = \"principal\" , min = 0 ), annual_interest = variable ( 6 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ), years = variable ( 10 , name = \"years\" , min = 0 ) ), name = \"compound_interest\" ) print ( ci ) It's not necessarily prettier to look at, but also works!","title":"Compound interest"},{"location":"examples/compound_interest/#example-3-compound-interest","text":"The third example calculates the compound interest \\(C\\) starting from a principal value \\(P\\) with annualt interest \\(I\\) after \\(N\\) years: \\[ C = P \\cdot (1 + I)^N - P \\] Very common formula in anything related to finance. Again, we start with the plain code, as you would implement it right away: calc_compound_interest.py 1 2 3 4 5 6 7 8 def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal principal = 10_000 interest = 6 years = 10 ci = compound_interest ( principal , interest , years ) print ( f \"Compound interest after { years } years: { ci } \" ) You can run this example with $ python ci.py and it should print 7908.47 . Which is the compound interest after 10 years if you started with 10000 units that grow by 6% each year. There are several problems with this solution: 1. You need to change the code to run it for other inputs 2. Units are unclear! principal is a currency, but that actually does not matter. The real problem is the interest . Is it decimal or in %?","title":"Example 3: Compound interest"},{"location":"examples/compound_interest/#annotate-with-lmrtfy","text":"Using lmrtfy, you would annotate the script as follows: calc_compound_interest_lmrtfy.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from lmrtfy.annotation import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal principal = 10_000. interest = 6. years = 10 principal = variable ( principal , name = \"principal\" , min = 0 ) annual_interest = variable ( principal , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ) years = variable ( years , name = \"years\" , min = 0 ) ci = compound_interest ( principal , annual_interest , years ) ci = result ( ci , name = \"compound_interest\" ) Now, we run python calc_compound_interest_lmrtfy_1.py to generate the profile. Warning The type annotation are not enforced when run locally. LMRTFY checks the types and units only if jobs are submitted through its API. This guarantees that you can run your code without our service.","title":"Annotate with lmrtfy"},{"location":"examples/compound_interest/#deployment","text":"After creating the profile we can easily deploy with $ lmrtfy deploy examples/compound_interest/calc_compound_interest.py --local","title":"Deployment"},{"location":"examples/compound_interest/#call-compound_interest-from-code","text":"Similar to the other examples we just need to import the catalog and call the correct function: call_compound_interest.py 1 2 3 4 5 6 7 8 9 10 11 12 from time import sleep from lmrtfy.functions import catalog job = catalog .< your_namespace >. calc_compound_interest_lmrtfy ( 5. , 10. , 5 ) # (1)! if job : print ( job . id , job . status ) while not job . ready : sleep ( 1. ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() .","title":"Call compound_interest from code"},{"location":"examples/compound_interest/#call-compound_interest-from-the-cli","text":"The output should be similar to this: INFO Profile_id to be used for requests: <profile_id> The <profile_id> is important to submit jobs. To submit a job you are currently required to save the input parameters as JSON (e.g. input.json ): input.json to calculate compound interest { \"argument_values\" : { \"annual_interest\" : 6.0 , \"principal\" : 5000.0 , \"years\" : 10 }, \"argument_units\" : { \"annual_interest\" : \"%\" } } Now, we have everything that is needed to start a job: $ lmrtfy submit <profile_id> input.json The job id for this job is printed to the terminal: INFO Job-id: <job_id> We need the <job_id> later to fetch the results from the computation.","title":"Call compound_interest from the CLI"},{"location":"examples/compound_interest/#alternative-annotation","text":"A more compact but working alternative is to create the result as follows: Alternative annotation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from lmrtfy.annotation import variable , result def compound_interest ( principal : float , annual_interest : float , years : int ): return principal * ( 1. + annual_interest / 100. ) ** years - principal if __name__ == \"__main__\" : ci = result ( compound_interest ( principal = variable ( 10000. , name = \"principal\" , min = 0 ), annual_interest = variable ( 6 , name = \"annual_interest\" , min = 0 , max = 100 , unit = \"%\" ), years = variable ( 10 , name = \"years\" , min = 0 ) ), name = \"compound_interest\" ) print ( ci ) It's not necessarily prettier to look at, but also works!","title":"Alternative Annotation"},{"location":"examples/free_fall/","text":"Example 2: Velocity due to gravtity in free fall \u00b6 The second example calculates the velocity of an object falling from the sky (without air resistance). The standard gravity on earth is 9.81 m*s^(-2). Multiplicated by the fall time, we will get the velocity of the object after that time. If you like equations more, you might recognize these from your physics class: $$ v = g \\cdot t $$ In regular python code that you run locally it would look like this: free_fall.py 1 2 3 4 5 standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now we want to be able to share that functionality via the lmrtfy web API. All we have to do is decide which variables are considered to be an input and a result of the computation: free_fall_lmrtfy.py 1 2 3 4 5 6 7 from lmrtfy import variable , result standard_gravity = 9.81 time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) velocity = result ( standard_gravity * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m/s\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now run $ python examples/velocity_from_gravity/calc_velocity.py to generate the required profile. This way you can also check if your code is actually working the way you expect it. Deploying the script \u00b6 To deploy, you simply run $ lmrtfy deploy examples/velocity_from_gravity/calc_velocity.py --local . Do not stop that process, because than you will not be able to submit a job. Calling from code \u00b6 Calling free_fall_lmrtfy by code as easy as it was for the first example . calc_free_fall.py 1 2 3 4 5 6 7 8 9 10 import time from lmrtfy.functions import catalog job = catalog .< your_namespace >. free_fall_lmrtfy ( time = 100.0 ) # (1)! if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . Note You can also run help(free_fall_lmrtfy) to see the corresponding help. Right now, only the function signature is shown but in the future you will also be able to see the docstrings. Calling from CLI \u00b6 Open a new terminal in the same directory and run $ lmrtfy submit <profile_id> . The profile_id has been printed in the lmrtfy deploy step. This does not work right out of the box, because you need to specify a JSON file that contains the input parameters for your job. A template for that JSON should have been printed in the CLI. Create such a JSON file and name it input.json and put values of the correct type into the values (no type conversion is happening in the API, so if float is required, you cannot input an int ). Alternatively, use the provided input.json in examples/free_fall/input.json : { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } Now run $ lmrtfy submit <profile_id> examples/free_fall/input.json . You will receive a job_id which we will shortly need to fetch the results after they are computed. After your job has run, you can get the results by running $ lmrtfy fetch <job_id> <path to store results> . The results are downloaded and stored inside the specified path within a directory that has the job_id as its name.","title":"Free fall"},{"location":"examples/free_fall/#example-2-velocity-due-to-gravtity-in-free-fall","text":"The second example calculates the velocity of an object falling from the sky (without air resistance). The standard gravity on earth is 9.81 m*s^(-2). Multiplicated by the fall time, we will get the velocity of the object after that time. If you like equations more, you might recognize these from your physics class: $$ v = g \\cdot t $$ In regular python code that you run locally it would look like this: free_fall.py 1 2 3 4 5 standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now we want to be able to share that functionality via the lmrtfy web API. All we have to do is decide which variables are considered to be an input and a result of the computation: free_fall_lmrtfy.py 1 2 3 4 5 6 7 from lmrtfy import variable , result standard_gravity = 9.81 time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) velocity = result ( standard_gravity * time , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m/s\" ) print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now run $ python examples/velocity_from_gravity/calc_velocity.py to generate the required profile. This way you can also check if your code is actually working the way you expect it.","title":"Example 2: Velocity due to gravtity in free fall"},{"location":"examples/free_fall/#deploying-the-script","text":"To deploy, you simply run $ lmrtfy deploy examples/velocity_from_gravity/calc_velocity.py --local . Do not stop that process, because than you will not be able to submit a job.","title":"Deploying the script"},{"location":"examples/free_fall/#calling-from-code","text":"Calling free_fall_lmrtfy by code as easy as it was for the first example . calc_free_fall.py 1 2 3 4 5 6 7 8 9 10 import time from lmrtfy.functions import catalog job = catalog .< your_namespace >. free_fall_lmrtfy ( time = 100.0 ) # (1)! if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . Note You can also run help(free_fall_lmrtfy) to see the corresponding help. Right now, only the function signature is shown but in the future you will also be able to see the docstrings.","title":"Calling from code"},{"location":"examples/free_fall/#calling-from-cli","text":"Open a new terminal in the same directory and run $ lmrtfy submit <profile_id> . The profile_id has been printed in the lmrtfy deploy step. This does not work right out of the box, because you need to specify a JSON file that contains the input parameters for your job. A template for that JSON should have been printed in the CLI. Create such a JSON file and name it input.json and put values of the correct type into the values (no type conversion is happening in the API, so if float is required, you cannot input an int ). Alternatively, use the provided input.json in examples/free_fall/input.json : { \"argument_values\" : { \"time\" : 6.0 }, \"argument_units\" : { \"time\" : \"s\" } } Now run $ lmrtfy submit <profile_id> examples/free_fall/input.json . You will receive a job_id which we will shortly need to fetch the results after they are computed. After your job has run, you can get the results by running $ lmrtfy fetch <job_id> <path to store results> . The results are downloaded and stored inside the specified path within a directory that has the job_id as its name.","title":"Calling from CLI"},{"location":"examples/starting_example/","text":"Example 1: Simple annotation \u00b6 This is a simple example to showcase the general usage of lmrtfy. It can be found in examples/starting_example/example1.py . The two core concepts are the variable and result functions which annotate the inputs and outputs of the script. They are needed to create the profile which is used to create the API. example1.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from lmrtfy.annotation import variable , result # (1)! x = variable ( 5 , name = \"x\" , min = 1 , max = 10 ) # (2)! y = variable ( np . linspace ( 0. , 1. , 101 , dtype = np . float64 ), name = \"y\" , min =- 1. , max = 11. , unit = \"m\" ) # (3)! z = variable ( \"abc\" , name = \"z\" ) z1 = variable ([ \"abc\" , \"def\" ], name = \"z1\" ) # (4)! z2 = variable ([ \"abc\" , 1 , 1.1 ], name = \"z2\" ) z3 = variable ({ 'a' : \"abc\" , 'b' : 1 }, name = \"z3\" ) a = result ( x * y , name = \"a\" ) # (5)! b = result ( x * z , name = \"b\" ) The functions need to be imported from the lmrtfy library The variable x has the local value 5 and can be between 1 and 10. You can have numpy arrays as inputs Lists and dictionaries work, too! Results are similar to variables. They have a name and an expression that they will become. Run python examples/starting/example1.py to create the profile needed for the deployment. Deployment \u00b6 To deploy the script run lmrtfy deploy examples/starting/example1.py --local Call example1 from code \u00b6 Now you can simply call catalog.example1() with the correct arguments, and you are good to go: call_example1.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import time from lmrtfy.functions import catalog job = catalog .< your_namespace > example1 ( x = 1 , # (1)! y = [ 1 , 2.0 , 3.0 ], z = \"foobar\" , z1 = [ \"bar\" , \"foo\" ], z2 = [ \"foo\" , 1 , 42 ], z3 = { \"foo\" : \"bar\" , \"bar\" : \"foo\" } ) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . if job: is currently required to ensure that you actually got a job object back from the function which would not be the case if the submission failed. Calling example1 from the CLI \u00b6 Note We encourage you to use code to submit jobs and get results. During the deployment you should have received a seven letter profile_id : Profile_id to be used for requests: CgHUejl We need the profile_id to submit a job from the CLI: lmrtfy submit CgHUejL examples/starting/example1.json If the JSON file has the correct inputs, in a valid range with correct units you will see that the job submission was successful. You will receive a ten-letter job ID. INFO Job submission successful. INFO Job-id: HgaUbcTFah With this job_id you can now get the job results: lmrtfy fetch HgaUbcTFah . That's all that is to it. Happy Hacking!","title":"Starting example"},{"location":"examples/starting_example/#example-1-simple-annotation","text":"This is a simple example to showcase the general usage of lmrtfy. It can be found in examples/starting_example/example1.py . The two core concepts are the variable and result functions which annotate the inputs and outputs of the script. They are needed to create the profile which is used to create the API. example1.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from lmrtfy.annotation import variable , result # (1)! x = variable ( 5 , name = \"x\" , min = 1 , max = 10 ) # (2)! y = variable ( np . linspace ( 0. , 1. , 101 , dtype = np . float64 ), name = \"y\" , min =- 1. , max = 11. , unit = \"m\" ) # (3)! z = variable ( \"abc\" , name = \"z\" ) z1 = variable ([ \"abc\" , \"def\" ], name = \"z1\" ) # (4)! z2 = variable ([ \"abc\" , 1 , 1.1 ], name = \"z2\" ) z3 = variable ({ 'a' : \"abc\" , 'b' : 1 }, name = \"z3\" ) a = result ( x * y , name = \"a\" ) # (5)! b = result ( x * z , name = \"b\" ) The functions need to be imported from the lmrtfy library The variable x has the local value 5 and can be between 1 and 10. You can have numpy arrays as inputs Lists and dictionaries work, too! Results are similar to variables. They have a name and an expression that they will become. Run python examples/starting/example1.py to create the profile needed for the deployment.","title":"Example 1: Simple annotation"},{"location":"examples/starting_example/#deployment","text":"To deploy the script run lmrtfy deploy examples/starting/example1.py --local","title":"Deployment"},{"location":"examples/starting_example/#call-example1-from-code","text":"Now you can simply call catalog.example1() with the correct arguments, and you are good to go: call_example1.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import time from lmrtfy.functions import catalog job = catalog .< your_namespace > example1 ( x = 1 , # (1)! y = [ 1 , 2.0 , 3.0 ], z = \"foobar\" , z1 = [ \"bar\" , \"foo\" ], z2 = [ \"foo\" , 1 , 42 ], z3 = { \"foo\" : \"bar\" , \"bar\" : \"foo\" } ) if job : while not job . ready : time . sleep ( 1 ) print ( job . results ) <your_namespace> is your private namespace on LMRTFY, which is typically your nickname. Available namespaces are shown when importing catalog or when calling catalog.update() . if job: is currently required to ensure that you actually got a job object back from the function which would not be the case if the submission failed.","title":"Call example1 from code"},{"location":"examples/starting_example/#calling-example1-from-the-cli","text":"Note We encourage you to use code to submit jobs and get results. During the deployment you should have received a seven letter profile_id : Profile_id to be used for requests: CgHUejl We need the profile_id to submit a job from the CLI: lmrtfy submit CgHUejL examples/starting/example1.json If the JSON file has the correct inputs, in a valid range with correct units you will see that the job submission was successful. You will receive a ten-letter job ID. INFO Job submission successful. INFO Job-id: HgaUbcTFah With this job_id you can now get the job results: lmrtfy fetch HgaUbcTFah . That's all that is to it. Happy Hacking!","title":"Calling example1 from the CLI"},{"location":"user_guide/installation/","text":"There are two ways to install the lmrtfy . We recommend the usage of virtual environments at the moment due to the frequent updates and changes of lmrtfy. Linux and MacOS \u00b6 We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy Windows \u00b6 On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy This way you will always have the most recent release of lmrtfy . Install from Source \u00b6 You can also install lmrtfy from source which is the best way to use the nightly features. Be aware that this might not always work. Issues are to be expected. Clone the git repository and install manually: $ git clone --branch main https://github.com/lmrtfy/lmrtfy.git $ cd lmrtfy $ pip install . The main branch is the release branch and should always work with the LMRTFY API. Alternatively, you can use the develop branch. This should be the most up-to-date branch in the repository, but things might break. So be careful while using the develop branch.","title":"Installation"},{"location":"user_guide/installation/#linux-and-macos","text":"We provide a PyPI package for Linux and MacOS which can be installed easily with pip : $ pip install lmrtfy","title":"Linux and MacOS"},{"location":"user_guide/installation/#windows","text":"On Windows the conda package manager provided by miniconda and Anaconda is the best way to use Python and install Python packages. Right now, we only support a PyPI package which can be installed with conda . If you have pip installed in your conda environment you can directly install from PyPI, otherwise you need to install pip first: $ conda install pip $ pip install lmrtfy This way you will always have the most recent release of lmrtfy .","title":"Windows"},{"location":"user_guide/installation/#install-from-source","text":"You can also install lmrtfy from source which is the best way to use the nightly features. Be aware that this might not always work. Issues are to be expected. Clone the git repository and install manually: $ git clone --branch main https://github.com/lmrtfy/lmrtfy.git $ cd lmrtfy $ pip install . The main branch is the release branch and should always work with the LMRTFY API. Alternatively, you can use the develop branch. This should be the most up-to-date branch in the repository, but things might break. So be careful while using the develop branch.","title":"Install from Source"},{"location":"user_guide/login/","text":"To use LMRTFY we need you to sign up with us to authenticate with our API. This step is necessary to prevent malicious activity. You can use one of our social logins or just sign up with email and password. With your account you can to the following things: deploy scripts as a callable function submit jobs and call cloud functions access the LMRTFY Management Board Sign Up \u00b6 Before you can login the first time you need to sign up. Currently, you have three options to do that: email address + password GitHub account Google account If you sign up with email/password you will be required to verify your email address. Login \u00b6 When you run an LMRTFY actions that require a token you will automatically be asked to login. Each token is valid for 24 hours before you need to login again. If you allow cookies our authentication provider Auth0 will recognize you. Info Tokens are currently valid for 24 hours. After that you will be requested to login again. That also means that you cannot have scripts deployed more than 24 hours right now. This will change soon so that you can deploy scripts longer than that. Just run lmrtfy deploy <script> --local again after 24h and you are fine if you need longer running deployments right now.)","title":"Sign Up/Login"},{"location":"user_guide/login/#sign-up","text":"Before you can login the first time you need to sign up. Currently, you have three options to do that: email address + password GitHub account Google account If you sign up with email/password you will be required to verify your email address.","title":"Sign Up"},{"location":"user_guide/login/#login","text":"When you run an LMRTFY actions that require a token you will automatically be asked to login. Each token is valid for 24 hours before you need to login again. If you allow cookies our authentication provider Auth0 will recognize you. Info Tokens are currently valid for 24 hours. After that you will be requested to login again. That also means that you cannot have scripts deployed more than 24 hours right now. This will change soon so that you can deploy scripts longer than that. Just run lmrtfy deploy <script> --local again after 24h and you are fine if you need longer running deployments right now.)","title":"Login"},{"location":"user_guide/calling_functions/fetch_results/","text":"Fetching results in your code \u00b6 When you call a function from your code, the results are part of the job object created when calling the function . You can only get the results when they are ready by using job.results : while not job . ready : sleep ( 1. ) print ( job . results ) Currently, the while loop is necessary to wait for the results. If the job has not been submitted for some reason the job.ready query triggers automatic resubmission. This could easily become a future (in the sense of concurrent programming) later on. We are also looking for feedback, what would work best for you. Get results with the CLI \u00b6 LMRTFY also provides a way to download the results of the computation. All you need is the <job_id> that you received when you submitted the job. Then, you simply run $ lmrtfy fetch <job_id> <save_path> The results will be saved in <save_path>/<job_id>/.. . Each result is currently saved as a JSON file with the following format: { \"<var_name>\" : < value > } Each variable has its own file.","title":"Get results"},{"location":"user_guide/calling_functions/fetch_results/#fetching-results-in-your-code","text":"When you call a function from your code, the results are part of the job object created when calling the function . You can only get the results when they are ready by using job.results : while not job . ready : sleep ( 1. ) print ( job . results ) Currently, the while loop is necessary to wait for the results. If the job has not been submitted for some reason the job.ready query triggers automatic resubmission. This could easily become a future (in the sense of concurrent programming) later on. We are also looking for feedback, what would work best for you.","title":"Fetching results in your code"},{"location":"user_guide/calling_functions/fetch_results/#get-results-with-the-cli","text":"LMRTFY also provides a way to download the results of the computation. All you need is the <job_id> that you received when you submitted the job. Then, you simply run $ lmrtfy fetch <job_id> <save_path> The results will be saved in <save_path>/<job_id>/.. . Each result is currently saved as a JSON file with the following format: { \"<var_name>\" : < value > } Each variable has its own file.","title":"Get results with the CLI"},{"location":"user_guide/calling_functions/submission/","text":"Calling a function from code \u00b6 Our goal is to provide an interface to deployed functions that works just like any other function in any other library that you have locally installed. We provide an example call_free_fall.py in the same directory as the free_fall_lmrtfy.py script. As you can see, calling a remote function via LMRTFY feels just like calling a native function. call_free_fall.py 1 2 3 4 5 6 7 8 9 10 11 12 from time import sleep from lmrtfy import catalog # (1)! job = catalog . examples . free_fall_lmrtfy ( time = 100. ) # (2)! if job : print ( job . id , job . status ) while not job . ready : # (3)! sleep ( 1. ) print ( job . results ) # (4)! Importing the catalog triggers the catalog update to get newly deployed functions every time you run the code. Our examples are all part of the catalog in the catalog.examples namespace. The function free_fall_lmrtfy is now part of the catalog and can be called just like a normal function in your code. The function is not executed in the same context as your Python interpreter. Instead it is run in one of the runners. Loop until the job is ready. In this context ready means that the results are ready to be fetched. Fetching the results is as simple as calling job.results . The return value is a dictionary with the keys corresponding to the names of the results and the values are the actual values of the result Run the script with your local python interpreter python call_free_fall.py . It runs just like a regular script but calls a remote function inside. The output of the script looks like this: INFO Validating auth token. INFO Auth token accepted. INFO Valid access token found. Login not necessary. INFO Updated function catalog. INFO Added function free_fall_lmrtfy. INFO Job CLuz1ZrpR7 created. Status is RUNNING. # (1)! CLuz1ZrpR7 JobStatus.RUNNING { 'velocity' : 981 .0 } # (2)! The job ID is always a 10-character long ID. The reported status is sometimes UNKNOWN which usually means that the LMRTFY platform has not processed the job yet. The result of the computation is a dictionary with the variable names as keys and the actual value as values. The ID of the job is going to be different from the one shown in the example output. Job IDs are always 10 characters long. Using the CLI \u00b6 LMRTFY also provides a way to submit jobs with the lmrtfy CLI tool. All you need for this is a profile_id (7 characters long) which is provided by you during the deployment and a JSON file that contains the input parameters. Attention This is a good way to call deployed scripts from another language as you can always build the JSON file and call the lmrtfy CLI. If you are using it this way, please contact us. We want to provide more native-feeling interfaces to languages other than python as well but would love to hear what you use to prioritize. For the example calculating the compound interest, the JSON file would look like this: { \"argument_values\" : { \"time\" : 100 }, \"argument_units\" : { \"time\" : \"s\" } } argument_values and argument_units contain a key-value pair each for each of the inputs in the annotation profile. The types need to match exactly. No implicit type casting in performed during the submission. The unit also has to match exactly. Save the JSON file es input.json and run: $ lmrtfy submit <profile_id> input.json Info Later on, we might perform automatic conversion in case of a unit mismatch, e.g. if the profile requires s (as in seconds) but the input is given as h (as in hours). There will be an option to enable/disable the function. If you have any opinions about that, please let us know When you submit your job you will receive a job_id which is needed to fetch the results as you will see in the next part of this guide.","title":"Call a function"},{"location":"user_guide/calling_functions/submission/#calling-a-function-from-code","text":"Our goal is to provide an interface to deployed functions that works just like any other function in any other library that you have locally installed. We provide an example call_free_fall.py in the same directory as the free_fall_lmrtfy.py script. As you can see, calling a remote function via LMRTFY feels just like calling a native function. call_free_fall.py 1 2 3 4 5 6 7 8 9 10 11 12 from time import sleep from lmrtfy import catalog # (1)! job = catalog . examples . free_fall_lmrtfy ( time = 100. ) # (2)! if job : print ( job . id , job . status ) while not job . ready : # (3)! sleep ( 1. ) print ( job . results ) # (4)! Importing the catalog triggers the catalog update to get newly deployed functions every time you run the code. Our examples are all part of the catalog in the catalog.examples namespace. The function free_fall_lmrtfy is now part of the catalog and can be called just like a normal function in your code. The function is not executed in the same context as your Python interpreter. Instead it is run in one of the runners. Loop until the job is ready. In this context ready means that the results are ready to be fetched. Fetching the results is as simple as calling job.results . The return value is a dictionary with the keys corresponding to the names of the results and the values are the actual values of the result Run the script with your local python interpreter python call_free_fall.py . It runs just like a regular script but calls a remote function inside. The output of the script looks like this: INFO Validating auth token. INFO Auth token accepted. INFO Valid access token found. Login not necessary. INFO Updated function catalog. INFO Added function free_fall_lmrtfy. INFO Job CLuz1ZrpR7 created. Status is RUNNING. # (1)! CLuz1ZrpR7 JobStatus.RUNNING { 'velocity' : 981 .0 } # (2)! The job ID is always a 10-character long ID. The reported status is sometimes UNKNOWN which usually means that the LMRTFY platform has not processed the job yet. The result of the computation is a dictionary with the variable names as keys and the actual value as values. The ID of the job is going to be different from the one shown in the example output. Job IDs are always 10 characters long.","title":"Calling a function from code"},{"location":"user_guide/calling_functions/submission/#using-the-cli","text":"LMRTFY also provides a way to submit jobs with the lmrtfy CLI tool. All you need for this is a profile_id (7 characters long) which is provided by you during the deployment and a JSON file that contains the input parameters. Attention This is a good way to call deployed scripts from another language as you can always build the JSON file and call the lmrtfy CLI. If you are using it this way, please contact us. We want to provide more native-feeling interfaces to languages other than python as well but would love to hear what you use to prioritize. For the example calculating the compound interest, the JSON file would look like this: { \"argument_values\" : { \"time\" : 100 }, \"argument_units\" : { \"time\" : \"s\" } } argument_values and argument_units contain a key-value pair each for each of the inputs in the annotation profile. The types need to match exactly. No implicit type casting in performed during the submission. The unit also has to match exactly. Save the JSON file es input.json and run: $ lmrtfy submit <profile_id> input.json Info Later on, we might perform automatic conversion in case of a unit mismatch, e.g. if the profile requires s (as in seconds) but the input is given as h (as in hours). There will be an option to enable/disable the function. If you have any opinions about that, please let us know When you submit your job you will receive a job_id which is needed to fetch the results as you will see in the next part of this guide.","title":"Using the CLI"},{"location":"user_guide/creating_functions/annotation/","text":"Now, we have seen how we can call a deployed function from our code, but how do we deploy something ourselves? This requires two main steps: You need to annotate your script to let LMRTFY know about the input variables and the results of your script. The actual deployment of your script. Annotate your script \u00b6 The annotation of your script tells the lmrtfy tool which python variables are considered inputs and outputs, which is done via the variable and results functions. This step is important, because lmrtfy traces the calls to variable and result to create a profile for the code. This profile includes the inputs and outputs as well as the additional meta information ( min , max , unit , and possibly more in the future). Let's assume that you have create a script to calculate the velocity of an object after a certain time: free_fall.py 1 2 3 4 5 standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now, if you want to recalculate for a different time, you would edit the script and run it again. While this might work for a small script like this, this becomes tedious if you have different input variables and want others to use your script easily, too. Let's change the script in such a way that lmrtfy can create a profile which can be used to deploy the function and make it available to other users: free_fall_lmrtfy.py 1 2 3 4 5 6 7 8 9 from lmrtfy.annotation import variable , result # (1)! time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # (2)! standard_gravity = 9.81 velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) velocity = result ( velocity , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) # (3)! variable and result are the imports that are needed to annotate your code to make it work with LMRTFY. variable annotates any inputs of your script. result annotates a result of script. A script can have multiple results. If you run python free_fall_lmrtfy.py you get the exact same result as before. During the run, lmrtfy created the profile for free_fall_lmrtfy.py which will be needed to deploy the function. Create the annotation profile \u00b6 It is required to run your script at least once with the regular python interpreter to create the annotation profile which will be used to generate the API. $ python <script.py> The profile is currently saved under ~/.lmrtfy/profiles . The profile is necessary for the deployment, and is human-readable; however, there should be no need to check the created profile unless for troubleshooting","title":"Annotation"},{"location":"user_guide/creating_functions/annotation/#annotate-your-script","text":"The annotation of your script tells the lmrtfy tool which python variables are considered inputs and outputs, which is done via the variable and results functions. This step is important, because lmrtfy traces the calls to variable and result to create a profile for the code. This profile includes the inputs and outputs as well as the additional meta information ( min , max , unit , and possibly more in the future). Let's assume that you have create a script to calculate the velocity of an object after a certain time: free_fall.py 1 2 3 4 5 standard_gravity = 9.81 time = 200. velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) Now, if you want to recalculate for a different time, you would edit the script and run it again. While this might work for a small script like this, this becomes tedious if you have different input variables and want others to use your script easily, too. Let's change the script in such a way that lmrtfy can create a profile which can be used to deploy the function and make it available to other users: free_fall_lmrtfy.py 1 2 3 4 5 6 7 8 9 from lmrtfy.annotation import variable , result # (1)! time = variable ( 200. , name = \"time\" , min = 0 , max = 1000 , unit = \"s\" ) # (2)! standard_gravity = 9.81 velocity = standard_gravity * time print ( f \"Velocity after { time } seconds is { velocity } m/s.\" ) velocity = result ( velocity , name = \"velocity\" , min = 0 , max = 9810 , unit = \"m\" ) # (3)! variable and result are the imports that are needed to annotate your code to make it work with LMRTFY. variable annotates any inputs of your script. result annotates a result of script. A script can have multiple results. If you run python free_fall_lmrtfy.py you get the exact same result as before. During the run, lmrtfy created the profile for free_fall_lmrtfy.py which will be needed to deploy the function.","title":"Annotate your script"},{"location":"user_guide/creating_functions/annotation/#create-the-annotation-profile","text":"It is required to run your script at least once with the regular python interpreter to create the annotation profile which will be used to generate the API. $ python <script.py> The profile is currently saved under ~/.lmrtfy/profiles . The profile is necessary for the deployment, and is human-readable; however, there should be no need to check the created profile unless for troubleshooting","title":"Create the annotation profile"},{"location":"user_guide/creating_functions/deployment/","text":"Deploying the function (local runner) \u00b6 Now you can deploy the function and make it available via the LMRTFY API. This is simply done by running $ lmrtfy deploy <path_to_script.py> --local The --local flag means that the script will run locally on your computer and waits for jobs from the outside. The LMRTFY API only allows job submissions that fit your deployed annotation profile. In the future you will be able to deploy directly to the cloud. Then, you do not have to host the runner yourself. Warning Don't change the script after you have deployed it. The current advice would be to copy and rename the script before deployment. In later versions, this will be taken care of by the lmrtfy tool. Example \u00b6 Let's assume that you want to deploy the example script free_fall_lmrtfy.py provided in examples/free_fall/free_fall_lmrtfy.py . This is simple done by running $ cd examples/free_fall $ python free_fall_lmrtfy.py # (1)! $ lmrtfy deploy free_fall_lmrtfy.py --local This step is necessary to create the annotation profile, but testing your script locally is likely part of your development process. Therefore, often this step is not needed. Running lmrtfy deploy ... --local does two things: Register the function with name free_fall_lmrtfy in your LMRTFY catalog . If the function already exists it will be replaced if the input and output signatures have not changed. Otherwise, the profile will be rejected. You can force the replacement with the --force-replacement flag. Start a runner on your local system that takes the submitted jobs and executes them. When you stop the runner you won't receive any results from calling the function. When a job is submitted the types of the job's input parameters are checked by the LMRTFY API. Furthermore, they are also checked for their bounds and their units. This way, only jobs that can be run successfully with your script will get executed by it. Deploying to the cloud \u00b6 LMRTFY is currently not able to deploy your scripts directly to the cloud; however, you can do this manually by using lmrtfy inside of a docker container. The docker container can be run on your laptop, one of your own servers or in the cloud. The current workaround would be to use lmrtfy on a server or inside a docker container which can be hosted in the cloud. Inside the docker container you run the same command as if you were to run locally. Easy cloud deployment will be added in the future.","title":"Deployment"},{"location":"user_guide/creating_functions/deployment/#deploying-the-function-local-runner","text":"Now you can deploy the function and make it available via the LMRTFY API. This is simply done by running $ lmrtfy deploy <path_to_script.py> --local The --local flag means that the script will run locally on your computer and waits for jobs from the outside. The LMRTFY API only allows job submissions that fit your deployed annotation profile. In the future you will be able to deploy directly to the cloud. Then, you do not have to host the runner yourself. Warning Don't change the script after you have deployed it. The current advice would be to copy and rename the script before deployment. In later versions, this will be taken care of by the lmrtfy tool.","title":"Deploying the function (local runner)"},{"location":"user_guide/creating_functions/deployment/#example","text":"Let's assume that you want to deploy the example script free_fall_lmrtfy.py provided in examples/free_fall/free_fall_lmrtfy.py . This is simple done by running $ cd examples/free_fall $ python free_fall_lmrtfy.py # (1)! $ lmrtfy deploy free_fall_lmrtfy.py --local This step is necessary to create the annotation profile, but testing your script locally is likely part of your development process. Therefore, often this step is not needed. Running lmrtfy deploy ... --local does two things: Register the function with name free_fall_lmrtfy in your LMRTFY catalog . If the function already exists it will be replaced if the input and output signatures have not changed. Otherwise, the profile will be rejected. You can force the replacement with the --force-replacement flag. Start a runner on your local system that takes the submitted jobs and executes them. When you stop the runner you won't receive any results from calling the function. When a job is submitted the types of the job's input parameters are checked by the LMRTFY API. Furthermore, they are also checked for their bounds and their units. This way, only jobs that can be run successfully with your script will get executed by it.","title":"Example"},{"location":"user_guide/creating_functions/deployment/#deploying-to-the-cloud","text":"LMRTFY is currently not able to deploy your scripts directly to the cloud; however, you can do this manually by using lmrtfy inside of a docker container. The docker container can be run on your laptop, one of your own servers or in the cloud. The current workaround would be to use lmrtfy on a server or inside a docker container which can be hosted in the cloud. Inside the docker container you run the same command as if you were to run locally. Easy cloud deployment will be added in the future.","title":"Deploying to the cloud"},{"location":"user_guide/creating_functions/variables_and_results/","text":"The API reference with the corresponding function signatures for variable and result can be found here . In this part we want to take a closer look at some details. Transparency \u00b6 The annotation functions variable and result are transparent for the Python interpreter. Let's say we have the following code: 1 2 3 4 5 x = variable ( 5. , name = \"x\" ) tmp = heavy_computation ( x ) y = result ( tmp , name = \"y\" ) To the Python interpreter the code looks more or less like this: 1 2 3 4 5 x = 5 tmp = heavy_computation ( x ) y = tmp The calls to variable and result always return the value itself. In the background, these calls create the annotation which is necessary for the LMRTFY platform. When a function is deployed the variable functions injects the input arguments of the job received through the LMRTFY platform into the script. The result function on the other hand funnels the result back to the LMRTFY platform which makes it available to the caller of the function. Checking the validity of input arguments \u00b6 If you checked the API reference you may have noticed that variable and result have some additional parameters: min , max , and unit . Even better, through the first argument, the actual value, you also create type information that allows us to thoroughly check incoming job submissions. Type checking \u00b6 The input type is inferred from the value that is used in the variable function call. function call inferred type of x accepted type with LMRTFY x = variable ( 5 , name = \"x\" ) int int x = variable ( 5. , name = \"x\" ) float float x = variable ([ 5 , 2 ], name = \"x\" ) int_array list [ int ], ndarray [ int ] x = variable ([ 5. , 2. ], name = \"x\" ) float_array list [ float ], ndarray [ float ] x = variable ( \"abc\" , name = \"x\" ) str str x = variable ([ \"abc\" , \"def\" ], name = \"x\" ) str_array list [ str ] x = variable ([ \"abd\" , 1 , 1.1 ], name = \"x\" ) json list [ any ] x = variable ({ \"a\" : \"a\" , \"b\" : 1 }, name = \"x\" ) json dict If the input arguments submitted via the catalog do not match the accepted type, we reject the job and tell the caller to fix their types. We chose a strict type checking because a type has its meaning which it might lose during automatic conversion. Bounds checking \u00b6 The min and max parameters limit the range in which numeric input variables are seen as valid. This is especially useful if your algorithm has known and well-defined limitations. If it only works between \\(0\\) and \\(1000\\) you can simply specify variable ( 500 , name = \"a\" , min = 0 , max = 1000 ) . When a job is submitted via the LMRTFY platform, these bounds are checked. If the input variable a is out of bounds we notify the caller and reject the job. Unit checking \u00b6 Another useful thing is the annotation with an actual unit for the variable. Currently, this is done via str but we will switch to pint units in later releases. Similar to the numeric bounds the units are checked during the job submission and the job is rejected if the unit do not match. Example In 1999 the NASA lost its Mars Climate Orbiter due to a navigation error, which was caused by a failure to convert units from the imperial system to the metric system .","title":"Variables and Results"},{"location":"user_guide/creating_functions/variables_and_results/#transparency","text":"The annotation functions variable and result are transparent for the Python interpreter. Let's say we have the following code: 1 2 3 4 5 x = variable ( 5. , name = \"x\" ) tmp = heavy_computation ( x ) y = result ( tmp , name = \"y\" ) To the Python interpreter the code looks more or less like this: 1 2 3 4 5 x = 5 tmp = heavy_computation ( x ) y = tmp The calls to variable and result always return the value itself. In the background, these calls create the annotation which is necessary for the LMRTFY platform. When a function is deployed the variable functions injects the input arguments of the job received through the LMRTFY platform into the script. The result function on the other hand funnels the result back to the LMRTFY platform which makes it available to the caller of the function.","title":"Transparency"},{"location":"user_guide/creating_functions/variables_and_results/#checking-the-validity-of-input-arguments","text":"If you checked the API reference you may have noticed that variable and result have some additional parameters: min , max , and unit . Even better, through the first argument, the actual value, you also create type information that allows us to thoroughly check incoming job submissions.","title":"Checking the validity of input arguments"},{"location":"user_guide/creating_functions/variables_and_results/#type-checking","text":"The input type is inferred from the value that is used in the variable function call. function call inferred type of x accepted type with LMRTFY x = variable ( 5 , name = \"x\" ) int int x = variable ( 5. , name = \"x\" ) float float x = variable ([ 5 , 2 ], name = \"x\" ) int_array list [ int ], ndarray [ int ] x = variable ([ 5. , 2. ], name = \"x\" ) float_array list [ float ], ndarray [ float ] x = variable ( \"abc\" , name = \"x\" ) str str x = variable ([ \"abc\" , \"def\" ], name = \"x\" ) str_array list [ str ] x = variable ([ \"abd\" , 1 , 1.1 ], name = \"x\" ) json list [ any ] x = variable ({ \"a\" : \"a\" , \"b\" : 1 }, name = \"x\" ) json dict If the input arguments submitted via the catalog do not match the accepted type, we reject the job and tell the caller to fix their types. We chose a strict type checking because a type has its meaning which it might lose during automatic conversion.","title":"Type checking"},{"location":"user_guide/creating_functions/variables_and_results/#bounds-checking","text":"The min and max parameters limit the range in which numeric input variables are seen as valid. This is especially useful if your algorithm has known and well-defined limitations. If it only works between \\(0\\) and \\(1000\\) you can simply specify variable ( 500 , name = \"a\" , min = 0 , max = 1000 ) . When a job is submitted via the LMRTFY platform, these bounds are checked. If the input variable a is out of bounds we notify the caller and reject the job.","title":"Bounds checking"},{"location":"user_guide/creating_functions/variables_and_results/#unit-checking","text":"Another useful thing is the annotation with an actual unit for the variable. Currently, this is done via str but we will switch to pint units in later releases. Similar to the numeric bounds the units are checked during the job submission and the job is rejected if the unit do not match. Example In 1999 the NASA lost its Mars Climate Orbiter due to a navigation error, which was caused by a failure to convert units from the imperial system to the metric system .","title":"Unit checking"},{"location":"user_guide/sharing/sharing/","text":"You can share your functions with others and they can simply use it from their code just as simply as you can. All you need is the email address of the person you want to share with. We take care of the rest. Sharing is implemented on a namespace level. Sharing via code \u00b6 Using code is the easiest way to share a namespace. There are 3 steps to share a function: Create a namespace that you want to share Add the function you want to share to the namespace Share the namespace via e-mail In code it looks just like this: Sharing a function 1 2 3 4 5 6 7 8 9 from lmrtfy.functions import catalog catalog . create_namespace ( catalog .< user_namespace > , \"new_namespace\" ) catalog . add_function_to_namespace ( catalog .< user_namespace >. new_namespace , catatlog .< user_namespace >.< function > ) catalog . share_namespace ( catalog .< user_namespace >. new_namespace , \"someone@somewhere.com\" ) <user_namespace> is the namespace that is bound to your user and is <username>_a0 if you use a username/password combination for your account. If you use a social login the username will be taken from there. To ensure uniqueness we add a suffix to the username to create the namespace: for Auth0 (username/password): <username>_a0 for GitHub: <username>_gh for Google: <username>_gl <function> is any function that is available in your catalog and owned by you. The output of th script above is an invite ID. The invite is sent via email to the specified recipient. The invite is not bound to this email address. If the invitee uses another email for LMRTFY they can just login with their regular account to accept the invite. Alternatively, the invitation can be accepted via code as well: Accepting an invite in code catalog . accept_invite ( \"<invite_id>\" )","title":"Sharing a function"},{"location":"user_guide/sharing/sharing/#sharing-via-code","text":"Using code is the easiest way to share a namespace. There are 3 steps to share a function: Create a namespace that you want to share Add the function you want to share to the namespace Share the namespace via e-mail In code it looks just like this: Sharing a function 1 2 3 4 5 6 7 8 9 from lmrtfy.functions import catalog catalog . create_namespace ( catalog .< user_namespace > , \"new_namespace\" ) catalog . add_function_to_namespace ( catalog .< user_namespace >. new_namespace , catatlog .< user_namespace >.< function > ) catalog . share_namespace ( catalog .< user_namespace >. new_namespace , \"someone@somewhere.com\" ) <user_namespace> is the namespace that is bound to your user and is <username>_a0 if you use a username/password combination for your account. If you use a social login the username will be taken from there. To ensure uniqueness we add a suffix to the username to create the namespace: for Auth0 (username/password): <username>_a0 for GitHub: <username>_gh for Google: <username>_gl <function> is any function that is available in your catalog and owned by you. The output of th script above is an invite ID. The invite is sent via email to the specified recipient. The invite is not bound to this email address. If the invitee uses another email for LMRTFY they can just login with their regular account to accept the invite. Alternatively, the invitation can be accepted via code as well: Accepting an invite in code catalog . accept_invite ( \"<invite_id>\" )","title":"Sharing via code"},{"location":"user_guide/web_app/catalog/","text":"Coming soon...","title":"LMRTFY catalog"},{"location":"user_guide/web_app/overview/","text":"Coming soon...","title":"Overview"}]}